
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>DriftBuddy Security Report</title>
        <style>
            * {
                margin: 0;
                padding: 0;
                box-sizing: border-box;
            }
            
            body {
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                line-height: 1.6;
                color: #333;
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                min-height: 100vh;
            }
            
            .container {
                max-width: 1200px;
                margin: 0 auto;
                padding: 20px;
            }
            
            .header {
                background: white;
                border-radius: 15px;
                padding: 30px;
                margin-bottom: 30px;
                box-shadow: 0 10px 30px rgba(0,0,0,0.1);
                text-align: center;
            }
            
            .header h1 {
                color: #2c3e50;
                font-size: 2.5em;
                margin-bottom: 10px;
            }
            
            .header .subtitle {
                color: #7f8c8d;
                font-size: 1.1em;
            }
            
            .summary-section {
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
                gap: 20px;
                margin-bottom: 30px;
            }
            
            .summary-card {
                background: white;
                border-radius: 15px;
                padding: 25px;
                text-align: center;
                box-shadow: 0 5px 15px rgba(0,0,0,0.1);
                transition: transform 0.3s ease;
            }
            
            .summary-card:hover {
                transform: translateY(-5px);
            }
            
            .summary-number {
                font-size: 2.5em;
                font-weight: bold;
                margin-bottom: 5px;
            }
            
            .summary-label {
                font-size: 1.1em;
                font-weight: 500;
            }
            
            .summary-card.critical {
                border-left: 5px solid #e74c3c;
            }
            .summary-card.critical .summary-number {
                color: #e74c3c;
            }
            
            .summary-card.high {
                border-left: 5px solid #f39c12;
            }
            .summary-card.high .summary-number {
                color: #f39c12;
            }
            
            .summary-card.medium {
                border-left: 5px solid #3498db;
            }
            .summary-card.medium .summary-number {
                color: #3498db;
            }
            
            .summary-card.low {
                border-left: 5px solid #27ae60;
            }
            .summary-card.low .summary-number {
                color: #27ae60;
            }
            
            .summary-card.info {
                border-left: 5px solid #95a5a6;
            }
            .summary-card.info .summary-number {
                color: #95a5a6;
            }
            
            .toc {
                background: white;
                border-radius: 15px;
                padding: 25px;
                margin-bottom: 30px;
                box-shadow: 0 5px 15px rgba(0,0,0,0.1);
                position: sticky;
                top: 20px;
                z-index: 100;
            }
            
            .toc h3 {
                color: #2c3e50;
                margin-bottom: 15px;
            }
            
            .toc ul {
                list-style: none;
            }
            
            .toc li {
                margin-bottom: 8px;
            }
            
            .toc a {
                color: #3498db;
                text-decoration: none;
                font-weight: 500;
                transition: color 0.3s ease;
            }
            
            .toc a:hover {
                color: #2980b9;
            }
            
            .severity-section {
                margin-bottom: 40px;
            }
            
            .severity-header {
                background: white;
                border-radius: 15px 15px 0 0;
                padding: 20px 25px;
                margin-bottom: 0;
                font-size: 1.5em;
                font-weight: 600;
            }
            
            .severity-header.critical {
                background: linear-gradient(135deg, #e74c3c, #c0392b);
                color: white;
            }
            
            .severity-header.high {
                background: linear-gradient(135deg, #f39c12, #e67e22);
                color: white;
            }
            
            .severity-header.medium {
                background: linear-gradient(135deg, #3498db, #2980b9);
                color: white;
            }
            
            .severity-header.low {
                background: linear-gradient(135deg, #27ae60, #229954);
                color: white;
            }
            
            .severity-header.info {
                background: linear-gradient(135deg, #95a5a6, #7f8c8d);
                color: white;
            }
            
            .finding-card {
                background: white;
                border-radius: 0 0 15px 15px;
                margin-bottom: 20px;
                box-shadow: 0 5px 15px rgba(0,0,0,0.1);
                overflow: hidden;
            }
            
            .finding-header {
                padding: 20px 25px;
                border-bottom: 1px solid #ecf0f1;
                display: flex;
                align-items: center;
                gap: 15px;
            }
            
            .severity-badge {
                padding: 5px 12px;
                border-radius: 20px;
                font-size: 0.8em;
                font-weight: 600;
                text-transform: uppercase;
                letter-spacing: 0.5px;
            }
            
            .severity-badge.critical {
                background: #e74c3c;
                color: white;
            }
            
            .severity-badge.high {
                background: #f39c12;
                color: white;
            }
            
            .severity-badge.medium {
                background: #3498db;
                color: white;
            }
            
            .severity-badge.low {
                background: #27ae60;
                color: white;
            }
            
            .severity-badge.info {
                background: #95a5a6;
                color: white;
            }
            
            .finding-title {
                color: #2c3e50;
                font-size: 1.3em;
                margin: 0;
            }
            
            .finding-details {
                padding: 20px 25px;
                background: #f8f9fa;
            }
            
            .detail-item {
                margin-bottom: 10px;
            }
            
            .detail-item:last-child {
                margin-bottom: 0;
            }
            
            .detail-item code {
                background: #e9ecef;
                padding: 2px 6px;
                border-radius: 4px;
                font-family: 'Courier New', monospace;
            }
            
            .finding-explanation {
                padding: 20px 25px;
            }
            
            .finding-explanation h4 {
                color: #2c3e50;
                margin-bottom: 15px;
                font-size: 1.1em;
            }
            
            .explanation-content {
                background: #f8f9fa;
                padding: 15px;
                border-radius: 8px;
                border-left: 4px solid #3498db;
                line-height: 1.7;
            }
            
            .finding-footer {
                padding: 15px 25px;
                background: #f8f9fa;
                border-top: 1px solid #ecf0f1;
            }
            
            .learn-more-btn {
                display: inline-block;
                background: #3498db;
                color: white;
                padding: 8px 16px;
                border-radius: 6px;
                text-decoration: none;
                font-weight: 500;
                transition: background 0.3s ease;
            }
            
            .learn-more-btn:hover {
                background: #2980b9;
            }
            
            .footer {
                text-align: center;
                padding: 30px;
                color: white;
                font-size: 0.9em;
            }
            
            @media (max-width: 768px) {
                .container {
                    padding: 10px;
                }
                
                .header h1 {
                    font-size: 2em;
                }
                
                .summary-section {
                    grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
                }
                
                .finding-header {
                    flex-direction: column;
                    align-items: flex-start;
                    gap: 10px;
                }
            }
        </style>
    </head>
    <body>
        <div class="container">
            <div class="header">
                <h1>üõ°Ô∏è DriftBuddy Security Report</h1>
                <p class="subtitle">Infrastructure as Code Security Analysis Dashboard</p>
                <p class="subtitle">Generated on 2025-07-25 21:16:34</p>
            </div>
            
            <div class="summary-section">
                
            <div class="summary-card high">
                <div class="summary-number">21</div>
                <div class="summary-label">HIGH</div>
            </div>
            
            <div class="summary-card medium">
                <div class="summary-number">52</div>
                <div class="summary-label">MEDIUM</div>
            </div>
            
            <div class="summary-card low">
                <div class="summary-number">45</div>
                <div class="summary-label">LOW</div>
            </div>
            
            <div class="summary-card info">
                <div class="summary-number">5</div>
                <div class="summary-label">INFO</div>
            </div>
            
            </div>
            
            
        <div class="toc">
            <h3>üìã Table of Contents</h3>
            <ul>
        <li><a href="#high-section">HIGH (21)</a></li><li><a href="#medium-section">MEDIUM (52)</a></li><li><a href="#low-section">LOW (45)</a></li><li><a href="#info-section">INFO (5)</a></li></ul></div>
            
            <div class="findings-content">
                <div id="high-section" class="severity-section"><h2 class="severity-header high">HIGH (21)</h2>
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge high">HIGH</span>
                        <h3 class="finding-title">Container Is Privileged</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>21</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Privileged containers lack essential security restrictions and should be avoided by removing the 'privileged' flag or by changing its value to false
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: <br>This security issue means that a container within your Kubernetes configuration is set to run in a 'privileged' mode. This means the container has access to all devices on the host machine, and it can perform almost any operation that the host machine can. <br><br>2. Security Concern: <br>Running a container in privileged mode is a significant security concern because it essentially provides root access to the host machine. If an attacker were to gain control of the container, they could potentially gain control of the entire host machine, leading to a serious security breach.<br><br>3. Secure Code Example: <br>To fix this issue, you need to change the 'privileged' flag to false in your Kubernetes configuration file. Here is an example:<br><br>Before:<br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: privileged-pod<br>spec:<br>  containers:<br>  - name: privileged<br>    image: nginx<br>    securityContext:<br>      privileged: true<br>```<br><br>After:<br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: privileged-pod<br>spec:<br>  containers:<br>  - name: privileged<br>    image: nginx<br>    securityContext:<br>      privileged: false<br>```<br><br>4. Best Practices: <br>- Avoid running containers in privileged mode unless absolutely necessary. <br>- Always follow the principle of least privilege, i.e., only grant the minimum permissions necessary for a task. <br>- Regularly review and update your Kubernetes configurations to ensure they follow security best practices. <br>- Use security tools to automatically scan your Infrastructure as Code (IaC) for potential security issues. <br>- Implement proper logging and monitoring to detect any unusual activities.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/workloads/pods/#privileged-mode-for-containers" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge high">HIGH</span>
                        <h3 class="finding-title">Non Kube System Pod With Host Mount</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>189</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> A non kube-system workload should not have hostPath mounted
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The security issue "Non Kube System Pod With Host Mount" means that a Kubernetes Pod, which is not part of the Kubernetes system namespace, has been configured to access the file system of the host machine where the Kubernetes node is running. This is done using a feature called hostPath.<br><br>2. Security Concern:<br>This is a high severity security concern because it can lead to potential data leakage or unauthorized access to sensitive files on the host machine. If the pod is compromised, the attacker could gain access to the entire host file system, leading to a serious security breach.<br><br>3. Secure Code Example:<br>To fix this issue, you should avoid using hostPath volumes unless absolutely necessary. Instead, use other types of storage like Persistent Volumes (PV) or Persistent Volume Claims (PVC). Here's an example of how to use a PVC instead of hostPath:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: mypod<br>spec:<br>  containers:<br>  - name: mypod<br>    image: myimage<br>    volumeMounts:<br>    - mountPath: /data<br>      name: myvolume<br>  volumes:<br>  - name: myvolume<br>    persistentVolumeClaim:<br>      claimName: myclaim<br>```<br><br>In this example, the pod `mypod` is using a Persistent Volume Claim `myclaim` to store its data, instead of mounting a directory from the host.<br><br>4. Best Practices:<br>- Avoid using hostPath volumes unless absolutely necessary.<br>- Always use the least privilege principle. Only give pods the permissions they need to function.<br>- Regularly review and audit your Kubernetes configurations.<br>- Use automated tools to scan your Infrastructure as Code (IaC) for security issues.<br>- Educate your team about the risks associated with using hostPath volumes.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/storage/volumes/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge high">HIGH</span>
                        <h3 class="finding-title">Passwords And Secrets - Generic Password</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>68</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Query to find passwords and secrets in infrastructure code.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue indicates that there is a password or secret embedded directly in the infrastructure code file `kubernetes_vulnerable.yaml` at line 68. This is a high severity issue because it exposes sensitive information that could be exploited by malicious actors.<br><br>2. Security Concern: Embedding passwords and secrets directly in code is a security risk because it increases the likelihood of these sensitive details being exposed. If the code is ever shared, committed to a public repository, or even just viewed by someone who shouldn't have access to these details, it can lead to unauthorized access to systems or data.<br><br>3. Secure Code Example: Instead of embedding secrets directly in the code, you should use a secure method for managing secrets, such as Kubernetes Secrets. Here's an example of how to do this:<br><br>   ```yaml<br>   apiVersion: v1<br>   kind: Secret<br>   metadata:<br>     name: mysecret<br>   type: Opaque<br>   data:<br>     password: <base64-encoded-password><br>   ```<br><br>   Note that the password should be base64-encoded. You can create a base64-encoded password using a command like `echo -n 'mypassword' | base64`.<br><br>4. Best Practices: To prevent this issue, never embed passwords or secrets directly in your code. Instead, use secure methods for managing secrets. In Kubernetes, you can use Kubernetes Secrets or third-party solutions like HashiCorp Vault. Also, consider using automated tools to scan your code for secrets before committing, and always use code reviews to catch potential security issues.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://docs.kics.io/latest/secrets/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge high">HIGH</span>
                        <h3 class="finding-title">Passwords And Secrets - Generic Token</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>58</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Query to find passwords and secrets in infrastructure code.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue means that there's a token (a kind of secret or password) in the Kubernetes configuration file (kubernetes_vulnerable.yaml) at line 58, which is likely stored in plain text. This token could be used to authenticate or gain access to certain resources or services.<br><br>2. Security Concern: Storing passwords or secrets in plain text within code or configuration files is a serious security concern. If an unauthorized person gains access to this file, they could potentially use this token to gain access to sensitive resources or services, resulting in a security breach.<br><br>3. Secure Code Example: The token should not be stored in plain text within the configuration file. Instead, it should be stored securely using Kubernetes Secrets. Here's an example of how to do it:<br><br>   ```yaml<br>   apiVersion: v1<br>   kind: Secret<br>   metadata:<br>     name: my-secret<br>   type: Opaque<br>   data:<br>     token: <base64-encoded-token><br>   ```<br><br>   The token should be base64 encoded and then referenced in the configuration file as follows:<br><br>   ```yaml<br>   apiVersion: v1<br>   kind: Pod<br>   metadata:<br>     name: my-pod<br>   spec:<br>     containers:<br>     - name: my-container<br>       env:<br>         - name: TOKEN<br>           valueFrom:<br>             secretKeyRef:<br>               name: my-secret<br>               key: token<br>   ```<br><br>4. Best Practices: To prevent this issue, never store passwords or secrets in plain text within your code or configuration files. Use secure storage solutions like Kubernetes Secrets, HashiCorp Vault, or AWS Secrets Manager. Always use encryption for sensitive data. Make sure to follow the principle of least privilege, i.e., only grant the minimum permissions necessary for a task. Regularly rotate and change your secrets. Finally, implement a robust code review process to catch such issues before the code is deployed.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://docs.kics.io/latest/secrets/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge high">HIGH</span>
                        <h3 class="finding-title">Passwords And Secrets - Password in URL</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>56</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Query to find passwords and secrets in infrastructure code.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: The security issue "Passwords And Secrets - Password in URL" refers to a situation where a password or secret key is embedded directly in a URL within your Infrastructure as Code (IaC) file. This is a high severity issue because it exposes sensitive data in plain text, which can be easily read and misused.<br><br>2. Security Concern: Having a password in a URL is a significant security risk because if the URL is logged or leaked in any way, the password is immediately exposed. This could lead to unauthorized access to your systems or data. Additionally, URLs are often cached or stored in browser history, further increasing the risk of exposure.<br><br>3. Secure Code Example: Instead of embedding the password directly in the URL, use secure methods to store and retrieve secrets. For example, in Kubernetes, you can use Secrets or ConfigMaps:<br><br>```yaml<br>apiVersion: v1<br>kind: Secret<br>metadata:<br>  name: mysecret<br>type: Opaque<br>data:<br>  password: <base64-encoded-password><br>```<br>Then, in your application configuration, reference the secret:<br><br>```yaml<br>env:<br>  - name: PASSWORD<br>    valueFrom:<br>      secretKeyRef:<br>        name: mysecret<br>        key: password<br>```<br><br>4. Best Practices: <br>   - Never include passwords or secrets directly in your code or configuration files. <br>   - Use secure storage solutions for secrets, such as Kubernetes Secrets, AWS Secrets Manager, or HashiCorp Vault.<br>   - Regularly rotate your secrets and passwords.<br>   - Use automated tools to scan your IaC for security issues, including exposed secrets.<br>   - Implement a strong access control policy to limit who can access your IaC files.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://docs.kics.io/latest/secrets/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge high">HIGH</span>
                        <h3 class="finding-title">Privilege Escalation Allowed</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>22</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Containers should not run with allowPrivilegeEscalation in order to prevent them from gaining more privileges than their parent process
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The security issue 'Privilege Escalation Allowed' means that the container in your Kubernetes configuration file is set to allow processes within it to gain more privileges than their parent process. This is usually done by setting the 'allowPrivilegeEscalation' flag to true.<br><br>2. Security Concern:<br>This is a security concern because if an attacker gains access to the container, they could exploit this setting to gain higher privileges, potentially allowing them to take control of the entire system. This could lead to data breaches, system damage, or other serious security incidents.<br><br>3. Secure Code Example:<br>To fix this issue, you should set the 'allowPrivilegeEscalation' flag to false in your Kubernetes configuration file. Here's how you can do it:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  containers:<br>  - name: secure-container<br>    image: secureimage<br>    securityContext:<br>      allowPrivilegeEscalation: false<br>```<br>In this example, the 'allowPrivilegeEscalation' flag is set to false, preventing any process within the container from gaining more privileges than its parent process.<br><br>4. Best Practices:<br>To prevent this issue from occurring, always follow these best practices:<br><br>- Always set 'allowPrivilegeEscalation' to false unless it's absolutely necessary for your application to function.<br>- Regularly review and update your Kubernetes configuration files to ensure they follow the latest security best practices.<br>- Implement a robust code review process to catch any potential security issues before they make it into production.<br>- Use automated security tools to scan your Infrastructure as Code (IaC) for potential security issues.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge high">HIGH</span>
                        <h3 class="finding-title">Privilege Escalation Allowed</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>202</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Containers should not run with allowPrivilegeEscalation in order to prevent them from gaining more privileges than their parent process
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: <br>This security issue means that the container in the Kubernetes configuration file is set to allow privilege escalation. This means that processes within the container can gain more privileges than their parent process, potentially allowing them to execute commands or access resources they should not be able to.<br><br>2. Security Concern: <br>This is a major security concern because if an attacker gains access to the container, they could escalate their privileges and gain full control over the container or even the host system. This could lead to unauthorized access to sensitive data, disruption of services, or use of the system for malicious activities.<br><br>3. Secure Code Example: <br>To fix this issue, you need to set the `allowPrivilegeEscalation` field to `false` in your Kubernetes configuration file. Here is an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  containers:<br>  - name: secure-container<br>    image: secureimage<br>    securityContext:<br>      allowPrivilegeEscalation: false<br>```<br><br>In this example, the `allowPrivilegeEscalation` field is set to `false` for the container named `secure-container`, which prevents any process within the container from gaining more privileges than its parent process.<br><br>4. Best Practices: <br>To prevent this issue from occurring, follow these best practices:<br><br>- Always set `allowPrivilegeEscalation` to `false` in your Kubernetes configuration files unless there is a specific, justified reason to do otherwise.<br>- Regularly review and update your Kubernetes configurations to ensure they follow security best practices.<br>- Implement a policy of least privilege, where processes are only given the minimum privileges they need to perform their tasks.<br>- Use automated tools to scan your Infrastructure as Code (IaC) for security issues and misconfigurations.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge high">HIGH</span>
                        <h3 class="finding-title">Privilege Escalation Allowed</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>181</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Containers should not run with allowPrivilegeEscalation in order to prevent them from gaining more privileges than their parent process
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The security issue "Privilege Escalation Allowed" means that the container in the Kubernetes configuration file is set to allow processes within the container to gain more privileges than their parent process. This could potentially allow a malicious process to gain unauthorized access or perform actions it should not be able to.<br><br>2. Security Concern:<br>This is a significant security concern because if a process within the container is compromised, it could escalate its privileges to perform actions that could harm the system, such as modifying critical system files, disrupting system processes, or accessing sensitive data.<br><br>3. Secure Code Example:<br>To fix this issue, you should set the `allowPrivilegeEscalation` field to `false` in your Kubernetes configuration file. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  containers:<br>  - name: secure-container<br>    image: secureimage<br>    securityContext:<br>      allowPrivilegeEscalation: false<br>```<br><br>In this example, the `allowPrivilegeEscalation` field is set to `false` for the `secure-container` container, which prevents processes within the container from gaining more privileges than their parent process.<br><br>4. Best Practices:<br>To prevent this issue, always explicitly set `allowPrivilegeEscalation` to `false` in your Kubernetes configuration files. Additionally, regularly review and update your security policies and configurations, and use automated tools to scan your Infrastructure as Code (IaC) for potential security issues. Also, follow the principle of least privilege, meaning that a user or process should have only the privileges necessary to perform its intended function, and no more.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge high">HIGH</span>
                        <h3 class="finding-title">Privilege Escalation Allowed</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>87</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Containers should not run with allowPrivilegeEscalation in order to prevent them from gaining more privileges than their parent process
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue means that the container in the Kubernetes configuration file is set to allow privilege escalation. This means that a process inside the container can gain more privileges than its parent process, potentially allowing it to execute commands or access resources it shouldn't be able to.<br><br>2. Security Concern: This is a security concern because if an attacker gains control of a process inside the container, they could escalate their privileges to gain control of the entire container or even the host system. This could lead to data breaches, unauthorized access to sensitive information, or disruption of services.<br><br>3. Secure Code Example: To fix this issue, you need to set the `allowPrivilegeEscalation` field to `false` in the container's security context in the Kubernetes configuration file. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  containers:<br>  - name: secure-container<br>    image: secureimage<br>    securityContext:<br>      allowPrivilegeEscalation: false<br>```<br><br>In this example, the `allowPrivilegeEscalation` field is set to `false`, preventing any process inside the container from gaining more privileges than its parent process.<br><br>4. Best Practices: To prevent this issue, always set `allowPrivilegeEscalation` to `false` unless there's a specific, justified reason to do otherwise. Regularly review and audit your Kubernetes configuration files to ensure they follow security best practices. Use automated tools to scan your Infrastructure as Code (IaC) for potential security issues. Educate your team about the importance of not allowing privilege escalation in containers.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge high">HIGH</span>
                        <h3 class="finding-title">RBAC Wildcard In Rule</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>124</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Roles and ClusterRoles with wildcard RBAC permissions provide excessive rights to the Kubernetes API and should be avoided. The principle of least privilege recommends to specify only the set of needed objects and actions
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: <br>This security issue is related to Role-Based Access Control (RBAC) in Kubernetes, a system for managing permissions and access to the Kubernetes API. The problem is that a role or cluster role in the Kubernetes configuration file is using a wildcard (*), which means it's granting all possible permissions. This goes against the principle of least privilege, which suggests that entities should only have the minimal permissions they need to perform their tasks.<br><br>2. Security Concern: <br>The concern here is that if a malicious actor gains access to an entity with such broad permissions, they could potentially perform any action on the Kubernetes API, including destructive actions like deleting resources or sensitive actions like reading confidential data.<br><br>3. Secure Code Example: <br>Instead of using a wildcard, specify the exact permissions needed. For example, if a service only needs to get, list, and watch pods, the rule should look like this:<br><br>```yaml<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: Role<br>metadata:<br>  namespace: default<br>  name: pod-reader<br>rules:<br>- apiGroups: [""]<br>  resources: ["pods"]<br>  verbs: ["get", "watch", "list"]<br>```<br><br>4. Best Practices: <br>To prevent this issue, always follow the principle of least privilege when assigning permissions. Avoid using wildcards in RBAC rules. Instead, explicitly list the permissions that are required for each role. Regularly review and update these permissions as needed. Consider using automated tools to scan your IaC for security issues like this one.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge high">HIGH</span>
                        <h3 class="finding-title">RBAC Wildcard In Rule</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>122</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Roles and ClusterRoles with wildcard RBAC permissions provide excessive rights to the Kubernetes API and should be avoided. The principle of least privilege recommends to specify only the set of needed objects and actions
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The security issue here is that the Kubernetes configuration file (kubernetes_vulnerable.yaml) contains a Role or ClusterRole with wildcard permissions. This means that the role has unrestricted access to perform any operation on any resource within the Kubernetes API. <br><br>2. Security Concern:<br>This is a security concern because it violates the principle of least privilege, which states that a user or process should only have the minimum permissions necessary to perform its function. If a role with wildcard permissions is compromised, the attacker would have full control over the Kubernetes API, potentially leading to unauthorized access, data breaches, or other malicious activities.<br><br>3. Secure Code Example:<br>Instead of using wildcard permissions, you should specify the exact permissions that a role requires. For example, if a role only needs to read ConfigMaps, the permissions could be defined as follows:<br><br>```yaml<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: Role<br>metadata:<br>  namespace: default<br>  name: configmap-reader<br>rules:<br>- apiGroups: [""]<br>  resources: ["configmaps"]<br>  verbs: ["get", "watch", "list"]<br>```<br><br>4. Best Practices:<br>To prevent this issue, follow these best practices:<br>- Always follow the principle of least privilege when defining roles and permissions. Only grant the minimum permissions necessary for a role to perform its function.<br>- Regularly review and update roles and permissions to ensure they are still necessary and appropriate.<br>- Use tools to automatically check your Infrastructure as Code (IaC) for security issues, such as misconfigured permissions.<br>- Train developers and administrators on secure coding practices and the importance of following the principle of least privilege.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge high">HIGH</span>
                        <h3 class="finding-title">RBAC Wildcard In Rule</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>137</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Roles and ClusterRoles with wildcard RBAC permissions provide excessive rights to the Kubernetes API and should be avoided. The principle of least privilege recommends to specify only the set of needed objects and actions
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Plain English Explanation:<br>This security issue is related to Role-Based Access Control (RBAC) in Kubernetes. The problem is that the roles and cluster roles are configured with wildcard permissions. This means that the roles have unrestricted access to perform any action on any object within the Kubernetes API, which is not a good practice.<br><br>2. Why it's a security concern:<br>This is a security concern because it violates the principle of least privilege. This principle suggests that a user or process should only have the minimum permissions necessary to perform its tasks. By giving a role wildcard permissions, you're potentially allowing it to perform actions it doesn't need to, which could be exploited if the role's credentials were compromised.<br><br>3. Secure Code Example:<br>Instead of using a wildcard, specify the exact permissions each role needs. For example, if a role only needs to get, list, and watch pods, the RBAC rule would look like this:<br><br>```yaml<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: Role<br>metadata:<br>  namespace: default<br>  name: pod-reader<br>rules:<br>- apiGroups: [""]<br>  resources: ["pods"]<br>  verbs: ["get", "watch", "list"]<br>```<br><br>4. Best Practices:<br>To prevent this issue, follow these best practices:<br><br>- Always follow the principle of least privilege when assigning permissions. Only give a role the permissions it needs to perform its tasks.<br>- Regularly review and update your roles' permissions to ensure they're still appropriate.<br>- Avoid using wildcards in your RBAC rules. Instead, specify the exact resources and verbs a role needs access to.<br>- Use tools to automatically check your IaC for security issues. These can help you catch and fix issues before they become a problem.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge high">HIGH</span>
                        <h3 class="finding-title">RBAC Wildcard In Rule</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>135</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Roles and ClusterRoles with wildcard RBAC permissions provide excessive rights to the Kubernetes API and should be avoided. The principle of least privilege recommends to specify only the set of needed objects and actions
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>This security issue is about a Kubernetes Role or ClusterRole that has been given wildcard permissions. This means that the role has been given access to perform any action on any resource within the Kubernetes API. This is a problem because it violates the principle of least privilege, which states that a user or role should only have the minimum permissions necessary to perform its duties.<br><br>2. Security Concern:<br>The concern here is that if a malicious actor were to gain access to this role, they would have unrestricted access to the Kubernetes API. They could potentially create, delete, or modify any resource, leading to a wide range of possible attacks, including data theft, service disruption, or even taking over the entire cluster.<br><br>3. Secure Code Example:<br>Instead of using a wildcard, specify the exact permissions the role needs. For example:<br><br>```yaml<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: Role<br>metadata:<br>  namespace: default<br>  name: pod-reader<br>rules:<br>- apiGroups: [""]<br>  resources: ["pods"]<br>  verbs: ["get", "watch", "list"]<br>```<br>In this example, the role "pod-reader" can only get, watch, and list pods, and nothing else.<br><br>4. Best Practices:<br>To prevent this issue, follow these best practices:<br><br>- Always follow the principle of least privilege when assigning permissions. Only give a role the permissions it needs to perform its duties, nothing more.<br>- Regularly review and audit your roles and their permissions to ensure they are still appropriate.<br>- Use tools to automatically check your Infrastructure as Code (IaC) for security issues, such as misconfigured permissions.<br>- Educate your team about the importance of proper permission management and the risks associated with overly permissive roles.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge high">HIGH</span>
                        <h3 class="finding-title">RBAC Wildcard In Rule</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>138</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Roles and ClusterRoles with wildcard RBAC permissions provide excessive rights to the Kubernetes API and should be avoided. The principle of least privilege recommends to specify only the set of needed objects and actions
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue refers to the use of wildcard permissions in Role-Based Access Control (RBAC) within a Kubernetes configuration file. Wildcard permissions are indicated by an asterisk (*) and grant all possible permissions. This means that the role or cluster role with this wildcard permission has unrestricted access to the Kubernetes API, which violates the principle of least privilege.<br><br>2. Security Concern: The principle of least privilege states that a user or process should only have the minimum permissions necessary to perform its function. By granting wildcard permissions, you're potentially giving unnecessary and excessive rights, which could be exploited if the role's credentials are compromised. This could lead to unauthorized data access, modification, or even deletion within the Kubernetes cluster.<br><br>3. Secure Code Example: Instead of using wildcard permissions, specify only the necessary permissions. For example, if a role only needs to get, list, and watch pods, the RBAC rule should look like this:<br><br>```yaml<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: Role<br>metadata:<br>  namespace: default<br>  name: pod-reader<br>rules:<br>- apiGroups: [""]<br>  resources: ["pods"]<br>  verbs: ["get", "watch", "list"]<br>```<br><br>4. Best Practices: To prevent this issue, follow these best practices:<br><br>   - Always follow the principle of least privilege when defining roles and permissions.<br>   - Regularly review and update roles and permissions as necessary.<br>   - Avoid using wildcard permissions unless absolutely necessary.<br>   - Use role-based access control (RBAC) to manage permissions.<br>   - Regularly audit your RBAC configurations for potential security issues.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge high">HIGH</span>
                        <h3 class="finding-title">RBAC Wildcard In Rule</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>126</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Roles and ClusterRoles with wildcard RBAC permissions provide excessive rights to the Kubernetes API and should be avoided. The principle of least privilege recommends to specify only the set of needed objects and actions
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue is related to Role-Based Access Control (RBAC) in Kubernetes. The problem is that a role or cluster role has been defined with wildcard permissions, meaning it can access and manipulate any resource within the Kubernetes API. This is against the principle of least privilege, which suggests that a role should only have the minimum permissions necessary to perform its tasks.<br><br>2. Security Concern: This is a significant security concern because if an attacker gains access to a role with wildcard permissions, they could potentially perform any action on any resource within the Kubernetes cluster. This could include deleting resources, modifying configurations, or even creating new resources that could be used to further compromise the system.<br><br>3. Code Fix: Here's an example of how to fix this issue. Instead of using wildcard permissions, specify the exact resources and verbs the role needs:<br><br>```yaml<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: Role<br>metadata:<br>  namespace: default<br>  name: pod-reader<br>rules:<br>- apiGroups: [""]<br>  resources: ["pods"]<br>  verbs: ["get", "watch", "list"]<br>```<br>In this example, the role "pod-reader" can only get, watch, and list pods. It cannot perform any other actions or access any other resources.<br><br>4. Best Practices: To prevent this issue, always follow the principle of least privilege when defining roles and permissions in Kubernetes. Avoid using wildcard permissions whenever possible. Instead, specify the exact resources and verbs each role needs. Regularly review and update your roles and permissions to ensure they are still necessary and appropriate. Use automated tools to scan your Infrastructure as Code (IaC) for security issues and misconfigurations.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge high">HIGH</span>
                        <h3 class="finding-title">RBAC Wildcard In Rule</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>123</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Roles and ClusterRoles with wildcard RBAC permissions provide excessive rights to the Kubernetes API and should be avoided. The principle of least privilege recommends to specify only the set of needed objects and actions
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>This security issue is related to the use of wildcard permissions in Kubernetes' Role-Based Access Control (RBAC). In the identified file, a role or cluster role has been defined with a wildcard (*), which means it grants all permissions. This goes against the principle of least privilege, which recommends only granting the minimum permissions necessary for a task.<br><br>2. Security Concern:<br>The use of wildcard permissions is a security concern because it provides excessive rights to the Kubernetes API. This could potentially allow an attacker to perform unwanted actions if they gain access to the role. It could lead to unauthorized data access, data manipulation, or even a complete system takeover.<br><br>3. Secure Code Example:<br>Instead of using a wildcard, specify the exact permissions required. For instance, if a role only needs to get and list pods, the rule should look like this:<br><br>```yaml<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: Role<br>metadata:<br>  namespace: default<br>  name: pod-reader<br>rules:<br>- apiGroups: [""]<br>  resources: ["pods"]<br>  verbs: ["get", "list"]<br>```<br><br>4. Best Practices:<br>To prevent this issue, follow these best practices:<br><br>- Always adhere to the principle of least privilege. Only grant the minimum permissions necessary for a task.<br>- Regularly review and update roles and permissions as necessary.<br>- Avoid the use of wildcards in RBAC rules. Always specify the exact resources and actions.<br>- Use tools to automatically scan your Infrastructure as Code (IaC) for security issues.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge high">HIGH</span>
                        <h3 class="finding-title">RBAC Wildcard In Rule</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>136</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Roles and ClusterRoles with wildcard RBAC permissions provide excessive rights to the Kubernetes API and should be avoided. The principle of least privilege recommends to specify only the set of needed objects and actions
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The issue "RBAC Wildcard In Rule" refers to a situation where a Role or ClusterRole in Kubernetes has been given wildcard permissions. This means that the role has been granted access to perform any action on any resource within the Kubernetes cluster. This is a violation of the principle of least privilege, which states that a user or process should only have access to the resources necessary for its legitimate purpose.<br><br>2. Security Concern:<br>This is a major security concern because if a malicious user or process gains access to this role, they would have unrestricted access to all resources within the Kubernetes cluster. They could potentially read, modify, or delete any data or even take control of the entire cluster.<br><br>3. Secure Code Example:<br>Instead of using a wildcard, you should specify the exact permissions that a role requires. For example, if a role only needs to read ConfigMaps, the RBAC rule should look like this:<br><br>```yaml<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: Role<br>metadata:<br>  namespace: default<br>  name: configmap-reader<br>rules:<br>- apiGroups: [""]<br>  resources: ["configmaps"]<br>  verbs: ["get", "watch", "list"]<br>```<br><br>4. Best Practices:<br>To prevent this issue, follow these best practices:<br>- Always adhere to the principle of least privilege when assigning permissions. Only grant the minimum permissions necessary for a role to function.<br>- Regularly review and update roles and permissions to ensure they are still necessary and appropriate.<br>- Use tools to automatically scan your Infrastructure as Code for security issues.<br>- Train your team on secure coding practices and the importance of limiting permissions.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge high">HIGH</span>
                        <h3 class="finding-title">RBAC Wildcard In Rule</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>139</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Roles and ClusterRoles with wildcard RBAC permissions provide excessive rights to the Kubernetes API and should be avoided. The principle of least privilege recommends to specify only the set of needed objects and actions
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>This security issue refers to the use of wildcard permissions in Kubernetes Role-Based Access Control (RBAC) roles and cluster roles. In simple terms, it means that a user or a service has been granted overly broad permissions, which could potentially allow them to perform any action on any resource within the Kubernetes API.<br><br>2. Security Concern:<br>This is a security concern because it violates the principle of least privilege, which states that a user or service should only have the minimum permissions necessary to perform their function. By granting wildcard permissions, you're potentially giving malicious actors the ability to perform harmful actions if they gain access to the user or service with these permissions.<br><br>3. Secure Code Example:<br>Instead of using wildcard permissions, you should specify the exact resources and actions that a role needs. For example, if a role only needs to get, list, and watch pods, you would define the role like this:<br><br>```yaml<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: Role<br>metadata:<br>  namespace: default<br>  name: pod-reader<br>rules:<br>- apiGroups: [""]<br>  resources: ["pods"]<br>  verbs: ["get", "watch", "list"]<br>```<br><br>4. Best Practices:<br>To prevent this issue, always follow the principle of least privilege when defining roles and cluster roles in Kubernetes. Avoid using wildcard permissions and instead specify the exact resources and actions that are needed. Regularly review and audit your RBAC configurations to ensure that they are still appropriate and secure. If a role's responsibilities change, update its permissions to match.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge high">HIGH</span>
                        <h3 class="finding-title">RBAC Wildcard In Rule</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>125</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Roles and ClusterRoles with wildcard RBAC permissions provide excessive rights to the Kubernetes API and should be avoided. The principle of least privilege recommends to specify only the set of needed objects and actions
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The security issue "RBAC Wildcard In Rule" refers to the use of wildcard permissions in Kubernetes Role-Based Access Control (RBAC). It means that a role or cluster role in your Kubernetes configuration has been given broad permissions using a wildcard (*), which allows it to access and perform actions on all resources in the Kubernetes API.<br><br>2. Security Concern:<br>This is a major security concern because it violates the principle of least privilege, which states that a user or process should only have access to the resources and actions that are necessary for it to function. By using a wildcard, you're potentially giving unnecessary permissions, which could be exploited if the role's credentials are compromised. This could lead to unauthorized access, data breaches, and other security incidents.<br><br>3. Secure Code Example:<br>Here's an example of how to fix this issue in a Kubernetes RBAC configuration:<br><br>Instead of:<br>```yaml<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: Role<br>metadata:<br>  namespace: default<br>  name: risky-role<br>rules:<br>- apiGroups: ["*"]<br>  resources: ["*"]<br>  verbs: ["*"]<br>```<br>Use this:<br>```yaml<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: Role<br>metadata:<br>  namespace: default<br>  name: secure-role<br>rules:<br>- apiGroups: ["apps"]<br>  resources: ["deployments"]<br>  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]<br>```<br>In the secure example, the role is only given the permissions it needs: it can only perform specific actions (get, list, watch, create, update, patch, delete) on specific resources (deployments) in a specific API group (apps).<br><br>4. Best Practices:<br>To prevent this issue, always follow the principle of least privilege when configuring RBAC in Kubernetes. Only grant the permissions that are absolutely necessary for each role. Avoid using wildcards in your RBAC configurations, and instead specify each required permission individually. Regularly review and update your RBAC configurations to ensure they're still appropriate and secure. Consider using automated tools to scan your Infrastructure as Code for security issues like this one.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge high">HIGH</span>
                        <h3 class="finding-title">Shared Host PID Namespace</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>199</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Container should not share the host process ID namespace
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The security issue "Shared Host PID Namespace" means that a container within a Kubernetes Pod is configured to share the same Process ID (PID) namespace as the host system. This means that the container can see and interact with all processes running on the host system.<br><br>2. Security Concern:<br>This is a security concern because it breaks the isolation between the container and the host system. A process within the container could potentially interfere with host processes, leading to unauthorized access or disruption of the host system. This could lead to privilege escalation, data breaches, or denial of service.<br><br>3. Secure Code Example:<br>To fix this, you need to ensure that the `hostPID` field in the Kubernetes Pod specification is set to `false`. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  hostPID: false<br>  containers:<br>  - name: secure-container<br>    image: secureimage<br>```<br><br>In this example, the `hostPID` field is explicitly set to `false`, ensuring that the container does not share the host's PID namespace.<br><br>4. Best Practices:<br>To prevent this issue, follow these best practices:<br><br>- Always set `hostPID` to `false` unless there's a specific, justified reason to do otherwise.<br>- Regularly review and audit your Kubernetes configurations to ensure they follow security best practices.<br>- Use automated tools to scan your Infrastructure as Code (IaC) for potential security issues.<br>- Limit the privileges of your containers as much as possible to reduce the potential impact of a security breach.<br>- Implement a strong security policy and ensure all team members are aware of and follow this policy.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge high">HIGH</span>
                        <h3 class="finding-title">Workload Mounting With Sensitive OS Directory</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>189</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Workload is mounting a volume with sensitive OS Directory
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue means that a workload (a set of processes running in a container) in a Kubernetes deployment is mounting a volume that is linked to a sensitive operating system directory. This could potentially expose sensitive system data to the workload, which is a security risk.<br><br>2. Security Concern: If a workload has access to sensitive system directories, it could read, modify, or delete critical system files. If the workload is compromised, an attacker could gain access to these directories, leading to a potential system compromise. <br><br>3. Secure Code Example: <br><br>   The issue lies in the volume mount section of the Kubernetes YAML file. Here's an example of how to fix it:<br><br>   ```yaml<br>   apiVersion: v1<br>   kind: Pod<br>   metadata:<br>     name: secure-pod<br>   spec:<br>     containers:<br>     - name: secure-container<br>       image: secureimage<br>       volumeMounts:<br>       - mountPath: /app/data<br>         name: app-data<br>     volumes:<br>     - name: app-data<br>       emptyDir: {}<br>   ```<br><br>   In this example, the workload is only mounting a volume to a non-sensitive directory (`/app/data`), and the volume is an `emptyDir`, which is ephemeral and not linked to any host OS directory.<br><br>4. Best Practices:<br><br>   - Always follow the principle of least privilege. Only grant your workloads access to the resources they need.<br>   - Avoid mounting host OS directories unless absolutely necessary. If you must, ensure they are non-sensitive directories.<br>   - Regularly review your IaC configurations for security issues.<br>   - Use automated tools to scan your IaC for security vulnerabilities.<br>   - Keep your IaC scripts as simple as possible to reduce the potential attack surface.<br>   - Regularly update your IaC scripts to incorporate the latest security best practices.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                </div><div id="medium-section" class="severity-section"><h2 class="severity-header medium">MEDIUM (52)</h2>
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Container Running As Root</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>87</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Containers should only run as non-root user. This limits the exploitability of security misconfigurations and restricts an attacker's possibilities in case of compromise
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The issue "Container Running As Root" means that the container in your Kubernetes configuration is set to run with root (administrator) privileges. This is similar to logging into a computer as the administrator, which gives you full access to all commands and files.<br><br>2. Security Concern:<br>Running containers as root is a security concern because if an attacker manages to exploit a vulnerability within the container, they would have full root access to the container and potentially to the host system. This could allow them to execute arbitrary commands, modify system files, or install malicious software.<br><br>3. Secure Code Example:<br>To fix this issue, you should modify your Kubernetes configuration to run the container as a non-root user. Here is an example of how to do this:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: non-root-pod<br>spec:<br>  securityContext:<br>    runAsUser: 1000  # Non-root user<br>  containers:<br>  - name: non-root-container<br>    image: yourimage<br>```<br><br>In this example, the `runAsUser: 1000` line specifies that the container should run as user ID 1000, which is a non-root user.<br><br>4. Best Practices:<br>To prevent this issue, always follow the principle of least privilege, which means giving each process the minimal permissions it needs to perform its function. Specifically, avoid running containers as root whenever possible. Instead, run them as non-root users or use a security context to drop unnecessary capabilities. Regularly review and update your configurations to ensure they follow best security practices. Use automated tools to scan your Infrastructure as Code (IaC) for security issues.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Container Running As Root</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>13</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Containers should only run as non-root user. This limits the exploitability of security misconfigurations and restricts an attacker's possibilities in case of compromise
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The security issue "Container Running As Root" means that the container in your Kubernetes configuration is set to run as the root user. The root user has full permissions and can perform any operations on the system, which can be dangerous if the container is compromised.<br><br>2. Security Concern:<br>Running a container as root is a security concern because if an attacker gains access to the container, they would have full control over it and could potentially gain access to the host system or other containers. This could lead to data breaches, system damage, or other security incidents.<br><br>3. Secure Code Example:<br>You can fix this issue by specifying a non-root user in your Kubernetes configuration file. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: non-root-pod<br>spec:<br>  securityContext:<br>    runAsUser: 1000  # Non-root user<br>  containers:<br>  - name: non-root-container<br>    image: your-image<br>```<br>In this example, the `runAsUser` field is set to 1000, which is a non-root user.<br><br>4. Best Practices:<br>To prevent this issue, always follow these best practices:<br>- Never run containers as root unless absolutely necessary. Always specify a non-root user in your Kubernetes configuration files.<br>- Regularly review and update your Kubernetes configurations to ensure they follow security best practices.<br>- Use automated tools to scan your Infrastructure as Code (IaC) for security issues.<br>- Educate your team about the risks of running containers as root and the importance of following security best practices.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Container Running As Root</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>181</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Containers should only run as non-root user. This limits the exploitability of security misconfigurations and restricts an attacker's possibilities in case of compromise
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>This security issue means that a container in your Kubernetes configuration is set to run as the root user. This is a problem because if an attacker gains access to the container, they would have root privileges, which could potentially allow them to take over the entire system.<br><br>2. Security Concern:<br>Running a container as root is a security concern because it gives the container unrestricted access to the host system. If an attacker were to compromise the container, they could potentially gain full control over the host system, leading to a serious security breach.<br><br>3. Secure Code Example:<br>You can fix this issue by specifying a non-root user in your Kubernetes configuration file. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: non-root-pod<br>spec:<br>  securityContext:<br>    runAsUser: 1000  # Non-root user<br>  containers:<br>  - name: non-root-container<br>    image: your_image<br>```<br>In this example, `runAsUser: 1000` sets the container to run as a non-root user.<br><br>4. Best Practices:<br>To prevent this issue, always follow these best practices:<br>   - Never run containers as root unless absolutely necessary.<br>   - Always specify a non-root user in your Kubernetes configuration files.<br>   - Regularly review and update your configurations to ensure they follow security best practices.<br>   - Use security tools to automatically check your configurations for potential security issues.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Container Running As Root</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>202</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Containers should only run as non-root user. This limits the exploitability of security misconfigurations and restricts an attacker's possibilities in case of compromise
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue means that the container specified in the Kubernetes configuration file is set to run as the root user. The root user has full administrative privileges, which can pose a security risk if the container is compromised.<br><br>2. Security Concern: Running a container as root means that if an attacker gains access to the container, they would have full control over it. They could potentially exploit this to gain access to other containers or the host system, leading to a serious security breach.<br><br>3. Secure Code Example: To fix this issue, you should specify a non-root user in your Kubernetes configuration file. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: non-root-pod<br>spec:<br>  securityContext:<br>    runAsUser: 1000  # Specify a non-root user here<br>  containers:<br>  - name: non-root-container<br>    image: someimage<br>```<br><br>In this example, the `runAsUser` field in the `securityContext` specifies that the container should run as user 1000, which is a non-root user.<br><br>4. Best Practices: To prevent this issue, always run containers as a non-root user whenever possible. This can be done by setting the `runAsUser` field in the `securityContext` of your Kubernetes configuration files. Additionally, regularly review your configuration files to ensure that no containers are accidentally set to run as root. Consider using automated tools to scan your Infrastructure as Code (IaC) for security vulnerabilities.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Container Running With Low UID</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>181</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Check if containers are running with low UID, which might cause conflicts with the host's user table.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>This security issue means that the container in your Kubernetes configuration file is running with a low User ID (UID). In Unix-like systems, UIDs are used to identify users and assign system resources. Low UIDs (typically between 0 and 1000) are usually reserved for system accounts and services. Running a container with a low UID might cause conflicts with the host's user table.<br><br>2. Security Concern:<br>Running a container with a low UID is a security concern because it can potentially give the container more privileges than it should have. If a container is compromised, an attacker could exploit these additional privileges to gain unauthorized access to system resources or perform malicious actions.<br><br>3. Secure Code Example:<br>To fix this issue, you should assign a high UID to your container. Here's an example of how to do this in a Kubernetes configuration file:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  securityContext:<br>    runAsUser: 10001  # Use a high UID<br>  containers:<br>  - name: secure-container<br>    image: secure-image<br>```<br><br>In this example, the `runAsUser` field is used to specify the UID that the container should run as. The value `10001` is an example of a high UID.<br><br>4. Best Practices:<br>To prevent this issue, follow these best practices:<br><br>- Always assign a high UID to your containers. Avoid using low UIDs that are reserved for system accounts and services.<br>- Regularly review your Kubernetes configuration files to ensure that they follow security best practices.<br>- Use automated tools to scan your Infrastructure as Code (IaC) for security issues. These tools can help you catch potential security issues before they become a problem.<br>- Implement a least privilege policy. This means giving your containers only the permissions they need to perform their tasks. This can help limit the potential damage if a container is compromised.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Container Running With Low UID</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>202</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Check if containers are running with low UID, which might cause conflicts with the host's user table.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: <br>In Unix-based systems, every user is assigned a unique user ID (UID). The lower the UID, the more privileges the user has. The root user typically has a UID of 0. In this case, the Kubernetes container is running with a low UID. This means that the process running inside the container is operating with high privileges, potentially equivalent to the root user on the host system.<br><br>2. Security Concern: <br>Running containers with low UIDs is a security concern because if an attacker manages to escape the container, they could potentially gain root-level access to the host system. This could lead to a variety of malicious activities, such as data theft, system damage, or using the system to launch further attacks.<br><br>3. Secure Code Example: <br>To fix this, you should assign a non-root UID to the container. Here's how you can do it in your Kubernetes YAML file:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: non-root-pod<br>spec:<br>  securityContext:<br>    runAsUser: 1000  # Set the UID here<br>  containers:<br>  - name: non-root-container<br>    image: your_image<br>```<br><br>In this example, the container is set to run as user with UID 1000, which is a non-root user.<br><br>4. Best Practices: <br>- Always run your containers as a non-root user whenever possible.<br>- Regularly check your IaC configurations for security vulnerabilities using automated tools.<br>- Follow the principle of least privilege, i.e., only provide the minimum necessary permissions to each user or process.<br>- Keep your systems and software updated to protect against known vulnerabilities.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Container Running With Low UID</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>87</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Check if containers are running with low UID, which might cause conflicts with the host's user table.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>This security issue indicates that a container in your Kubernetes configuration is running with a low User ID (UID). In Unix-based systems, UIDs are used to identify users, and lower UIDs are typically reserved for system users. If a container runs with a low UID, it could potentially conflict with the host's user table.<br><br>2. Security Concern:<br>Running a container with a low UID is a security concern because it might grant the container more privileges than it needs. This could potentially lead to unauthorized access or manipulation of system files, or even allow an attacker to gain control over the system.<br><br>3. Secure Code Example:<br>To fix this issue, you should assign a higher UID to the container. Here's an example of how to do this in your Kubernetes configuration file:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: non-root-pod<br>spec:<br>  securityContext:<br>    runAsUser: 1000  # Non-root user<br>  containers:<br>  - name: non-root-container<br>    image: your_image<br>```<br><br>In this example, `runAsUser: 1000` sets the UID of the container to 1000, which is a non-system user.<br><br>4. Best Practices:<br>To prevent this issue, follow these best practices:<br><br>- Always run containers with the least privileges necessary. Avoid running containers as root or with a low UID.<br>- Regularly review your Infrastructure as Code (IaC) configurations for security issues.<br>- Use automated tools to scan your IaC for security issues.<br>- Educate your team about the importance of container security and the risks of running containers with low UIDs.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Container Running With Low UID</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>13</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Check if containers are running with low UID, which might cause conflicts with the host's user table.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>This security issue refers to a situation where a container in a Kubernetes environment is running with a low User ID (UID). User IDs are used in Unix-like systems to identify users and manage their permissions. A low UID typically refers to a UID below 1000, which are usually reserved for system accounts and services.<br><br>2. Security Concern:<br>Running a container with a low UID can lead to conflicts with the host's user table. This can potentially allow the container to access system resources or perform actions that it shouldn't be able to, leading to security vulnerabilities. If a malicious user gains control of the container, they could potentially exploit this to gain unauthorized access to system resources.<br><br>3. Secure Code Example:<br>To fix this issue, you should assign a higher UID to the container. In your Kubernetes configuration file, you can do this using the `runAsUser` security context. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: my-pod<br>spec:<br>  securityContext:<br>    runAsUser: 10000  # Non-root user<br>  containers:<br>  - name: my-container<br>    image: my-image<br>```<br><br>In this example, the `runAsUser` field is set to 10000, which is a non-system, non-root user.<br><br>4. Best Practices:<br>To prevent this issue, always ensure that containers are not running with low UIDs. Avoid using root (UID 0) or any other system UIDs for your containers. Instead, use higher UIDs that are not reserved by the system. You can enforce this using Pod Security Policies in Kubernetes, which can restrict the UIDs that can be used by containers. Regularly review and update your Infrastructure as Code (IaC) configurations to ensure they follow these best practices.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Containers With Added Capabilities</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>25</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Containers should not have extra capabilities allowed
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue is about the configuration of a container in a Kubernetes deployment. The container has been given additional capabilities beyond the default set. In Linux, capabilities are a set of privileges that can be used to perform system-level operations. By default, containers are given a minimal set of capabilities to limit their potential to cause harm. If a container has extra capabilities, it means it has more permissions than it should, which can be a security risk.<br><br>2. Security Concern: The concern here is that if a container has extra capabilities, it could potentially be exploited by an attacker to perform unauthorized actions. For example, if a container has the CAP_NET_RAW capability, it can create raw network packets, which could be used in a network attack. If the container was compromised, the attacker would have these extra capabilities at their disposal.<br><br>3. Secure Code Example: To fix this issue, you need to remove the extra capabilities from the container's security context. Here's an example of how to do this in a Kubernetes YAML file:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  containers:<br>  - name: secure-container<br>    image: secureimage<br>    securityContext:<br>      capabilities:<br>        drop: ["ALL"]<br>```<br><br>In this example, the `drop: ["ALL"]` directive removes all extra capabilities from the container.<br><br>4. Best Practices: To prevent this issue, follow these best practices:<br><br>   - Always follow the principle of least privilege. Only give containers the capabilities they need to perform their function, and no more.<br>   <br>   - Regularly review your container configurations to ensure they're secure. Use automated tools to help with this.<br>   <br>   - Educate your team about the risks of giving containers extra capabilities. Make sure everyone understands the security implications.<br>   <br>   - Use a container runtime that supports security features like seccomp, AppArmor, and SELinux. These can provide additional layers of security.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Memory Limits Not Defined</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>202</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Memory limits should be defined for each container. This prevents potential resource exhaustion by ensuring that containers consume not more than the designated amount of memory
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: <br>This security issue means that the Kubernetes configuration file doesn't specify a limit on how much memory each container can use. Without a limit, a container could potentially use up all available memory, which can cause problems for other containers or applications running on the same system.<br><br>2. Security Concern: <br>This is a security concern because an attacker could exploit this to cause a Denial of Service (DoS) attack. They could run a process within the container that consumes a large amount of memory, causing other containers or applications to crash due to lack of memory. <br><br>3. Secure Code Example: <br>To fix this issue, you need to define memory limits in your Kubernetes configuration file. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: memory-demo<br>spec:<br>  containers:<br>  - name: memory-demo-ctr<br>    image: v1<br>    resources:<br>      limits:<br>        memory: "200Mi"<br>```<br>In this example, the `memory-demo-ctr` container is limited to using 200 MiB of memory.<br><br>4. Best Practices: <br>To prevent this issue, always define resource limits for each container in your Kubernetes configuration files. This includes not just memory, but also CPU and other resources. Make sure to monitor your containers' resource usage to determine appropriate limits. Also, consider using Kubernetes namespaces to isolate resources between different projects or teams.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Memory Limits Not Defined</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>17</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Memory limits should be defined for each container. This prevents potential resource exhaustion by ensuring that containers consume not more than the designated amount of memory
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>This security issue means that in the Kubernetes configuration file, there are no memory limits set for a container. This means that a container can potentially use up all the available memory on the system, which can lead to resource exhaustion.<br><br>2. Security Concern:<br>This is a security concern because if a container uses up all the system's memory, it can cause other containers or even the entire system to crash. This can lead to service disruption and potential data loss. It can also be exploited by malicious users or software to perform a Denial of Service (DoS) attack.<br><br>3. Secure Code Example:<br>To fix this issue, you need to define memory limits for each container in the Kubernetes configuration file. Here is an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: memory-demo<br>spec:<br>  containers:<br>  - name: memory-demo-ctr<br>    image: polinux/stress<br>    resources:<br>      limits:<br>        memory: "200Mi"<br>      requests:<br>        memory: "100Mi"<br>```<br><br>In this example, the container `memory-demo-ctr` has a memory limit of 200Mi and a memory request of 100Mi.<br><br>4. Best Practices:<br>To prevent this issue, always define resource limits for each container in your Kubernetes configuration files. This includes not only memory, but also CPU and other resources. Use the `requests` and `limits` parameters to specify the minimum and maximum amount of each resource that a container can use. Also, monitor your containers' resource usage to adjust these limits as needed.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Memory Limits Not Defined</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>181</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Memory limits should be defined for each container. This prevents potential resource exhaustion by ensuring that containers consume not more than the designated amount of memory
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>This security issue means that the Kubernetes configuration file (kubernetes_vulnerable.yaml) does not have memory limits set for a container. Without these limits, a container could potentially use up all available memory on the system, which could negatively impact other processes or services.<br><br>2. Security Concern:<br>This is a security concern because an attacker could exploit this by running a process within the container that consumes an excessive amount of memory, causing other containers or even the host system to run out of memory. This could lead to service disruptions, performance degradation, or even system crashes, making it a potential Denial of Service (DoS) attack vector.<br><br>3. Secure Code Example:<br>To fix this issue, you need to define memory limits in the Kubernetes configuration file for each container. Here is an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: memory-limited-pod<br>spec:<br>  containers:<br>  - name: memory-limited-container<br>    image: example-image<br>    resources:<br>      limits:<br>        memory: "200Mi"<br>```<br>In this example, the container 'memory-limited-container' has a memory limit of 200 MiB.<br><br>4. Best Practices:<br>To prevent this issue, always define resource limits (both CPU and memory) for your containers in your Kubernetes configuration files. This not only helps in preventing potential DoS attacks but also ensures that your containers are not consuming more resources than they need. <br><br>Additionally, regularly review and adjust these limits based on the actual resource usage of your containers to ensure they are set at an optimal level. Use tools like Kubernetes Metrics Server or Prometheus to monitor your containers' resource usage. <br><br>Finally, consider implementing ResourceQuotas and LimitRanges in your Kubernetes namespaces to enforce resource usage policies and prevent the creation of pods that do not meet these policies.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Memory Limits Not Defined</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>87</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Memory limits should be defined for each container. This prevents potential resource exhaustion by ensuring that containers consume not more than the designated amount of memory
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>This security issue means that there are no defined memory limits for a container in the Kubernetes configuration file. Without these limits, a container could potentially use up all available memory on the host machine, causing other containers or the host itself to run out of memory.<br><br>2. Security Concern:<br>This is a security concern because an attacker could exploit this to cause a Denial of Service (DoS) attack. By running a process that consumes a large amount of memory in the container, they could exhaust the host's memory, causing other services to fail.<br><br>3. Secure Code Example:<br>You can fix this issue by defining memory limits in the Kubernetes configuration file. Here is an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: memory-limited-pod<br>spec:<br>  containers:<br>  - name: memory-limited-container<br>    image: example<br>    resources:<br>      limits:<br>        memory: "200Mi"<br>```<br>In this example, the container `memory-limited-container` has a memory limit of 200Mi (200 Megabytes).<br><br>4. Best Practices:<br>To prevent this issue, always define resource limits for your containers. This includes not only memory, but also CPU and other resources. This will ensure that no single container can consume all resources on the host. Additionally, monitor your containers' resource usage to adjust the limits as necessary.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Memory Requests Not Defined</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>17</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Memory requests should be defined for each container. This allows the kubelet to reserve the requested amount of system resources and prevents over-provisioning on individual nodes
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue means that in the Kubernetes configuration file, there are containers that do not have a specified memory request. A memory request is a way to tell the Kubernetes system how much memory a specific container should be allocated. Without this, the system will not reserve any specific amount of memory for the container.<br><br>2. Security Concern: This is a security concern because if a container doesn't have a defined memory request, it could potentially use up all available memory on a node. This could lead to other services on the node being starved of resources, causing them to fail or perform poorly. It could also be exploited by an attacker to cause a denial of service by forcing the container to consume excessive memory.<br><br>3. Secure Code Example: Here's how you can define a memory request for a container in a Kubernetes configuration file:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: my-pod<br>spec:<br>  containers:<br>  - name: my-container<br>    image: my-image<br>    resources:<br>      requests:<br>        memory: "64Mi"<br>      limits:<br>        memory: "128Mi"<br>```<br>In this example, the container `my-container` has a memory request of 64Mi and a memory limit of 128Mi.<br><br>4. Best Practices: To prevent this issue, always define memory requests for all containers in your Kubernetes configuration files. This will ensure that each container gets a fair share of memory resources. Also, consider defining memory limits to prevent containers from using more memory than they need. Regularly review your memory requests and limits to ensure they are still appropriate for your workloads. Use automated tools to scan your Infrastructure as Code (IaC) for missing or inappropriate resource requests and limits.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Memory Requests Not Defined</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>87</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Memory requests should be defined for each container. This allows the kubelet to reserve the requested amount of system resources and prevents over-provisioning on individual nodes
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue means that in the Kubernetes configuration file, there are no memory requests defined for a container. In Kubernetes, memory requests are used to tell the system how much memory a container is expected to use, which allows the system to manage resources more efficiently.<br><br>2. Security Concern: If memory requests are not defined, the system cannot properly allocate resources, which can lead to over-provisioning. Over-provisioning can cause a node to run out of resources, causing applications to crash or slow down. It could also allow a single container to consume more resources than it should, potentially affecting other containers running on the same node.<br><br>3. Secure Code Example:<br>   Here is an example of how to define memory requests in a Kubernetes configuration file:<br>   ```<br>   apiVersion: v1<br>   kind: Pod<br>   metadata:<br>     name: memory-demo<br>   spec:<br>     containers:<br>     - name: memory-demo-ctr<br>       image: v1<br>       resources:<br>         requests:<br>           memory: "64Mi"<br>         limits:<br>           memory: "128Mi"<br>   ```<br>   In this example, the container `memory-demo-ctr` requests 64Mi of memory and is limited to use up to 128Mi.<br><br>4. Best Practices: To prevent this issue, always define memory requests and limits for each container in your Kubernetes configuration files. This helps ensure that each container gets the resources it needs and prevents any single container from consuming more resources than it should. Also, regularly review and adjust these settings based on the actual resource usage of your containers.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Memory Requests Not Defined</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>181</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Memory requests should be defined for each container. This allows the kubelet to reserve the requested amount of system resources and prevents over-provisioning on individual nodes
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: <br>This security issue means that in the Kubernetes configuration file, there is a container that does not have a defined memory request. A memory request in Kubernetes is a way of specifying the minimum amount of memory that a container needs to run. Without this, the Kubernetes system doesn't know how much memory to allocate to the container, which could lead to over-provisioning on individual nodes.<br><br>2. Security Concern: <br>The absence of memory requests can lead to resource exhaustion, where one or more containers consume more resources than they should, leaving little or no resources for other containers. This can cause other applications to crash or perform poorly, and in worst-case scenarios, it can lead to denial of service. <br><br>3. Secure Code Example: <br>Here's how to define memory requests in your Kubernetes configuration:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: memory-demo<br>spec:<br>  containers:<br>  - name: memory-demo-ctr<br>    image: images.example.com/memory-demo<br>    resources:<br>      requests:<br>        memory: "64Mi"<br>      limits:<br>        memory: "128Mi"<br>```<br>In this example, we've set a memory request of 64Mi and a limit of 128Mi for the container. This means that the container is guaranteed to get at least 64Mi of memory, but it won't use more than 128Mi.<br><br>4. Best Practices: <br>- Always define resource requests and limits for your containers. This includes not just memory, but also CPU and other resources.<br>- Regularly review and adjust these settings based on the actual resource usage of your containers.<br>- Use Kubernetes namespaces to isolate resources between different projects or teams.<br>- Monitor your cluster's resource usage to identify and address any resource contention issues.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Memory Requests Not Defined</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>202</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Memory requests should be defined for each container. This allows the kubelet to reserve the requested amount of system resources and prevents over-provisioning on individual nodes
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>This security issue means that in the Kubernetes configuration file (kubernetes_vulnerable.yaml), there is a container that does not have a memory request defined. A memory request is a specification that tells Kubernetes how much memory a container needs to run. Without this, Kubernetes cannot properly manage the system resources, which could lead to over-provisioning.<br><br>2. Security Concern:<br>Over-provisioning can lead to resource exhaustion, where a node runs out of resources because too many containers are running on it. This can cause applications to crash or become unresponsive, and it can even affect the entire system if it becomes too overloaded. Additionally, an attacker could potentially exploit this by creating containers that consume a lot of memory, causing a denial-of-service (DoS) attack.<br><br>3. Secure Code Example:<br>To fix this issue, you need to define the memory request for each container in the Kubernetes configuration file. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: my-pod<br>spec:<br>  containers:<br>  - name: my-container<br>    image: my-image<br>    resources:<br>      requests:<br>        memory: "64Mi"<br>      limits:<br>        memory: "128Mi"<br>```<br>In this example, the container `my-container` requests 64Mi of memory and has a limit of 128Mi.<br><br>4. Best Practices:<br>- Always define resource requests and limits for each container in your Kubernetes configuration files. This helps Kubernetes manage system resources more effectively and prevents over-provisioning.<br>- Regularly review and update your resource requests and limits based on the actual usage of your applications. This can help you optimize resource usage and prevent resource exhaustion.<br>- Use Kubernetes namespaces to isolate resources between different projects or teams. This can prevent one team or project from consuming all the resources.<br>- Use monitoring and alerting tools to keep track of your resource usage and get notified when resources are running low. This can help you prevent resource exhaustion before it happens.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">NET_RAW Capabilities Not Being Dropped</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>87</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Containers should drop 'ALL' or at least 'NET_RAW' capabilities
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: In Kubernetes, capabilities are optional security settings that can be added or removed from a container to either enhance or limit its access to certain system calls or features. The 'NET_RAW' capability allows a container to use raw sockets, which bypasses the TCP/IP stack and can be used to craft custom network packets. If this capability is not dropped, it means the container has the potential to misuse this feature.<br><br>2. Security Concern: If a container is compromised, the 'NET_RAW' capability could be exploited by an attacker to craft malicious network packets, launch denial-of-service attacks, or perform network snooping. This could lead to data breaches or disruptions in service.<br><br>3. Secure Code Example: In your Kubernetes YAML file, you should drop the 'NET_RAW' capability as shown below:<br><br>   ```yaml<br>   apiVersion: v1<br>   kind: Pod<br>   metadata:<br>     name: secure-pod<br>   spec:<br>     containers:<br>     - name: secure-container<br>       image: secureimage<br>       securityContext:<br>         capabilities:<br>           drop:<br>           - ALL<br>           add:<br>           - "NET_ADMIN"<br>   ```<br><br>   In this example, we drop all capabilities and only add back the ones we need. If 'NET_ADMIN' is not needed, it should be removed.<br><br>4. Best Practices: To prevent this issue, follow these best practices:<br><br>   - Always drop all capabilities by default and only add back the ones that are absolutely necessary for your application to function.<br>   - Regularly review your container capabilities to ensure they align with the principle of least privilege.<br>   - Use automated tools to scan your Infrastructure as Code (IaC) for security vulnerabilities.<br>   - Educate your development team about the importance of container security and the risks associated with unnecessary capabilities.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">NET_RAW Capabilities Not Being Dropped</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>181</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Containers should drop 'ALL' or at least 'NET_RAW' capabilities
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. **Plain English Explanation:** In Kubernetes, a capability is a type of security setting that allows a process to perform certain actions that are normally restricted. The 'NET_RAW' capability allows a process to use network protocols that require raw sockets, like ICMP (Internet Control Message Protocol). The finding suggests that your Kubernetes configuration does not drop the 'NET_RAW' capability, which means that your containers have more privileges than they should.<br><br>2. **Why it's a Security Concern:** If an attacker manages to exploit a vulnerability within your container, they could use the 'NET_RAW' capability to perform malicious activities, such as creating raw packets to attack other systems, spoofing network traffic, or eavesdropping on network communications. This could lead to data breaches or service disruptions.<br><br>3. **Secure Code Example:** To fix this issue, you need to explicitly drop the 'NET_RAW' capability in your Kubernetes configuration. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  containers:<br>  - name: secure-container<br>    image: secureimage<br>    securityContext:<br>      capabilities:<br>        drop:<br>        - ALL<br>        add:<br>        - "NET_BIND_SERVICE"<br>```<br><br>In this example, we're dropping all capabilities and then only adding back the ones we need (in this case, 'NET_BIND_SERVICE').<br><br>4. **Best Practices:** To prevent this issue, follow these best practices:<br><br>   - Always drop all capabilities by default and only add the ones you need.<br>   - Regularly review your Kubernetes configurations and remove unnecessary capabilities.<br>   - Use automated tools to scan your Infrastructure as Code (IaC) for security issues.<br>   - Implement least privilege principle, i.e., only grant the permissions necessary for a task.<br>   - Keep your Kubernetes environment and all your applications up-to-date to minimize vulnerabilities.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">NET_RAW Capabilities Not Being Dropped</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>24</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Containers should drop 'ALL' or at least 'NET_RAW' capabilities
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The issue is that the Kubernetes configuration file (kubernetes_vulnerable.yaml) does not drop the 'NET_RAW' capability for the containers it deploys. The 'NET_RAW' capability allows a program to use network protocols that require raw sockets, such as ICMP (Internet Control Message Protocol). If a container has this capability, it can potentially send any kind of network traffic it wants, which can be a security risk.<br><br>2. Security Concern:<br>If a malicious actor gains control of a container with 'NET_RAW' capability, they can craft raw packets to perform attacks such as IP spoofing, ARP spoofing, or other network-level attacks. This can lead to data breaches, service disruptions, or other serious security incidents.<br><br>3. Secure Code Example:<br>To fix this issue, you need to explicitly drop the 'NET_RAW' capability in your Kubernetes configuration file. Here's an example of how to do it:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  containers:<br>  - name: secure-container<br>    image: my-image<br>    securityContext:<br>      capabilities:<br>        drop:<br>        - ALL<br>        add:<br>        - "NET_BIND_SERVICE"<br>```<br>In this example, we're dropping all capabilities and then only adding back the 'NET_BIND_SERVICE' capability, which allows the container to bind to low-numbered ports.<br><br>4. Best Practices:<br>- Always drop 'ALL' capabilities and then only add back the ones you need. This follows the principle of least privilege, which states that a process should only have the minimum permissions it needs to function.<br>- Regularly review your Kubernetes configuration files to ensure they follow best practices for security.<br>- Use automated tools to scan your Infrastructure as Code (IaC) for security issues. These tools can catch issues like this one before they make it into production.<br>- Educate your team about the importance of container security and the potential risks associated with capabilities like 'NET_RAW'.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">NET_RAW Capabilities Not Being Dropped</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>202</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Containers should drop 'ALL' or at least 'NET_RAW' capabilities
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The issue is that the Kubernetes configuration file does not indicate that the 'NET_RAW' capability should be dropped for the containers it creates. 'NET_RAW' is a Linux capability that allows applications to create raw network packets. When a container has this capability, it can potentially craft malicious network packets.<br><br>2. Security Concern:<br>If a container is compromised and it has the 'NET_RAW' capability, an attacker can use it to create raw packets to perform network attacks, such as IP spoofing, which can lead to unauthorized access or data loss. It's a best practice to run containers with the least privileges necessary, which often means dropping all capabilities and then only adding back the ones that are needed.<br><br>3. Secure Code Example:<br>To fix this issue, you need to add the 'drop' field under 'capabilities' in the securityContext of the container specification and specify 'NET_RAW' or 'ALL'. Here is an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  containers:<br>  - name: secure-container<br>    image: my_image<br>    securityContext:<br>      capabilities:<br>        drop:<br>        - ALL<br>```<br><br>In this example, 'ALL' capabilities are dropped, which is the most secure configuration. If you know that your container needs certain capabilities, you can add them back under 'add' field.<br><br>4. Best Practices:<br>- Always drop 'ALL' capabilities by default and only add back the ones that are necessary for your application to function.<br>- Regularly review the capabilities your containers are running with and remove any that are not needed.<br>- Use automated tools to scan your Infrastructure as Code (IaC) for security issues.<br>- Train your developers on secure coding practices for containerized applications.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Permissive Access to Create Pods</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>126</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> The permission to create pods in a cluster should be restricted because it allows privilege escalation.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: <br>This security issue means that the current configuration in the Kubernetes YAML file allows any user or service to create new pods in the cluster. Pods are the smallest deployable units of computing that can be created and managed in Kubernetes. <br><br>2. Security Concern: <br>This is a security concern because if an attacker gains access to your Kubernetes cluster, they can exploit this permission to create new pods. These pods can be configured to run malicious code or to have escalated privileges, which could lead to unauthorized access to sensitive data or disruption of your services.<br><br>3. Secure Code Example: <br>To fix this, you should restrict the `create` permission to specific roles or users. Here's an example:<br><br>```yaml<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: Role<br>metadata:<br>  namespace: default<br>  name: pod-creator<br>rules:<br>- apiGroups: [""]<br>  resources: ["pods"]<br>  verbs: ["get", "list", "watch"]<br>---<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: RoleBinding<br>metadata:<br>  name: pod-creator-binding<br>  namespace: default<br>subjects:<br>- kind: User<br>  name: trusted-user<br>  apiGroup: rbac.authorization.k8s.io<br>roleRef:<br>  kind: Role<br>  name: pod-creator<br>  apiGroup: rbac.authorization.k8s.io<br>```<br>In this example, only the `trusted-user` is allowed to `get`, `list`, and `watch` pods, but not create them.<br><br>4. Best Practices: <br>To prevent this issue, follow these best practices:<br><br>- Principle of Least Privilege: Only grant the minimum permissions necessary for a user or service to function properly.<br>- Regular Audits: Regularly review and audit your permissions to ensure they are still necessary and appropriate.<br>- Role-Based Access Control (RBAC): Use RBAC to manage permissions in your Kubernetes cluster. This allows you to assign permissions to specific roles and then assign those roles to users or services.<br>- Security Policies: Implement security policies that restrict the creation of resources in your cluster.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#privilege-escalation-prevention-and-bootstrapping" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Permissive Access to Create Pods</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>136</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> The permission to create pods in a cluster should be restricted because it allows privilege escalation.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>This security issue refers to the fact that the Kubernetes configuration file ('kubernetes_vulnerable.yaml') is allowing overly permissive access to create pods in a cluster. This means that any user or service with access to the Kubernetes API can create new pods, potentially allowing them to run arbitrary code within the cluster.<br><br>2. Security Concern:<br>This is a security concern because it can lead to privilege escalation. If an attacker gains access to the Kubernetes API, they can create a new pod running a malicious container, potentially gaining access to sensitive data or disrupting the operation of other pods in the cluster. This could lead to data breaches or service outages.<br><br>3. Secure Code Example:<br>To fix this issue, you should restrict the `create` permissions to specific roles or users. Here is an example of a Kubernetes Role and RoleBinding that grants `create` permissions on pods only to a specific service account:<br><br>```yaml<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: Role<br>metadata:<br>  namespace: default<br>  name: pod-creator<br>rules:<br>- apiGroups: [""]<br>  resources: ["pods"]<br>  verbs: ["create"]<br><br>---<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: RoleBinding<br>metadata:<br>  name: pod-creator-binding<br>  namespace: default<br>subjects:<br>- kind: ServiceAccount<br>  name: my-service-account<br>  namespace: default<br>roleRef:<br>  kind: Role<br>  name: pod-creator<br>  apiGroup: rbac.authorization.k8s.io<br>```<br><br>4. Best Practices:<br>To prevent this issue, follow these best practices:<br><br>- Principle of Least Privilege: Only grant the minimum permissions necessary for a user or service to perform its function.<br>- Regular Audits: Regularly review and audit your Kubernetes RBAC configurations to ensure they are still appropriate and secure.<br>- Use Namespaces: Use Kubernetes namespaces to isolate different applications or environments, and apply RBAC rules at the namespace level.<br>- Service Accounts: Use service accounts for applications that need to interact with the Kubernetes API, and apply RBAC rules to these service accounts.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#privilege-escalation-prevention-and-bootstrapping" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Permissive Access to Create Pods</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>139</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> The permission to create pods in a cluster should be restricted because it allows privilege escalation.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>This security issue means that the Kubernetes configuration file (kubernetes_vulnerable.yaml) at line 139 is granting overly broad permissions to create pods in the cluster. This could potentially allow unauthorized users or malicious actors to escalate their privileges within the Kubernetes cluster.<br><br>2. Security Concern:<br>The concern here is that if an attacker gains access to the cluster, they could create a pod with escalated privileges, potentially gaining full control over the cluster. This could lead to unauthorized access to sensitive data, disruption of services, or even the deployment of malicious software within the cluster.<br><br>3. Secure Code Example:<br>To fix this issue, you should restrict the permissions to create pods to only the necessary service accounts or users. Here's an example of how you might do this in Kubernetes using Role-Based Access Control (RBAC):<br><br>```yaml<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: Role<br>metadata:<br>  namespace: default<br>  name: pod-creator<br>rules:<br>- apiGroups: [""]<br>  resources: ["pods"]<br>  verbs: ["create"]<br>---<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: RoleBinding<br>metadata:<br>  name: pod-creator-binding<br>  namespace: default<br>subjects:<br>- kind: ServiceAccount<br>  name: my-service-account<br>  namespace: default<br>roleRef:<br>  kind: Role<br>  name: pod-creator<br>  apiGroup: rbac.authorization.k8s.io<br>```<br>In this example, only the `my-service-account` service account in the `default` namespace has the permission to create pods.<br><br>4. Best Practices:<br>To prevent this issue, follow these best practices:<br>- Principle of Least Privilege: Always grant the minimum permissions necessary for a user or service account to perform its function.<br>- Regular Audits: Regularly review and audit your Kubernetes RBAC configurations to ensure they are still appropriate and secure.<br>- Segregation of Duties: Separate duties among different roles and service accounts to limit the impact of a potential breach.<br>- Use Namespaces: Use Kubernetes namespaces to isolate different applications or environments, reducing the scope of permissions needed for each role or service account.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#privilege-escalation-prevention-and-bootstrapping" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Permissive Access to Create Pods</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>123</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> The permission to create pods in a cluster should be restricted because it allows privilege escalation.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The security issue 'Permissive Access to Create Pods' refers to the situation where too many users or services have the ability to create pods in a Kubernetes cluster. A pod is the smallest and simplest unit in the Kubernetes object model that you create or deploy. If too many entities have the ability to create pods, it can lead to unauthorized access or misuse of the cluster.<br><br>2. Security Concern:<br>This is a security concern because if an attacker gains access to a user or service with permissions to create pods, they can potentially escalate their privileges. They could create a pod that has more permissions than they should have, giving them the ability to perform actions they should not be able to do. This could lead to data breaches, unauthorized access to sensitive information, or disruption of the service.<br><br>3. Secure Code Example:<br>To fix this, you should restrict the ability to create pods to only those users or services that absolutely need it. This can be done by using Kubernetes Role-Based Access Control (RBAC). Here is an example of a Role and RoleBinding that only allows a specific service account to create pods:<br><br>```yaml<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: Role<br>metadata:<br>  namespace: default<br>  name: pod-creator<br>rules:<br>- apiGroups: [""]<br>  resources: ["pods"]<br>  verbs: ["create"]<br><br>---<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: RoleBinding<br>metadata:<br>  name: pod-creator-binding<br>  namespace: default<br>subjects:<br>- kind: ServiceAccount<br>  name: my-service-account<br>  namespace: default<br>roleRef:<br>  kind: Role<br>  name: pod-creator<br>  apiGroup: rbac.authorization.k8s.io<br>```<br><br>4. Best Practices:<br>To prevent this issue, follow these best practices:<br>- Always follow the principle of least privilege. Only give users or services the permissions they need to perform their tasks.<br>- Regularly review and audit your permissions. Remove any permissions that are no longer needed.<br>- Use Role-Based Access Control (RBAC) to manage permissions in your Kubernetes cluster.<br>- Monitor your cluster for any unusual or suspicious activity.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#privilege-escalation-prevention-and-bootstrapping" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Readiness Probe Is Not Configured</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>181</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Check if Readiness Probe is not configured.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: <br>A Readiness Probe in Kubernetes is a diagnostic performed periodically by the kubelet on a container. If the probe fails, the kubelet doesn't send traffic to the pod until it passes. The issue here is that the Readiness Probe is not configured for a container in your Kubernetes deployment. This means that Kubernetes has no way of knowing whether the application in your container is ready to serve requests or not.<br><br>2. Security Concern: <br>The absence of a readiness probe can lead to service disruption and potential security issues. If a container isn't ready to serve requests but is receiving traffic, it could lead to errors or crashes, which could be exploited by malicious actors. It could also lead to a denial of service if the application crashes under load because it wasn't ready to receive traffic.<br><br>3. Secure Code Example: <br>You can fix this issue by adding a readiness probe to your Kubernetes configuration. Here's an example of how to do this in a Kubernetes YAML file:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: readiness-pod<br>spec:<br>  containers:<br>  - name: readiness-container<br>    image: k8s.gcr.io/busybox<br>    readinessProbe:<br>      exec:<br>        command:<br>        - cat<br>        - /tmp/healthy<br>      initialDelaySeconds: 5<br>      periodSeconds: 5<br>```<br>In this example, the readiness probe executes the command `cat /tmp/healthy`. If the command succeeds, the container is considered ready. If it fails, the container is not ready.<br><br>4. Best Practices: <br>To prevent this issue, always configure readiness probes for your containers in Kubernetes. Ensure that the readiness probe accurately reflects whether your application is ready to serve requests. Also, make sure to set appropriate `initialDelaySeconds` and `periodSeconds` values to avoid unnecessary load or false negatives. Regularly review your Kubernetes configuration for missing or misconfigured readiness probes.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-readiness-probes" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Readiness Probe Is Not Configured</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>17</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Check if Readiness Probe is not configured.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The issue "Readiness Probe Is Not Configured" means that your Kubernetes configuration file does not have a readiness probe set up for your container. A readiness probe is a diagnostic tool used by Kubernetes to determine when a container is ready to start accepting traffic. If a readiness probe is not configured, Kubernetes has no way of knowing when the container is ready to receive requests, which can lead to errors and downtime.<br><br>2. Security Concern:<br>While this issue is more of a reliability concern than a direct security issue, it can indirectly lead to security problems. If a container starts accepting traffic before it's fully ready, it could behave unpredictably or expose vulnerabilities. For instance, if a database isn't fully initialized before the container starts accepting requests, it could lead to data corruption or exposure.<br><br>3. Secure Code Example:<br>Here's an example of how you can configure a readiness probe in your Kubernetes configuration file:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: readiness-probe-example<br>spec:<br>  containers:<br>  - name: readiness-container<br>    image: k8s.gcr.io/busybox<br>    readinessProbe:<br>      exec:<br>        command:<br>        - cat<br>        - /tmp/healthy<br>      initialDelaySeconds: 5<br>      periodSeconds: 5<br>```<br>In this example, the readiness probe checks if the file `/tmp/healthy` exists in the container every 5 seconds, starting 5 seconds after the container is launched. The container is considered ready when the command `cat /tmp/healthy` succeeds.<br><br>4. Best Practices:<br>To prevent this issue, always configure a readiness probe for your containers in your Kubernetes configuration files. The readiness probe should accurately reflect whether or not your container is ready to start accepting traffic. Also, make sure to set appropriate `initialDelaySeconds` and `periodSeconds` values to avoid unnecessary checks or long delays in detecting readiness. Regularly review and update your readiness probes as necessary to ensure they remain accurate as your application evolves.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-readiness-probes" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Readiness Probe Is Not Configured</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>202</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Check if Readiness Probe is not configured.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. **Plain English Explanation:**<br>The issue at hand relates to a Kubernetes configuration file where a 'Readiness Probe' is not configured for a container. A readiness probe is a diagnostic performed periodically by the Kubernetes to determine if a specific Pod is ready to accept traffic. If the readiness probe is not configured, Kubernetes has no way of knowing when the Pod is ready to start accepting incoming requests.<br><br>2. **Why It's a Security Concern:**<br>If a readiness probe is not configured, it might lead to service disruption or degraded performance. This is because Kubernetes might start sending traffic to a Pod before it's fully ready to handle it. While this isn't a direct security vulnerability, it can lead to Denial of Service (DoS) if the application crashes due to premature traffic. It can also cause issues with load balancing and service discovery.<br><br>3. **Secure Code Example:**<br>Here's an example of how to configure a readiness probe in your Kubernetes configuration file:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: readiness-probe<br>spec:<br>  containers:<br>  - name: readiness-container<br>    image: k8s.gcr.io/busybox<br>    readinessProbe:<br>      exec:<br>        command:<br>        - cat<br>        - /tmp/healthy<br>```<br>In this example, the readiness probe checks the existence of the file `/tmp/healthy` in the container. If the file exists, the container is considered ready.<br><br>4. **Best Practices to Prevent This Issue:**<br>- Always configure readiness probes for your Pods in Kubernetes. This helps Kubernetes know when your application is ready to serve traffic.<br>- Use appropriate readiness probes based on your application. Kubernetes supports three types of probes: HTTP GET, TCP Socket, and Exec (executing a command inside the container). Choose the one that best suits your application's readiness condition.<br>- Regularly review and update your Infrastructure as Code (IaC) templates to ensure they adhere to security best practices. Use automated IaC security scanning tools to help with this process.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-readiness-probes" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Readiness Probe Is Not Configured</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>87</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Check if Readiness Probe is not configured.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>A Readiness Probe is a diagnostic performed periodically by the Kubernetes to determine if a specific Pod is ready to accept traffic or not. The issue here is that the Readiness Probe is not configured for a specific Kubernetes Pod in the given YAML file. This means that Kubernetes has no way to know when the Pod is ready to serve requests.<br><br>2. Security Concern:<br>Without a readiness probe, Kubernetes may start sending traffic to a Pod before it is fully initialized and ready to accept it. This can lead to several issues such as service disruption, increased error rates, and potential cascading failures in a microservices architecture. It can also expose partially initialized or unsecured services to potential attackers.<br><br>3. Secure Code Example:<br>To fix this, you need to add a readiness probe to the Pod specification in the Kubernetes YAML file. Here's an example of how to do it:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: readiness-probe<br>spec:<br>  containers:<br>  - name: readiness-container<br>    image: k8s.gcr.io/busybox<br>    readinessProbe:<br>      exec:<br>        command:<br>        - cat<br>        - /tmp/healthy<br>      initialDelaySeconds: 5<br>      periodSeconds: 5<br>```<br>In this example, the readiness probe is configured to check the existence of the file `/tmp/healthy` in the container every 5 seconds, starting from 5 seconds after the container starts. If the file exists, the Pod is considered ready.<br><br>4. Best Practices:<br>To prevent this issue, always configure readiness probes for your Kubernetes Pods. The specific configuration will depend on the application running in the Pod. For example, you might check for the existence of a specific file, as in the example above, or you might make a HTTP request to a specific endpoint. Make sure to also configure appropriate `initialDelaySeconds` and `periodSeconds` values to avoid unnecessary checks or delayed detection of readiness.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-readiness-probes" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Seccomp Profile Is Not Configured</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>20</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Containers should be configured with a secure Seccomp profile to restrict potentially dangerous syscalls
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>Seccomp (Secure Computing Mode) is a Linux kernel feature that can be used to limit the system calls a process can make. In the context of Kubernetes, a Seccomp profile can be used to restrict the system calls that a container can make. The issue here is that a Seccomp profile has not been configured for a container, which means it has unrestricted access to system calls.<br><br>2. Security Concern:<br>Without a Seccomp profile, a container can make any system call, including those that are not necessary for its operation. If an attacker gains control of the container, they could use these system calls to perform malicious activities, such as modifying system data or launching attacks on other parts of the system.<br><br>3. Secure Code Example:<br>To fix this issue, you need to define a Seccomp profile and apply it to the container. Here is an example of how you can do this in Kubernetes:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  securityContext:<br>    seccompProfile:<br>      type: Localhost<br>      localhostProfile: my-seccomp-profile.json<br>  containers:<br>  - name: secure-container<br>    image: secureimage<br>```<br><br>In this example, `my-seccomp-profile.json` is a Seccomp profile that defines the allowed system calls. This profile must be available on the node in the `/var/lib/kubelet/seccomp/` directory.<br><br>4. Best Practices:<br>- Always define a Seccomp profile for your containers. The profile should only allow the system calls that are necessary for the operation of the container.<br>- Regularly review and update your Seccomp profiles to ensure they are still appropriate for your containers.<br>- Use tools to automate the process of checking for Seccomp profiles in your Kubernetes configurations. This can help you catch any missing profiles before they become a security issue.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tutorials/security/seccomp/#create-pod-that-uses-the-container-runtime-default-seccomp-profile" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Seccomp Profile Is Not Configured</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>181</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Containers should be configured with a secure Seccomp profile to restrict potentially dangerous syscalls
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: <br>Seccomp (Secure Computing Mode) is a Linux kernel feature that can be used to limit the system calls a process can make. In the context of Kubernetes, a Seccomp profile can be used to restrict the system calls that a container can make. The issue here is that a Seccomp profile has not been configured for a container, which means that the container has unrestricted access to all system calls.<br><br>2. Security Concern: <br>Without a Seccomp profile, a container can make any system call, including those that are not necessary for its operation. If an attacker manages to compromise the container, they could use these unnecessary system calls to perform malicious activities, such as changing system configurations or accessing sensitive data.<br><br>3. Code Example: <br>To fix this issue, you need to define a Seccomp profile for your container. Here's an example of how to do this in a Kubernetes Deployment:<br><br>```yaml<br>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: secure-deployment<br>spec:<br>  template:<br>    metadata:<br>      annotations:<br>        seccomp.security.alpha.kubernetes.io/pod: 'runtime/default'<br>    spec:<br>      containers:<br>      - name: secure-container<br>        image: secureimage<br>```<br>In this example, the annotation `seccomp.security.alpha.kubernetes.io/pod: 'runtime/default'` applies the default Seccomp profile to all containers in the pod.<br><br>4. Best Practices: <br>To prevent this issue, always define a Seccomp profile for your containers. The profile should only allow the system calls that are necessary for the container's operation. If possible, use the default Seccomp profile provided by Kubernetes, as it blocks many dangerous system calls by default. If the default profile is too restrictive, you can create a custom profile that meets your needs.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tutorials/security/seccomp/#create-pod-that-uses-the-container-runtime-default-seccomp-profile" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Seccomp Profile Is Not Configured</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>202</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Containers should be configured with a secure Seccomp profile to restrict potentially dangerous syscalls
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: Seccomp (Secure Computing Mode) is a Linux kernel feature used to limit the system calls a process can make. In the context of Kubernetes, it's used to restrict the system calls a container can make. The issue here is that a Seccomp profile is not configured for a container, meaning it has unrestricted access to system calls, which could potentially be dangerous.<br><br>2. Security Concern: Without a Seccomp profile, a container can make any system call, including those that are not necessary for its operation. If an attacker gains control of the container, they can use these system calls to perform malicious activities, such as modifying system data or disrupting system operations.<br><br>3. Code Fix: You can specify a Seccomp profile in the securityContext of the Pod's specification. Here's an example:<br><br>    ```<br>    apiVersion: v1<br>    kind: Pod<br>    metadata:<br>      name: secure-pod<br>    spec:<br>      securityContext:<br>        seccompProfile:<br>          type: Localhost<br>          localhostProfile: profiles/my-seccomp-profile.json<br>      containers:<br>      - name: secure-container<br>        image: myimage<br>    ```<br><br>    In this example, `my-seccomp-profile.json` is a Seccomp profile stored on the node in the `seccomp.security.alpha.kubernetes.io/pod` and `container.seccomp.security.alpha.kubernetes.io/...` annotation.<br><br>4. Best Practices: Always define a Seccomp profile for your containers. The profile should only allow the minimum set of system calls required for the container to function. Regularly review and update the Seccomp profiles as necessary. Also, consider using tools that can automatically generate Seccomp profiles based on the typical behavior of the container.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tutorials/security/seccomp/#create-pod-that-uses-the-container-runtime-default-seccomp-profile" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Seccomp Profile Is Not Configured</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>87</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Containers should be configured with a secure Seccomp profile to restrict potentially dangerous syscalls
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>   The issue here is that the Seccomp (Secure Computing Mode) profile is not configured for a container in your Kubernetes configuration file. Seccomp is a Linux kernel feature that can be used to limit the system calls a process can make. In the context of containers, it can be used to restrict the system calls that a container can make to the host kernel, which can limit the impact of container escapes and vulnerabilities in the container or the applications it runs.<br><br>2. Security Concern:<br>   Without a Seccomp profile, a container has unrestricted system call access, which means it can make any system calls to the host kernel. This could potentially be exploited by an attacker to perform malicious activities, such as executing arbitrary code or gaining unauthorized access to data. This is particularly concerning in a multi-tenant environment where a single compromised container could potentially affect others.<br><br>3. Secure Code Example:<br>   To fix this issue, you need to define a Seccomp profile for your containers. Here is an example of how to do this in your Kubernetes configuration file:<br><br>   ```yaml<br>   apiVersion: v1<br>   kind: Pod<br>   metadata:<br>     name: secure-pod<br>   spec:<br>     securityContext:<br>       seccompProfile:<br>         type: Localhost<br>         localhostProfile: profiles/my-seccomp-profile.json<br>     containers:<br>     - name: test-container<br>       image: gcr.io/my_project/my_image:tag<br>   ```<br>   In this example, a Seccomp profile is specified in the `securityContext` of the Pod spec. The `type: Localhost` means that the profile is a file on the node located at `/var/lib/kubelet/seccomp/profiles/my-seccomp-profile.json`.<br><br>4. Best Practices:<br>   - Always define a Seccomp profile for your containers. This is a good practice even if you trust the applications running in your containers, as it can limit the impact of potential vulnerabilities.<br>   - Regularly review and update your Seccomp profiles. As new vulnerabilities are discovered and new system calls are added to the Linux kernel, you may need to update your profiles to restrict these new system calls.<br>   - Use tools like kube-score or kube-bench to automatically check your Kubernetes configuration for common security issues, including missing Seccomp profiles.<br>   - Consider using a more restrictive default Seccomp profile for your entire cluster, which can be overridden on a per-p
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tutorials/security/seccomp/#create-pod-that-uses-the-container-runtime-default-seccomp-profile" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Service Account Name Undefined Or Empty</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>10</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> A Kubernetes Pod should have a Service Account defined so to restrict Kubernetes API access, which means the attribute 'serviceAccountName' should be defined and not empty.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: <br>This security issue means that in the Kubernetes configuration file, there is a pod that doesn't have a defined Service Account. A Service Account is a special kind of account used by an application or a pod to interact with the Kubernetes API. If the 'serviceAccountName' attribute is not defined or is empty, it means the pod may be using the default Service Account, which could have more permissions than necessary.<br><br>2. Security Concern: <br>This is a security concern because it violates the principle of least privilege. If a pod has more permissions than it needs, it could be exploited by an attacker to perform unauthorized actions. For instance, if a pod is compromised, an attacker could use the pod's permissions to access sensitive data or disrupt the operation of the Kubernetes cluster.<br><br>3. Secure Code Example: <br>You should explicitly define a Service Account for each pod. Here is an example of how to do it:<br><br>```yaml<br>apiVersion: v1<br>kind: ServiceAccount<br>metadata:<br>  name: my-service-account<br>---<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: my-pod<br>spec:<br>  serviceAccountName: my-service-account<br>  containers:<br>  - name: my-container<br>    image: my-image<br>```<br><br>In this example, a Service Account named 'my-service-account' is created and then used in the pod 'my-pod'.<br><br>4. Best Practices: <br>To prevent this issue, always define a Service Account for each pod and ensure it has only the permissions it needs. Avoid using the default Service Account. Regularly review and update the permissions of your Service Accounts to ensure they are in line with the principle of least privilege. Use tools to automate the detection of security issues in your IaC code.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Service Account Name Undefined Or Empty</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>196</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> A Kubernetes Pod should have a Service Account defined so to restrict Kubernetes API access, which means the attribute 'serviceAccountName' should be defined and not empty.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue means that a Kubernetes Pod in your Infrastructure as a Code (IaC) setup does not have a Service Account defined or the 'serviceAccountName' attribute is empty. A Service Account in Kubernetes provides an identity for processes that run in a Pod. Without it, the Pod may have unrestricted access to the Kubernetes API, which can be a security risk.<br><br>2. Security Concern: If a Pod has unrestricted access to the Kubernetes API, it could potentially be exploited by an attacker to perform malicious actions on your Kubernetes cluster. This could include creating, deleting, or modifying resources, or gaining access to sensitive data.<br><br>3. Secure Code Example: To fix this issue, you need to define a Service Account for your Pod in your Kubernetes configuration file. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: ServiceAccount<br>metadata:<br>  name: my-service-account<br>---<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: my-pod<br>spec:<br>  serviceAccountName: my-service-account<br>  ...<br>```<br><br>In this example, a Service Account named 'my-service-account' is created and then assigned to a Pod named 'my-pod'.<br><br>4. Best Practices: To prevent this issue, always define a Service Account for your Pods. Avoid using the default Service Account as it may have more permissions than necessary. Instead, create a separate Service Account for each Pod with the minimum required permissions. Regularly review and update these permissions as needed. Consider using tools to automate the detection of such issues in your IaC setup.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Service Account Name Undefined Or Empty</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>179</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> A Kubernetes Pod should have a Service Account defined so to restrict Kubernetes API access, which means the attribute 'serviceAccountName' should be defined and not empty.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue means that a Kubernetes Pod in your Infrastructure as a Code (IaC) setup does not have a Service Account defined, or the 'serviceAccountName' attribute is empty. Service Accounts in Kubernetes are used to provide an identity for processes that run in a Pod. Without a Service Account, a Pod might have unrestricted access to the Kubernetes API, which could be a security risk.<br><br>2. Security Concern: If a Pod has unrestricted access to the Kubernetes API, it could potentially be exploited by a malicious user or software to perform unauthorized actions on your Kubernetes cluster. This could lead to data breaches, service disruptions, or other security incidents.<br><br>3. Code Example: To fix this issue, you need to define a Service Account for your Pod in your Kubernetes YAML file. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: ServiceAccount<br>metadata:<br>  name: my-service-account<br>---<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: my-pod<br>spec:<br>  serviceAccountName: my-service-account<br>  containers:<br>  - name: my-container<br>    image: my-image<br>```<br><br>In this example, a Service Account named 'my-service-account' is created, and then it is assigned to a Pod named 'my-pod'.<br><br>4. Best Practices: To prevent this issue, always ensure that each Pod in your Kubernetes cluster has a Service Account defined. Regularly review your IaC configurations to ensure they adhere to security best practices. Use automated tools to scan your IaC for security issues. Also, follow the principle of least privilege, i.e., only grant the minimum permissions necessary for a Pod to function correctly.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Service Account Token Automount Not Disabled</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>179</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Service Account Tokens are automatically mounted even if not necessary
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. **Plain English Explanation**<br>   This security issue means that the Kubernetes configuration file (kubernetes_vulnerable.yaml) is set up in a way that Service Account Tokens are automatically attached to the pods, even when they are not required. A Service Account Token is a type of credential that a pod can use to authenticate itself to other services in the Kubernetes cluster.<br><br>2. **Why it's a Security Concern**<br>   Automatically mounting service account tokens, even when not necessary, can lead to potential security risks. If a pod gets compromised, an attacker could use the token to gain unauthorized access to other services in the Kubernetes cluster. This could lead to data breaches or disruption of services.<br><br>3. **Secure Code Example**<br>   To fix this issue, you should disable the automatic mounting of service account tokens when they are not needed. This can be done by setting `automountServiceAccountToken` to `false` in the pod specification. Here's an example:<br><br>   ```yaml<br>   apiVersion: v1<br>   kind: Pod<br>   metadata:<br>     name: my-pod<br>   spec:<br>     serviceAccountName: my-service-account<br>     automountServiceAccountToken: false<br>     containers:<br>     - name: my-container<br>       image: my-image<br>   ```<br><br>4. **Best Practices**<br>   - Only enable `automountServiceAccountToken` when it's necessary for a pod to authenticate itself to other services in the Kubernetes cluster.<br>   - Regularly review and update your Kubernetes configurations to ensure they follow the principle of least privilege, i.e., only granting the minimum permissions necessary for a task.<br>   - Use tools to automatically scan your Infrastructure as Code (IaC) for security vulnerabilities.<br>   - Regularly update and patch your Kubernetes clusters to the latest version to benefit from the latest security enhancements.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#use-the-default-service-account-to-access-the-api-server" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Service Account Token Automount Not Disabled</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>196</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Service Account Tokens are automatically mounted even if not necessary
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue means that the Kubernetes configuration file (kubernetes_vulnerable.yaml) has been set up in a way that Service Account Tokens are automatically mounted to the pods, even when they are not needed. Service Account Tokens are essentially credentials that allow applications running inside a pod to interact with the Kubernetes API.<br><br>2. Security concern: If a pod gets compromised, an attacker can use these tokens to interact with the Kubernetes API, potentially escalating their privileges and causing more damage. This is especially concerning if the tokens are not necessary for the application's functionality, as it provides an unnecessary attack vector.<br><br>3. Secure code example: To fix this issue, you should disable the automatic mounting of service account tokens. In your Kubernetes configuration file, you can do this by setting `automountServiceAccountToken` to `false` under the `spec` of the relevant `ServiceAccount` or `Pod`. Here's an example:<br><br>   ```<br>   apiVersion: v1<br>   kind: ServiceAccount<br>   metadata:<br>     name: my-serviceaccount<br>   automountServiceAccountToken: false<br>   ```<br><br>   Or for a specific pod:<br><br>   ```<br>   apiVersion: v1<br>   kind: Pod<br>   metadata:<br>     name: my-pod<br>   spec:<br>     serviceAccountName: my-serviceaccount<br>     automountServiceAccountToken: false<br>   ```<br><br>4. Best Practices: <br>   - Always disable the automatic mounting of service account tokens unless they are explicitly required.<br>   - Regularly review and audit your Kubernetes configurations to ensure that unnecessary permissions and tokens are not being used.<br>   - Use role-based access control (RBAC) to limit the permissions of service accounts.<br>   - Regularly rotate service account tokens to limit the potential damage if they are compromised.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#use-the-default-service-account-to-access-the-api-server" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Service Account Token Automount Not Disabled</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>84</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Service Account Tokens are automatically mounted even if not necessary
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The issue "Service Account Token Automount Not Disabled" means that the Kubernetes service account is automatically providing access tokens to the containers in the pod, even if they are not required. This could potentially allow unauthorized access to the Kubernetes API.<br><br>2. Security Concern:<br>The security concern here is that if an attacker gains access to a pod, they could use the service account token to interact with the Kubernetes API, potentially escalating their privileges or accessing sensitive information. <br><br>3. Secure Code Example:<br>You can disable the automatic mounting of service account tokens by setting `automountServiceAccountToken` to `false` in your Kubernetes configuration file. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: my-pod<br>spec:<br>  serviceAccountName: my-service-account<br>  automountServiceAccountToken: false<br>  containers:<br>  - name: my-container<br>    image: my-image<br>```<br><br>In this example, the `automountServiceAccountToken` field is set to `false`, which means that the service account token will not be automatically mounted to the pod's containers.<br><br>4. Best Practices:<br>- Always disable the automatic mounting of service account tokens unless they are explicitly required.<br>- Regularly review and update your Kubernetes configurations to ensure they follow security best practices.<br>- Use role-based access control (RBAC) to limit the permissions of service accounts.<br>- Regularly rotate service account tokens.<br>- Use tools to scan your Infrastructure as Code (IaC) for security issues.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#use-the-default-service-account-to-access-the-api-server" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Service Account Token Automount Not Disabled</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>10</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Service Account Tokens are automatically mounted even if not necessary
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: In Kubernetes, a service account token is a type of credential that a pod can use to authenticate with the Kubernetes API server. The issue here is that these tokens are being automatically mounted into every pod, even if they're not needed. This means that every pod has a way to authenticate with the Kubernetes API, whether it needs to or not.<br><br>2. Security Concern: If a pod gets compromised, an attacker could use the service account token to interact with the Kubernetes API, potentially escalating their privileges or affecting other parts of the system. This is a principle of least privilege violation, where entities (in this case, pods) have more permissions than they need to perform their function.<br><br>3. Secure Code Example:<br>   You can disable the automounting of service account tokens at the pod level or at the service account level. Here's how to do it at the pod level:<br><br>   ```yaml<br>   apiVersion: v1<br>   kind: Pod<br>   metadata:<br>     name: my-pod<br>   spec:<br>     automountServiceAccountToken: false<br>     ...<br>   ```<br><br>   And here's how to do it at the service account level:<br><br>   ```yaml<br>   apiVersion: v1<br>   kind: ServiceAccount<br>   metadata:<br>     name: my-service-account<br>   automountServiceAccountToken: false<br>   ```<br><br>4. Best Practices: <br>   - Always follow the principle of least privilege. Only give pods the permissions they need to do their job.<br>   - Regularly review and audit your Kubernetes configurations for security issues.<br>   - Use tools to automatically scan your Infrastructure as Code (IaC) for security issues.<br>   - Consider using more granular forms of authentication and authorization, such as Role-Based Access Control (RBAC), instead of service account tokens.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#use-the-default-service-account-to-access-the-api-server" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Service With External Load Balancer</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>40</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Service has an external load balancer, which may cause accessibility from other networks and the Internet
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Plain English Explanation:<br>This security issue means that the Kubernetes service defined in the `kubernetes_vulnerable.yaml` file is configured to use an external load balancer. This could potentially allow access to the service from any network, including the internet, which may not be intended or secure.<br><br>2. Why it's a Security Concern:<br>Using an external load balancer without proper security measures can expose the service to the internet, making it vulnerable to attacks. If the service is not intended to be publicly accessible, or if it processes sensitive data, this could lead to data breaches or unauthorized access.<br><br>3. Secure Code Example:<br>To fix this issue, you could switch to using an internal load balancer or add network policies to restrict access. Here's an example of how to make the service internal in Kubernetes:<br><br>```yaml<br>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: internal-service<br>  annotations:<br>    cloud.google.com/load-balancer-type: "Internal"<br>spec:<br>  type: LoadBalancer<br>  ports:<br>  - port: 80<br>    targetPort: 8080<br>    protocol: TCP<br>  selector:<br>    app: my-app<br>```<br>In this example, the `cloud.google.com/load-balancer-type: "Internal"` annotation makes the load balancer internal, restricting its accessibility to within the same network.<br><br>4. Best Practices:<br>- Always use the least privilege principle when configuring services. If a service doesn't need to be accessible from the internet, don't make it so.<br>- Regularly review and update your IaC configurations to ensure they meet current security standards.<br>- Implement network policies to control traffic to and from your Kubernetes pods.<br>- Consider using a private cluster for sensitive workloads.<br>- Regularly monitor and audit your services for any unexpected or unauthorized access.<br>- Encrypt sensitive data in transit and at rest.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Shared Host IPC Namespace</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>200</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Container should not share the host IPC namespace
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: In Kubernetes, Inter-Process Communication (IPC) namespaces isolate the communication between processes running in different containers. If a container shares the host IPC namespace, it means that the processes in this container can communicate directly with all processes on the host system. This is a violation of the principle of least privilege, which states that a process should only have access to the resources it needs to perform its function.<br><br>2. Security Concern: If a container shares the host IPC namespace, it can lead to potential security risks. For instance, a malicious process running in the container could interfere with other processes on the host system, leading to a potential denial of service attack, data corruption, or even data theft.<br><br>3. Secure Code Example: To fix this issue, you need to ensure that the `hostIPC` field is set to `false` in your Kubernetes configuration file. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  containers:<br>  - name: secure-container<br>    image: secureimage<br>  hostIPC: false<br>```<br><br>In this example, the `hostIPC` field is set to `false`, which means the container will not share the host's IPC namespace.<br><br>4. Best Practices: To prevent this issue, always follow the principle of least privilege when configuring your containers. Avoid sharing the host IPC namespace unless it's absolutely necessary. Regularly review and audit your Kubernetes configuration files to ensure they're following best security practices. Use automated tools to scan your Infrastructure as Code (IaC) for potential security issues.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Shared Host Network Namespace</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>198</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Container should not share the host network namespace
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: <br>The issue "Container should not share the host network namespace" means that the container is configured to use the same networking as the host machine. In Kubernetes, a "namespace" is a virtual cluster being created within the actual Kubernetes cluster. When a container shares the host network namespace, it has access to the same network interfaces, IP addresses, and ports as the host machine.<br><br>2. Security Concern: <br>This is a security concern because it increases the attack surface. If the container is compromised, the attacker could potentially gain access to the host network, which could lead to further attacks on other containers or the host machine itself. It also violates the principle of least privilege, which states that a process should only have the minimum privileges necessary to perform its function.<br><br>3. Secure Code Example: <br>To fix this, you need to remove the `hostNetwork: true` line from your Kubernetes configuration file. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: mypod<br>spec:<br>  containers:<br>  - name: mycontainer<br>    image: myimage<br>  # hostNetwork: true  <-- Remove this line<br>```<br><br>4. Best Practices: <br>To prevent this issue, follow these best practices:<br>- Always follow the principle of least privilege. Only give containers the minimum permissions they need to function.<br>- Regularly review and update your Kubernetes configurations to ensure they meet security best practices.<br>- Use automated tools to scan your Infrastructure as Code (IaC) for security issues.<br>- Implement network policies to control the traffic between pods in your Kubernetes cluster.<br>- Consider using a dedicated tool for managing your Kubernetes network, such as Calico or Weave. These tools can provide additional security features, such as network segmentation and encryption.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Using Unrecommended Namespace</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>74</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Namespaces like 'default', 'kube-system' or 'kube-public' should not be used
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: <br>This security issue is about the use of unrecommended namespaces in a Kubernetes configuration file. Namespaces in Kubernetes are like virtual clusters within a single physical cluster. They are used to divide cluster resources between multiple users. The namespaces 'default', 'kube-system' or 'kube-public' are default namespaces created by Kubernetes and have special purposes. Using these for your own applications can lead to conflicts or unintended behavior.<br><br>2. Security Concern: <br>The 'kube-system' namespace is where Kubernetes system processes run, and the 'kube-public' namespace is readable by all users. Using these namespaces for your own applications can lead to potential security risks, such as unauthorized access to sensitive system processes or data. The 'default' namespace is where objects without a specified namespace get placed. This can lead to confusion and potential conflicts.<br><br>3. Secure Code Example:<br>Instead of using a default or system namespace, create a new namespace for your application. Here is an example of how to do this in a Kubernetes YAML file:<br><br>```yaml<br>apiVersion: v1<br>kind: Namespace<br>metadata:<br>  name: my-namespace<br>---<br>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: my-app<br>  namespace: my-namespace<br>...<br>```<br><br>4. Best Practices: <br>- Always create and use custom namespaces for your applications. This helps to isolate your application resources from system resources and other applications.<br>- Do not use the 'default', 'kube-system', or 'kube-public' namespaces for your applications.<br>- Regularly review and update your IaC configurations to ensure they follow best practices for security and resource management.<br>- Use automated tools to scan your IaC configurations for security vulnerabilities and misconfigurations.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Using Unrecommended Namespace</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>63</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Namespaces like 'default', 'kube-system' or 'kube-public' should not be used
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. **Plain English Explanation:**<br>This security issue is about the use of certain namespaces in Kubernetes, specifically 'default', 'kube-system', or 'kube-public'. A namespace in Kubernetes is like a virtual cluster within a Kubernetes cluster. It provides a scope for names and segregates clusters to avoid interference with each other. The 'default', 'kube-system', or 'kube-public' namespaces are built-in and have special purposes, so using them for your own applications can cause conflicts and security issues.<br><br>2. **Why It's a Security Concern:**<br>The 'default' namespace is where resources go if no other namespace is specified, so it can become cluttered and difficult to manage. The 'kube-system' namespace is for resources created by the Kubernetes system, so if you add your own resources there, you might interfere with the operation of your Kubernetes system. The 'kube-public' namespace is readable by all users, including those not authenticated, so putting your own resources there can expose them to unauthorized access.<br><br>3. **Secure Code Example:**<br>Instead of using the built-in namespaces, create your own. Here's an example of how to do this in a Kubernetes YAML file:<br><br>```yaml<br>apiVersion: v1<br>kind: Namespace<br>metadata:<br>  name: my-namespace<br>```<br><br>Then, specify your namespace when creating resources:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: my-pod<br>  namespace: my-namespace<br>...<br>```<br><br>4. **Best Practices:**<br>- Always create and use your own namespaces for your applications, rather than using the built-in ones.<br>- Use meaningful and descriptive names for your namespaces to make your system easier to understand and manage.<br>- Regularly review your namespaces and the resources within them to ensure that they are being used correctly and securely.<br>- Limit the access to each namespace using Role-Based Access Control (RBAC). This ensures that only authorized users can view or modify resources in a namespace.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Using Unrecommended Namespace</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>40</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Namespaces like 'default', 'kube-system' or 'kube-public' should not be used
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. **Plain English Explanation**: This security issue means that in your Kubernetes configuration file (kubernetes_vulnerable.yaml), you are using a namespace that is not recommended. Specifically, you're using one of the following namespaces: 'default', 'kube-system', or 'kube-public'. <br><br>2. **Why It's a Security Concern**: These namespaces are created by Kubernetes by default and contain system pods that are critical for the functioning of the Kubernetes system. If you deploy your applications in these namespaces, it could potentially interfere with the Kubernetes system pods. This can lead to instability in the system, and in worst case scenarios, it could allow malicious applications to gain elevated privileges.<br><br>3. **Secure Code Example**:<br>    Instead of using the default or system namespaces, create a new namespace for your application. Here's how you can do it:<br><br>    ```yaml<br>    apiVersion: v1<br>    kind: Namespace<br>    metadata:<br>      name: my-namespace<br>    ```<br><br>    Then, in your application configuration, specify the newly created namespace:<br><br>    ```yaml<br>    apiVersion: apps/v1<br>    kind: Deployment<br>    metadata:<br>      name: my-app<br>      namespace: my-namespace<br>    ...<br>    ```<br><br>4. **Best Practices to Prevent This Issue**:<br>    - Always create and use separate namespaces for your applications. This isolates your applications from system pods and from each other, reducing the potential impact of a security breach.<br>    - Regularly review your Kubernetes configuration files to ensure that they adhere to security best practices.<br>    - Use automated tools to scan your Infrastructure as Code (IaC) for security vulnerabilities. These tools can help you catch and fix security issues before they become a problem.<br>    - Educate your team about the importance of namespaces in Kubernetes and the risks associated with using default or system namespaces.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Using Unrecommended Namespace</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>53</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Namespaces like 'default', 'kube-system' or 'kube-public' should not be used
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>This security issue is about the use of unrecommended namespaces in Kubernetes configuration. Namespaces are a way to divide cluster resources between multiple users. The 'default', 'kube-system', and 'kube-public' namespaces are built-in and have special purposes in Kubernetes. Using them for deploying your applications can lead to unintended consequences and potential security risks.<br><br>2. Security Concern:<br>The 'default' namespace is where resources go if no other namespace is specified. The 'kube-system' namespace is for resources created by the Kubernetes system itself. The 'kube-public' namespace is readable by all users. Using these namespaces for your own applications can lead to confusion, misconfiguration, and potential access by unauthorized users. It could also disrupt critical system processes if your application interferes with system resources in the 'kube-system' namespace.<br><br>3. Code Fix:<br>Instead of using the 'default', 'kube-system', or 'kube-public' namespaces, create a new namespace for your application. Here's how you can do it:<br><br>```yaml<br>apiVersion: v1<br>kind: Namespace<br>metadata:<br>  name: my-namespace<br>---<br>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: my-app<br>  namespace: my-namespace<br>spec:<br>  ...<br>```<br><br>In this example, a new namespace 'my-namespace' is created and used for deploying an application.<br><br>4. Best Practices:<br>- Always create and use custom namespaces for your applications. This helps in isolating your applications from each other and from system resources.<br>- Avoid using 'default', 'kube-system', or 'kube-public' namespaces for your applications.<br>- Regularly review your Kubernetes configurations and ensure that they follow best practices for security.<br>- Use automated tools to scan your Infrastructure as Code (IaC) for potential security issues.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Using Unrecommended Namespace</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>178</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Namespaces like 'default', 'kube-system' or 'kube-public' should not be used
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The security issue "Using Unrecommended Namespace" means that in your Kubernetes configuration file (kubernetes_vulnerable.yaml), you are using a namespace that is not recommended, such as 'default', 'kube-system', or 'kube-public'. Namespaces in Kubernetes are like virtual clusters within a physical cluster. They are used to divide cluster resources between multiple users.<br><br>2. Security Concern:<br>Using these namespaces is a security concern because they are used by Kubernetes itself for its internal processes. If a user deploys applications in these namespaces, it could potentially interfere with the Kubernetes system processes. Also, these namespaces often have more privileges than regular namespaces, so if a malicious actor gains access to an application in these namespaces, they could potentially do more damage.<br><br>3. Code Example:<br>To fix this issue, you should create a new namespace and deploy your application in that namespace. Here is an example of how to do this:<br><br>```yaml<br>apiVersion: v1<br>kind: Namespace<br>metadata:<br>  name: my-namespace<br>---<br>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: my-app<br>  namespace: my-namespace<br>...<br>```<br><br>In this example, a new namespace called 'my-namespace' is created, and the application 'my-app' is deployed in this namespace.<br><br>4. Best Practices:<br>To prevent this issue, follow these best practices:<br>- Always create a new namespace for your applications instead of using the default or system namespaces.<br>- Limit the privileges of your namespaces as much as possible. Do not give them more permissions than they need.<br>- Regularly review your Kubernetes configuration files to ensure that you are not accidentally using a system namespace.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Using Unrecommended Namespace</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>157</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Namespaces like 'default', 'kube-system' or 'kube-public' should not be used
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: <br>This security issue means that in your Kubernetes configuration file (kubernetes_vulnerable.yaml), you are using a namespace that is not recommended, specifically one of 'default', 'kube-system', or 'kube-public'. A namespace in Kubernetes is a way to divide cluster resources between multiple users.<br><br>2. Security Concern: <br>Using these namespaces is a security concern because they are used by Kubernetes itself for its internal processes. If a user deploys applications in these namespaces, it could potentially interfere with the Kubernetes system processes. This could lead to system instability or even allow malicious activities if the application is compromised.<br><br>3. Secure Code Example: <br>Instead of using the 'default', 'kube-system', or 'kube-public' namespaces, you should define and use a custom namespace. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Namespace<br>metadata:<br>  name: custom-namespace<br>---<br>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: secure-deployment<br>  namespace: custom-namespace<br>spec:<br>  ...<br>```<br>In this example, a new namespace 'custom-namespace' is created and used for the deployment 'secure-deployment'.<br><br>4. Best Practices:<br>To prevent this issue, always create and use custom namespaces for your applications and avoid using the 'default', 'kube-system', and 'kube-public' namespaces. This will help isolate your applications from the Kubernetes system processes and from each other, reducing the potential impact of a security breach. Additionally, follow the principle of least privilege by only granting necessary permissions to each namespace.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Using Unrecommended Namespace</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>195</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Namespaces like 'default', 'kube-system' or 'kube-public' should not be used
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The security issue indicates that in your Kubernetes configuration file (kubernetes_vulnerable.yaml), you're using a namespace that is not recommended. Specifically, on line 195, you're using either 'default', 'kube-system', or 'kube-public' namespace. <br><br>2. Security Concern:<br>Namespaces in Kubernetes are like virtual clusters intended for environments with many users spread across multiple teams or projects. The 'default', 'kube-system', and 'kube-public' namespaces are default namespaces that come with Kubernetes and are used for special purposes. Using these namespaces for your applications can lead to potential security risks, as malicious users could exploit this to affect the entire system, not just your application.<br><br>3. Secure Code Example:<br>Instead of using the default namespaces, you should create a new namespace for your application. Here's how you can do it:<br><br>```yaml<br>apiVersion: v1<br>kind: Namespace<br>metadata:<br>  name: my-namespace<br>---<br>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: my-app<br>  namespace: my-namespace<br>...<br>```<br>In this example, 'my-namespace' is a new namespace created for the application 'my-app'.<br><br>4. Best Practices:<br>- Always create a new namespace for your applications and avoid using the default namespaces.<br>- Follow the principle of least privilege. Only give the necessary permissions that a user or application needs.<br>- Regularly review and update your Kubernetes configurations to ensure they adhere to security best practices.<br>- Use automated tools to scan your Infrastructure as Code (IaC) for potential security issues.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Using Unrecommended Namespace</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>7</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Namespaces like 'default', 'kube-system' or 'kube-public' should not be used
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The security issue "Using Unrecommended Namespace" means that in your Kubernetes configuration file (kubernetes_vulnerable.yaml), you are using a namespace that is not recommended, specifically 'default', 'kube-system', or 'kube-public'. These are special namespaces used by Kubernetes system itself.<br><br>2. Security Concern:<br>Using these namespaces can be a security concern because they are used for system processes in Kubernetes. If an attacker gains access to these namespaces, they could potentially gain control over the entire Kubernetes cluster. This could lead to unauthorized access, data breaches, or disruption of services.<br><br>3. Code Fix:<br>You should create and use your own namespaces instead of using the default or system namespaces. Here's an example of how you can do this:<br><br>```yaml<br>apiVersion: v1<br>kind: Namespace<br>metadata:<br>  name: my-namespace<br>---<br>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: my-app<br>  namespace: my-namespace<br>spec:<br>  ...<br>```<br>In this example, a new namespace 'my-namespace' is created and used for the deployment of 'my-app'.<br><br>4. Best Practices:<br>- Always create and use your own namespaces for your applications. This helps in isolating your applications from system processes and from each other.<br>- Regularly review and update your IaC scripts to ensure they adhere to security best practices.<br>- Use automated tools to scan your IaC scripts for security vulnerabilities.<br>- Limit the permissions of your applications to only what they need to function properly. This follows the principle of least privilege, which can minimize the potential damage of a breach.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge medium">MEDIUM</span>
                        <h3 class="finding-title">Using Unrecommended Namespace</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>118</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Namespaces like 'default', 'kube-system' or 'kube-public' should not be used
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>This security issue means that in the Kubernetes configuration file located at './test_data/iac_example/kubernetes_vulnerable.yaml', there is a use of a namespace which is not recommended. Specifically, the namespaces 'default', 'kube-system', or 'kube-public' are being used.<br><br>2. Security Concern:<br>These namespaces are created by default in every Kubernetes cluster and have specific roles. The 'default' namespace is where resources are placed when no other namespace is specified. The 'kube-system' namespace is for objects created by the Kubernetes system. The 'kube-public' namespace is automatically created and readable by all users. Using these namespaces for your own applications can lead to conflicts, misconfigurations, and potential security risks. For example, if an attacker gains access to a pod in the 'kube-system' namespace, they could potentially control the entire Kubernetes cluster.<br><br>3. Secure Code Example:<br>Instead of using the default namespaces, create a new namespace for your application. Here's how you can do it:<br><br>```yaml<br>apiVersion: v1<br>kind: Namespace<br>metadata:<br>  name: my-namespace<br>```<br>And then use it in your Kubernetes configuration:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: my-pod<br>  namespace: my-namespace<br>spec:<br>  containers:<br>  - name: my-container<br>    image: my-image<br>```<br><br>4. Best Practices:<br>- Always create and use separate namespaces for different applications or services. This helps in isolating resources, limiting the scope of user permissions, and improving security.<br>- Avoid using the 'default', 'kube-system', and 'kube-public' namespaces for your own applications.<br>- Regularly review and update your Infrastructure as Code (IaC) scripts to ensure they follow the latest security best practices.<br>- Use automated tools to scan your IaC scripts for potential security issues.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                </div><div id="low-section" class="severity-section"><h2 class="severity-header low">LOW (45)</h2>
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">CPU Limits Not Set</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>17</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> CPU limits should be set because if the system has CPU time free, a container is guaranteed to be allocated as much CPU as it requests
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue means that in the Kubernetes configuration file (kubernetes_vulnerable.yaml), there is no CPU limit set for a container. This means a container can use as much CPU as it requests, assuming the system has free CPU time.<br><br>2. Security Concern: This is a security concern because if a container starts consuming more CPU resources than expected (due to a bug, inefficient code, or a malicious attack like a Denial of Service), it can starve other containers of CPU time, causing performance degradation or even system failure. <br><br>3. Secure Code Example: To fix this issue, you should set a CPU limit in the Kubernetes configuration file. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: cpu-limited-pod<br>spec:<br>  containers:<br>  - name: cpu-limited-container<br>    image: nginx<br>    resources:<br>      limits:<br>        cpu: "1"<br>      requests:<br>        cpu: "0.5"<br>```<br>In this example, the container `cpu-limited-container` has a CPU limit of 1 CPU unit and a CPU request of 0.5 CPU units.<br><br>4. Best Practices: To prevent this issue, always set resource limits (including CPU and memory) for your containers in your Kubernetes configurations. This helps to ensure that no single container can consume all available resources and negatively impact other containers or the entire system. Regularly review and adjust these limits based on the actual resource usage of your containers.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">CPU Limits Not Set</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>87</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> CPU limits should be set because if the system has CPU time free, a container is guaranteed to be allocated as much CPU as it requests
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This issue means that the Kubernetes configuration file does not have a CPU limit set for a particular container. This means that if the system has free CPU time, the container can use as much CPU as it requests, potentially leading to resource exhaustion if the container uses too much CPU.<br><br>2. Security Concern: This is a security concern because an attacker could potentially exploit this to cause a Denial of Service (DoS) attack. By running a process that consumes a lot of CPU in the container, the attacker could exhaust the system's CPU resources, causing other containers or even the entire system to slow down or become unresponsive.<br><br>3. Secure Code Example: <br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: cpu-limited-pod<br>spec:<br>  containers:<br>  - name: cpu-limited-container<br>    image: nginx<br>    resources:<br>      limits:<br>        cpu: "1"<br>      requests:<br>        cpu: "0.5"<br>```<br>In this example, the CPU limit for the container is set to 1 CPU unit, and the container requests 0.5 CPU units. This means that the container is guaranteed to get 0.5 CPU units, but it can use up to 1 CPU unit if the system has free CPU time.<br><br>4. Best Practices: Always set resource limits (CPU and memory) for your containers in your Kubernetes configuration files. This will prevent containers from using up all the system's resources, which could lead to performance issues or even system crashes. Also, monitor your containers' resource usage to ensure they are not using too much CPU or memory. If a container consistently uses more resources than it should, it might be a sign of a security issue or a poorly optimized application.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">CPU Limits Not Set</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>202</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> CPU limits should be set because if the system has CPU time free, a container is guaranteed to be allocated as much CPU as it requests
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue means that the Kubernetes configuration file does not have a limit set on the amount of CPU resources that a container can use. This means that if the system has available CPU time, a container can use as much as it requests, potentially consuming all available CPU resources.<br><br>2. Security Concern: The absence of CPU limits can lead to resource exhaustion, where one container uses up all the available CPU time, leaving none for other containers or processes. This can cause performance degradation or even system crashes, which can be exploited by malicious users to create a denial of service (DoS) attack.<br><br>3. Secure Code Example: To fix this issue, you need to set CPU limits in the Kubernetes configuration file. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: cpu-limited-pod<br>spec:<br>  containers:<br>  - name: cpu-limited-container<br>    image: nginx<br>    resources:<br>      limits:<br>        cpu: "1"<br>      requests:<br>        cpu: "0.5"<br>```<br><br>In this example, the `cpu-limited-container` is limited to use 1 CPU unit at maximum, and it requests 0.5 CPU units to start with.<br><br>4. Best Practices: To prevent this issue, always set CPU and memory limits for your containers in your Kubernetes configuration files. This ensures that no single container can monopolize system resources. Monitor your containers' resource usage regularly to adjust the limits as necessary. Also, consider using Kubernetes namespaces to segregate resources among multiple users or teams, each with its own resource limits.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">CPU Limits Not Set</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>181</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> CPU limits should be set because if the system has CPU time free, a container is guaranteed to be allocated as much CPU as it requests
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The issue "CPU Limits Not Set" means that there is no upper limit set for the CPU usage for a particular container in the Kubernetes configuration file. This means that a container can use as much CPU as it wants, potentially starving other containers or processes of CPU resources.<br><br>2. Security Concern:<br>This is a security concern because an attacker could exploit this configuration to launch a Denial of Service (DoS) attack. If a container is compromised and starts consuming excessive CPU resources, it could cause other containers or even the entire system to become unresponsive or slow down significantly.<br><br>3. Secure Code Example:<br>To fix this issue, you need to set a CPU limit in the Kubernetes configuration file. Here is an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: cpu-limited-pod<br>spec:<br>  containers:<br>  - name: cpu-limited-container<br>    image: nginx<br>    resources:<br>      limits:<br>        cpu: "1" # This sets the CPU limit to 1 core<br>```<br><br>In this example, the CPU limit is set to 1 core. This means that the container can't use more than 1 core, preventing it from consuming excessive CPU resources.<br><br>4. Best Practices:<br>- Always set resource limits (CPU and memory) for your containers to prevent them from consuming excessive resources.<br>- Regularly review and adjust these limits based on the actual resource usage of your containers.<br>- Use monitoring tools to keep track of the resource usage of your containers and alert you when they are close to their limits.<br>- Consider using Kubernetes namespaces to isolate resources between different projects or teams.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">CPU Requests Not Set</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>202</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> CPU requests should be set to ensure the sum of the resource requests of the scheduled Containers is less than the capacity of the node
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue means that in the Kubernetes configuration file, there is no specific CPU request set for the containers. This could lead to a situation where the total CPU resources required by all the containers scheduled on a node exceed the node's capacity.<br><br>2. Security Concern: If CPU requests are not set, it can lead to resource exhaustion. This can cause performance degradation, instability, and even denial of service if the node becomes overwhelmed. This can be exploited by malicious actors to cause a denial of service attack by intentionally trying to consume more resources.<br><br>3. Secure Code Example: To fix this issue, you need to set CPU requests in the Kubernetes configuration file. Here is an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: cpu-demo<br>spec:<br>  containers:<br>  - name: cpu-demo-ctr<br>    image: v1<br>    resources:<br>      requests:<br>        cpu: "0.5" # This sets the CPU request to 0.5 cores<br>```<br>In this example, the CPU request is set to 0.5 cores for the container `cpu-demo-ctr`. This means the container is guaranteed to get half a CPU core's worth of compute resources.<br><br>4. Best Practices: To prevent this issue, always set CPU and memory requests and limits for your containers. This helps Kubernetes make more intelligent decisions when scheduling pods and ensures that your applications have access to the resources they need. Monitor your application's resource usage over time to understand its needs and adjust the requests and limits as necessary.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">CPU Requests Not Set</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>17</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> CPU requests should be set to ensure the sum of the resource requests of the scheduled Containers is less than the capacity of the node
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: The issue at hand is that the Kubernetes configuration file does not have a CPU request set for a container. This means the container doesn't specify the amount of CPU it needs to run properly. When a container doesn't specify its CPU requirements, it could potentially use more CPU resources than intended, which could impact other containers running on the same node.<br><br>2. Security Concern: This is a security concern because if a container uses more CPU resources than it should, it could potentially cause a Denial of Service (DoS) within the node by starving other containers of CPU resources. In a worst-case scenario, if the container was compromised, it could be used to exhaust the CPU resources of the node, affecting all other containers on the same node.<br><br>3. Secure Code Example: <br>   Here is how to set CPU requests in your Kubernetes configuration file:<br><br>   ```<br>   apiVersion: v1<br>   kind: Pod<br>   metadata:<br>     name: cpu-demo<br>   spec:<br>     containers:<br>     - name: cpu-demo-ctr<br>       image: v1<br>       resources:<br>         requests:<br>           cpu: "0.5" # This line sets the CPU request to 0.5 cores<br>   ```<br><br>   In this example, the container `cpu-demo-ctr` requests 0.5 CPU cores from the node it's scheduled on.<br><br>4. Best Practices: Always specify CPU and memory requests and limits for every container in your Kubernetes configuration files. This not only ensures that your containers have the resources they need to operate correctly, but also prevents them from using more resources than they should. Additionally, regularly review your resource usage and adjust your requests and limits as necessary to match your application's needs.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">CPU Requests Not Set</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>87</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> CPU requests should be set to ensure the sum of the resource requests of the scheduled Containers is less than the capacity of the node
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>This issue means that in the Kubernetes configuration file, there is no specific amount of CPU resources allocated for the containers. Kubernetes uses these resource requests to decide on which nodes to place pods. Without these requests, Kubernetes may over-allocate resources to a node, causing it to run out of capacity.<br><br>2. Security Concern:<br>This is a security concern because if a container uses more CPU resources than the node can handle, it could cause the node to crash, leading to potential downtime. It could also allow a single container to monopolize the node's resources, starving other containers and applications of the necessary resources to function correctly. This could potentially be exploited by malicious actors to perform denial-of-service attacks.<br><br>3. Secure Code Example:<br>To fix this, you need to specify the CPU requests in the Kubernetes configuration file. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: cpu-demo<br>spec:<br>  containers:<br>  - name: cpu-demo-ctr<br>    image: v1<br>    resources:<br>      requests:<br>        cpu: "0.5"  # Requesting 0.5 CPU<br>      limits:<br>        cpu: "1"    # Limiting to 1 CPU<br>```<br>In this example, the container is requesting 0.5 CPU and has a limit of 1 CPU.<br><br>4. Best Practices:<br>- Always specify resource requests and limits for your containers. This helps Kubernetes make better scheduling decisions and prevents containers from monopolizing resources.<br>- Monitor your containers' resource usage to understand how much CPU and memory they typically use, and adjust your requests and limits accordingly.<br>- Use namespaces to segregate resources among users spread across multiple projects.<br>- Implement resource quotas to limit the amount of CPU and memory that can be used in a namespace.<br>- Regularly review and update your resource requests and limits as your application's needs change.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">CPU Requests Not Set</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>181</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> CPU requests should be set to ensure the sum of the resource requests of the scheduled Containers is less than the capacity of the node
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>This security issue means that in the Kubernetes configuration file, there is no specific CPU request set for the containers. This could lead to a situation where the total CPU resources requested by all the containers scheduled on a node could exceed the node's capacity.<br><br>2. Security Concern:<br>This is a security concern because if a container doesn't have a CPU request set, it could potentially consume all available CPU resources on a node. This could lead to Denial of Service (DoS) attacks, where other containers on the same node are starved of CPU resources, causing them to perform poorly or fail.<br><br>3. Secure Code Example:<br>To fix this issue, you need to set a CPU request in the Kubernetes configuration file. Here is an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: cpu-demo<br>spec:<br>  containers:<br>  - name: cpu-demo-ctr<br>    image: vish/stress<br>    resources:<br>      requests:<br>        cpu: "1" # This line sets the CPU request<br>      limits:<br>        cpu: "2"<br>```<br>In this example, the container `cpu-demo-ctr` requests 1 CPU core and has a limit of 2 CPU cores.<br><br>4. Best Practices:<br>- Always define resource requests and limits for your containers. This ensures that your containers are scheduled on nodes with sufficient resources and prevents resource starvation issues.<br>- Regularly review and adjust these values based on the actual resource usage of your containers. This helps to avoid over-provisioning or under-provisioning resources.<br>- Use Kubernetes namespaces to isolate resources between different projects or teams. This can prevent a resource-hungry application in one namespace from consuming resources needed by applications in other namespaces.<br>- Consider using Kubernetes Resource Quotas and Limit Ranges to set upper and lower bounds on resources per namespace or per node. This can prevent any single application from consuming all available resources.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Deployment Has No PodAntiAffinity</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>84</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Check if Deployment resources don't have a podAntiAffinity policy, which prevents multiple pods from being scheduled on the same node.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Plain English Explanation:<br>This security issue means that the Kubernetes deployment configuration in the file 'kubernetes_vulnerable.yaml' does not have a 'podAntiAffinity' policy. This policy ensures that multiple instances (pods) of the same application are not scheduled to run on the same physical server (node) in a Kubernetes cluster. Without this policy, there's a risk that all instances of an application could end up on the same node, which could lead to a single point of failure.<br><br>2. Why it's a Security Concern:<br>If all instances of an application are running on the same node and that node fails or is compromised, the entire application could become unavailable or vulnerable. This could lead to downtime and potential data loss, which could be exploited by malicious actors.<br><br>3. Secure Code Example:<br>To fix this issue, you need to define a 'podAntiAffinity' policy in your Kubernetes deployment configuration. Here's an example:<br><br>```yaml<br>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: my-deployment<br>spec:<br>  replicas: 3<br>  selector:<br>    matchLabels:<br>      app: my-app<br>  template:<br>    metadata:<br>      labels:<br>        app: my-app<br>    spec:<br>      containers:<br>      - name: my-container<br>        image: my-image<br>      affinity:<br>        podAntiAffinity:<br>          requiredDuringSchedulingIgnoredDuringExecution:<br>          - labelSelector:<br>              matchExpressions:<br>              - key: "app"<br>                operator: In<br>                values:<br>                - my-app<br>            topologyKey: "kubernetes.io/hostname"<br>```<br>In this example, the 'podAntiAffinity' policy ensures that the scheduler does not co-locate pods with the same 'app' label on a single node.<br><br>4. Best Practices:<br>To prevent this issue, always define a 'podAntiAffinity' policy for your Kubernetes deployments, especially when running multiple instances of the same application. This will ensure that your applications are distributed across multiple nodes, reducing the risk of a single point of failure. It's also a good practice to regularly review and update your Infrastructure as Code (IaC) configurations to ensure they follow the latest security best practices.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Deployment Without PodDisruptionBudget</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>78</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Deployments should be assigned with a PodDisruptionBudget to ensure high availability
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>A PodDisruptionBudget (PDB) is a Kubernetes feature that allows you to specify the minimum number of available pods your application needs to function properly. The issue here is that your Kubernetes deployment does not have a PodDisruptionBudget defined. This means that during a disruption (like a node failure or maintenance), Kubernetes might terminate more pods than your application can tolerate, leading to potential downtime.<br><br>2. Security Concern:<br>While this is more of a reliability issue than a direct security concern, it could indirectly lead to security issues. If your application becomes unavailable, it could trigger a cascading failure in other parts of your system, or it might cause users to fall back to less secure methods. It could also make your application a more attractive target for DoS attacks, as it would be easier to disrupt.<br><br>3. Secure Code Example:<br>To fix this issue, you need to define a PodDisruptionBudget for your deployment. Here's an example:<br><br>```yaml<br>apiVersion: policy/v1beta1<br>kind: PodDisruptionBudget<br>metadata:<br>  name: myapp-pdb<br>spec:<br>  minAvailable: 2  # Adjust this to the minimum number of pods you need<br>  selector:<br>    matchLabels:<br>      app: myapp  # This should match the labels of your deployment's pods<br>```<br><br>Add this to your `kubernetes_vulnerable.yaml` file, adjusting `minAvailable` and `matchLabels` as needed for your application.<br><br>4. Best Practices:<br>To prevent this issue, always define a PodDisruptionBudget for your Kubernetes deployments, especially for production environments. Consider the minimum number of pods your application needs to function and set your PDB accordingly. Regularly review your PDB settings as your application scales and its requirements change. Also, ensure that your team is aware of the importance of PodDisruptionBudgets and knows how to use them properly.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Image Without Digest</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>203</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Images should be specified together with their digests to ensure integrity
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: <br>The security issue "Image Without Digest" means that in your Kubernetes configuration file (kubernetes_vulnerable.yaml), at line 203, you are referencing a container image without specifying its digest. The digest is a unique identifier (like a hash) for a specific version of an image. Without it, Kubernetes might pull a different version of the image than expected.<br><br>2. Security Concern:<br>Not specifying the digest can lead to security issues because an attacker could replace the image on the registry with a malicious one. If the image is not specified with a digest, Kubernetes will pull the latest image from the registry, which could now be the malicious image.<br><br>3. Secure Code Example:<br>To fix this issue, you should specify the digest when referencing the image. Here's an example:<br><br>Instead of:<br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: mypod<br>spec:<br>  containers:<br>  - name: mycontainer<br>    image: myimage:latest<br>```<br><br>Use:<br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: mypod<br>spec:<br>  containers:<br>  - name: mycontainer<br>    image: myimage@sha256:6e4343a6e7a8b8c67c58a2f6a2c1306b5c4a3b9b5aeb2b5f1ecc5d7f5f5784e3<br>```<br>In the second example, `sha256:6e4343a6e7a8b8c67c58a2f6a2c1306b5c4a3b9b5aeb2b5f1ecc5d7f5f5784e3` is the digest of the image.<br><br>4. Best Practices:<br>To prevent this issue, always specify the digest when referencing images in your Kubernetes configuration files. Avoid using tags like `latest` as they can be easily overwritten. Always verify the integrity of the images you are using. Consider using a private registry where you have control over the images and their updates.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/containers/images/#updating-images" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Image Without Digest</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>18</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Images should be specified together with their digests to ensure integrity
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue means that in the Kubernetes configuration file, an image is being used without a specific digest. A digest is a unique identifier for an image, similar to a version number. When you specify an image in a Kubernetes configuration, you can either specify it by its tag (like "latest" or "v1.0") or by its digest. The digest is a SHA256 hash of the image's contents, guaranteeing that the image you're pulling is exactly the one you expect.<br><br>2. Security Concern: If you don't specify an image's digest, you risk pulling a different image than you expect. For example, if an attacker gains control of the image repository, they could replace the image with a malicious one. If you're pulling images by tag, you wouldn't notice the change. But if you're pulling by digest, the pull would fail, alerting you to the issue.<br><br>3. Secure Code Example: <br>   You can fix this by specifying the digest when you declare the image in your Kubernetes configuration. For example:<br>   ```<br>   apiVersion: v1<br>   kind: Pod<br>   metadata:<br>     name: mypod<br>   spec:<br>     containers:<br>     - name: mycontainer<br>       image: myrepo/myimage@sha256:3a54ad57f5f3b16cbb121cd4f5a133a5865f2c125fd8bd9f1c208c63b6325e6e<br>   ```<br><br>4. Best Practices: <br>   - Always use image digests instead of tags in your Kubernetes configurations. This guarantees that you're always pulling the exact image you expect.<br>   - Regularly update your image digests to pull in security updates. You can automate this with a tool like Dependabot.<br>   - Use a private image repository if possible. This reduces the risk of an attacker replacing your images.<br>   - Regularly scan your images for vulnerabilities. You can use a tool like Clair or Anchore for this.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/containers/images/#updating-images" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Image Without Digest</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>182</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Images should be specified together with their digests to ensure integrity
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The security issue "Image Without Digest" means that in your Kubernetes configuration file, you are specifying a Docker image to be used without including its digest. A digest is a unique identifier for a Docker image, similar to a version number or a checksum. When you don't specify a digest, Kubernetes will pull the latest version of the image, which may not be what you intended.<br><br>2. Security Concern:<br>Not specifying a digest is a security concern because it can lead to non-reproducible builds and deployments. If the image is updated, your application might behave differently or even break. More importantly, if a malicious actor gains control of the Docker image repository, they could push a compromised image that your system would then pull and run.<br><br>3. Secure Code Example:<br>Here's how you can fix this issue. Instead of specifying the image like this:<br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: mypod<br>spec:<br>  containers:<br>  - name: mycontainer<br>    image: myrepo/myimage:latest<br>```<br>You should specify it with a digest like this:<br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: mypod<br>spec:<br>  containers:<br>  - name: mycontainer<br>    image: myrepo/myimage@sha256:3a54d377acd6d3b5b8658402902e6b3dd3c277055d5d35fd3f5c9743725e2b39<br>```<br>You can get the digest for an image by running `docker pull myrepo/myimage:latest` and noting the digest that is output.<br><br>4. Best Practices:<br>To prevent this issue, always specify a digest when using Docker images in your Kubernetes configuration files. This ensures that you are always using the exact version of the image that you tested and approved. Also, consider using a private Docker registry where you control all the images and their updates. This adds an extra layer of security by ensuring that only approved images are used in your deployments.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/containers/images/#updating-images" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Image Without Digest</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>88</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Images should be specified together with their digests to ensure integrity
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: <br>This security issue "Image Without Digest" means that in your Kubernetes configuration file (kubernetes_vulnerable.yaml), at line 88, you're using a Docker image for a container without specifying its digest. A digest is a unique identifier of the image content, similar to a hash.<br><br>2. Security Concern: <br>Not specifying the digest can be a security concern because it allows for potential inconsistencies and vulnerabilities. If the image tag changes, it could point to a different image, possibly containing malicious code or vulnerabilities. By using a digest, you ensure that you're always pulling the exact same image, guaranteeing the integrity and immutability of the image.<br><br>3. Secure Code Example: <br>Instead of using just the image name and tag, you should include the digest as well. Here's an example:<br><br>Instead of:<br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: mypod<br>spec:<br>  containers:<br>  - name: mycontainer<br>    image: myimage:1.0<br>```<br>Use:<br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: mypod<br>spec:<br>  containers:<br>  - name: mycontainer<br>    image: myimage@sha256:abc123<br>```<br>In the second example, `sha256:abc123` is the digest of the image.<br><br>4. Best Practices: <br>To prevent this issue, always specify the digest when using Docker images in your Kubernetes configuration files. This ensures that you're always using the exact version of the image that you expect. Also, regularly update the digests to use the latest secure versions of the images. Using a tool to automatically update the digests can help manage this process.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/containers/images/#updating-images" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Invalid Image Tag</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>18</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Image tag must be defined and not be empty or equal to latest.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The security issue 'Invalid Image Tag' refers to the scenario where the image tag in a Kubernetes configuration file is either not defined, empty, or set to 'latest'. Image tags are used to specify the version of the Docker image that should be used when deploying a container. If it's not defined, empty, or set to 'latest', it means that the system will pull the latest version of the image from the Docker repository, which could potentially be unstable or have security vulnerabilities.<br><br>2. Security Concern:<br>Using the 'latest' tag or not specifying a tag can lead to inconsistencies and potential vulnerabilities. If the 'latest' tag is used, it's possible that different nodes in your cluster could be running different versions of the image, leading to unpredictable behavior. Also, if a new version of the image is pushed to the repository with a security vulnerability, your system could automatically pull and run this vulnerable image.<br><br>3. Secure Code Example:<br>Here is an example of how to fix this issue in the Kubernetes configuration file (kubernetes_vulnerable.yaml):<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: my-app<br>spec:<br>  containers:<br>  - name: my-app-container<br>    image: my-app-image:1.0.0<br>```<br>In this example, the image tag is explicitly set to '1.0.0' instead of 'latest' or being left empty.<br><br>4. Best Practices:<br>To prevent this issue, always specify an explicit version for your Docker images in your Kubernetes configuration files. Avoid using the 'latest' tag. This ensures that you're always using a known, stable version of the image. It's also a good practice to regularly update your image tags to use the latest secure and stable versions, and to regularly scan your images for vulnerabilities.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/containers/images/#updating-images" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Invalid Image Tag</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>88</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Image tag must be defined and not be empty or equal to latest.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: <br>This security issue indicates that in the Kubernetes configuration file (kubernetes_vulnerable.yaml), at line 88, an image tag for a Docker container is either not defined, empty, or set to 'latest'. <br><br>2. Security Concern:<br>Using 'latest' or not specifying a tag can lead to unpredictable results as 'latest' does not necessarily mean the most recent image version. It's the default tag when no tag is provided. This can lead to inconsistencies in the environment due to different nodes pulling different versions of the image at different times. Also, it might inadvertently pull an image with vulnerabilities if the latest version hasn't been properly vetted for security issues.<br><br>3. Secure Code Example:<br>Instead of using 'latest' or leaving the tag empty, specify a particular version of the image. For example, if the current line 88 looks like this:<br>```yaml<br>image: myapp:latest<br>```<br>or<br>```yaml<br>image: myapp<br>```<br>Change it to a specific version like:<br>```yaml<br>image: myapp:1.0.1<br>```<br><br>4. Best Practices:<br>- Always use specific tags for Docker images in Kubernetes configuration files. This ensures that you're using a known version of the image.<br>- Regularly update the tags as new, secure versions of the images become available.<br>- Regularly scan your Docker images for vulnerabilities and fix them before deploying.<br>- Implement a policy to disallow the use of 'latest' tag in your IaC scripts.<br>- Use automated tools to check your IaC scripts for such issues before deployment.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/containers/images/#updating-images" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Invalid Image Tag</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>203</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Image tag must be defined and not be empty or equal to latest.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The security issue 'Invalid Image Tag' means that in the Kubernetes configuration file (kubernetes_vulnerable.yaml), on line 203, the image tag for a Docker container is either not defined, empty, or set to 'latest'. <br><br>2. Security Concern:<br>This is a security concern because if the image tag is not defined or empty, Kubernetes will not know which version of the Docker image to pull and run. If the image tag is set to 'latest', Kubernetes will always pull and run the most recent version of the Docker image. This can lead to potential issues as the latest version may not have been thoroughly tested and could contain vulnerabilities. <br><br>3. Secure Code Example:<br>A secure code example would involve specifying a particular version of the Docker image to use. For example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: myapp-pod<br>spec:<br>  containers:<br>  - name: myapp-container<br>    image: myapp:1.0.0<br>```<br><br>In this example, the image tag is '1.0.0', which is a specific version of the 'myapp' Docker image.<br><br>4. Best Practices:<br>To prevent this issue, always specify a particular version of a Docker image to use in your Kubernetes configuration files. Avoid using the 'latest' tag. This ensures that you are always using a known, tested version of the Docker image. Regularly update the image tags in your configuration files as new, tested versions of the Docker images become available.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/containers/images/#updating-images" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Invalid Image Tag</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>182</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Image tag must be defined and not be empty or equal to latest.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: The security issue "Invalid Image Tag" means that the Docker image tag in your Kubernetes configuration file (kubernetes_vulnerable.yaml) at line 182 is either not defined, is empty, or is set to "latest". Docker image tags are used to specify different versions of an image. If it's not defined or set to "latest", it means that the system will always pull the latest version of the image from the Docker registry.<br><br>2. Security Concern: This is a security concern because using the "latest" tag or an undefined tag can lead to unpredictability. If a newer version of the image is pushed to the Docker registry, it will be pulled automatically when the Kubernetes pod is restarted. This new version might have new vulnerabilities, changes in functionality, or it may not be compatible with your application.<br><br>3. Secure Code Example: <br><br>   Here is an example of how to fix it in your Kubernetes configuration file:<br><br>   ```yaml<br>   apiVersion: v1<br>   kind: Pod<br>   metadata:<br>     name: myapp-pod<br>   spec:<br>     containers:<br>     - name: myapp-container<br>       image: myapp:1.0.0<br>   ```<br><br>   In this example, instead of using "latest" or leaving it blank, we specify the image tag as "1.0.0". This ensures that the system will always pull the same version of the image.<br><br>4. Best Practices: <br><br>   - Always use specific image tags in your Kubernetes configuration files. Avoid using the "latest" tag or leaving it blank.<br>   - Regularly update your image tags to use the latest secure versions of your images. This requires you to manually update your configuration files, but it ensures that you are aware of the changes in the new versions.<br>   - Use an image scanner to scan your Docker images for vulnerabilities. This can help you identify insecure versions of your images.<br>   - Implement a policy to regularly review and update your Docker images and Kubernetes configuration files. This can help you stay up-to-date with the latest security practices.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/containers/images/#updating-images" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Missing AppArmor Profile</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>81</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Containers should be configured with an AppArmor profile to enforce fine-grained access control over low-level system resources
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>This security issue means that the container specified in the Kubernetes configuration file (kubernetes_vulnerable.yaml) at line 81 does not have an AppArmor profile configured. AppArmor is a Linux kernel security module that allows the system administrator to restrict programs' capabilities with per-program profiles. Without an AppArmor profile, a container has more access to system resources than it might actually need.<br><br>2. Security Concern:<br>This is a security concern because if a container gets compromised, the attacker could potentially exploit the excessive permissions to gain unauthorized access to system resources, perform malicious activities, or escalate privileges.<br><br>3. Secure Code Example:<br>To fix this issue, you need to define an AppArmor profile for your container in the Kubernetes configuration file. Here is an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  containers:<br>    - name: secure-container<br>      image: my_image<br>      securityContext:<br>        apparmor.security.beta.kubernetes.io/pod: runtime/default<br>```<br>In this example, the `apparmor.security.beta.kubernetes.io/pod` annotation is used to specify the AppArmor profile. The `runtime/default` profile is a good starting point, as it restricts the container's access to the host system resources.<br><br>4. Best Practices:<br>- Always define an AppArmor profile for your containers. If you're unsure which profile to use, start with `runtime/default`.<br>- Regularly review and update your AppArmor profiles to ensure they only grant the minimum necessary permissions.<br>- Use automated tools to check your IaC files for missing security configurations like AppArmor profiles.<br>- Train your team on the importance of container security and the use of security contexts in Kubernetes.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Missing AppArmor Profile</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>7</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Containers should be configured with an AppArmor profile to enforce fine-grained access control over low-level system resources
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>   This security issue means that the Kubernetes configuration file does not specify an AppArmor profile for its containers. AppArmor is a Linux security module that protects an operating system and its applications from security threats. By not specifying an AppArmor profile, the container could have unrestricted access to system resources, which could be exploited by malicious actors.<br><br>2. Security Concern:<br>   Without an AppArmor profile, a container can have more access to system resources than it needs. If an attacker gains control of the container, they could exploit this to gain unauthorized access to system resources, potentially compromising the entire system.<br><br>3. Secure Code Example:<br>   To fix this issue, you should specify an AppArmor profile in your Kubernetes configuration file. Here is an example of how to do it:<br><br>   ```yaml<br>   apiVersion: v1<br>   kind: Pod<br>   metadata:<br>     name: secure-pod<br>   spec:<br>     securityContext:<br>       apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default<br>     containers:<br>     - name: secure-container<br>       image: secure-image<br>   ```<br><br>   In this example, the `apparmor.security.beta.kubernetes.io/defaultProfileName` field is set to `runtime/default`, which is a pre-defined AppArmor profile that restricts the container's access to system resources.<br><br>4. Best Practices:<br>   - Always specify an AppArmor profile for your containers in your Kubernetes configuration files. If possible, use a pre-defined profile like `runtime/default`.<br>   - Regularly update your AppArmor profiles to account for new security threats.<br>   - Limit the access of your containers to system resources as much as possible. The principle of least privilege, which states that a process should only have the minimum privileges it needs to function, should be followed.<br>   - Regularly audit your Kubernetes configuration files for security issues.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Missing AppArmor Profile</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>178</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Containers should be configured with an AppArmor profile to enforce fine-grained access control over low-level system resources
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The security issue here is that the container specified in the Kubernetes configuration file (kubernetes_vulnerable.yaml) at line 178 does not have an AppArmor profile configured. AppArmor is a Linux kernel security module that allows the system administrator to restrict programs' capabilities with per-program profiles. These profiles can control file access permissions, network access, raw socket access, and the execution of other processes, among other things.<br><br>2. Security Concern:<br>Without an AppArmor profile, a container has more access to the system than it might actually need. In the event of a container compromise, this could allow an attacker to exploit these unnecessary permissions to gain further access into the system, potentially leading to data leakage, system damage, or other malicious activities.<br><br>3. Secure Code Example:<br>To fix this, you need to specify an AppArmor profile for the container in your Kubernetes configuration file. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  securityContext:<br>    apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default<br>  containers:<br>  - name: secure-container<br>    image: secure-image<br>    securityContext:<br>      apparmor.security.beta.kubernetes.io/localhost/profile-name: localhost/profile-name<br>```<br><br>In this example, `runtime/default` is the default AppArmor profile provided by Docker, and `localhost/profile-name` is a custom profile that you have created on your host.<br><br>4. Best Practices:<br>- Always define an AppArmor profile for your containers. The profile should be as restrictive as possible, granting only the necessary permissions for your application to function.<br>- Regularly review and update your AppArmor profiles to ensure they are still relevant and secure as your application evolves.<br>- Use automated tools to scan your IaC for missing or misconfigured security controls, such as AppArmor profiles.<br>- Incorporate security considerations into your development process, rather than treating it as an afterthought. This includes training developers on secure coding practices and integrating security checks into your CI/CD pipeline.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Missing AppArmor Profile</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>195</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Containers should be configured with an AppArmor profile to enforce fine-grained access control over low-level system resources
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The security issue "Missing AppArmor Profile" means that the container defined in the Kubernetes configuration file does not have an AppArmor profile associated with it. AppArmor is a Linux kernel security module that allows the system administrator to restrict programs' capabilities with per-program profiles. Without an AppArmor profile, a container has more access to system resources than it might need, which could be a security risk.<br><br>2. Security Concern:<br>Without an AppArmor profile, a container could potentially have access to sensitive system resources, which could be exploited if the container is compromised. This could lead to unauthorized access, data leakage, or even control over the system.<br><br>3. Secure Code Example:<br>To fix this issue, you need to define an AppArmor profile for your container in the Kubernetes configuration file. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: my-pod<br>spec:<br>  containers:<br>  - name: my-container<br>    image: my-image<br>    securityContext:<br>      apparmor.security.beta.kubernetes.io/defaultProfileName: "runtime/default"<br>      apparmor.security.beta.kubernetes.io/localhost/my-custom-profile<br>```<br><br>In this example, the `my-container` container is associated with the `my-custom-profile` AppArmor profile.<br><br>4. Best Practices:<br>- Always define an AppArmor profile for your containers. This limits their access to system resources and reduces potential attack vectors.<br>- Regularly review and update your AppArmor profiles to ensure they only grant the necessary permissions.<br>- Use tools to automate the detection of missing AppArmor profiles in your IaC scripts.<br>- Train your developers on the importance of AppArmor profiles and how to use them.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tutorials/clusters/apparmor/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">No Drop Capabilities for Containers</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>181</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Sees if Kubernetes Drop Capabilities exists to ensure containers security context
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue means that the Kubernetes configuration file does not specify any capabilities to be dropped for the containers it creates. In Linux systems, capabilities are a set of privileges that can be assigned to processes to perform certain high-privilege tasks. When a container is created, it is assigned a set of default capabilities. If no capabilities are dropped, the container retains all these default capabilities, which could potentially be exploited if the container is compromised.<br><br>2. Security Concern: This is a security concern because if a container is compromised, an attacker could potentially use these capabilities to perform malicious activities, such as modifying system files or disrupting system processes. By dropping unnecessary capabilities, the potential damage that can be done by a compromised container is significantly reduced.<br><br>3. Secure Code Example:<br>   Here's how to fix it in the Kubernetes configuration file:<br><br>   ```yaml<br>   apiVersion: v1<br>   kind: Pod<br>   metadata:<br>     name: secure-pod<br>   spec:<br>     containers:<br>     - name: secure-container<br>       image: secure-image<br>       securityContext:<br>         capabilities:<br>           drop:<br>           - ALL<br>           add:<br>           - NET_BIND_SERVICE<br>   ```<br><br>   In this example, we're dropping all capabilities and only adding back the `NET_BIND_SERVICE` capability, which allows the container to bind to low-numbered ports.<br><br>4. Best Practices: <br>   - Always drop all capabilities by default and only add back the ones that are absolutely necessary for the container to function.<br>   - Regularly review the capabilities required by your containers and remove any that are no longer needed.<br>   - Use tools to automate the process of checking for dropped capabilities in your Kubernetes configuration files.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">No Drop Capabilities for Containers</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>202</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Sees if Kubernetes Drop Capabilities exists to ensure containers security context
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The issue "No Drop Capabilities for Containers" means that the Kubernetes configuration file does not include a setting to drop certain capabilities for a container. In Linux, capabilities are a set of privileges that can be used to grant or limit the rights of a process. By not dropping unnecessary capabilities, you are potentially giving more privileges to the container than it needs to function, which can pose a security risk.<br><br>2. Security Concern:<br>The main security concern here is the principle of least privilege, which states that a process should only have the minimum privileges necessary to perform its function. If a container has more privileges than it needs, it becomes a potential target for exploitation. An attacker who gains control of such a container could use the extra privileges to perform malicious actions.<br><br>3. Secure Code Example:<br>Here's an example of how you can drop all capabilities and then add only the necessary ones in your Kubernetes configuration file:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  containers:<br>  - name: secure-container<br>    image: secureimage<br>    securityContext:<br>      capabilities:<br>        drop:<br>        - ALL<br>        add:<br>        - NET_BIND_SERVICE<br>```<br><br>In this example, we first drop all capabilities with `- ALL` and then add back only the `NET_BIND_SERVICE` capability, which might be necessary for the container to function.<br><br>4. Best Practices:<br>To prevent this issue, always follow the principle of least privilege when configuring your containers. Drop all capabilities by default and only add the ones that are necessary. Regularly review your container's capabilities to ensure they are still necessary for its function. Also, consider using tools to automate the detection of unnecessary capabilities in your containers.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">No Drop Capabilities for Containers</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>87</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Sees if Kubernetes Drop Capabilities exists to ensure containers security context
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>   This issue means that the Kubernetes configuration file for a container does not include a setting to drop certain capabilities. In Linux, capabilities are a set of privileges that can be used to perform system-level operations. By default, a container gets a set of default capabilities that provide certain privileges. If these capabilities are not dropped, it could potentially provide an attacker with more privileges than necessary.<br><br>2. Security Concern:<br>   If a container is compromised, the attacker could exploit these extra capabilities to perform malicious activities, such as modifying system files or disrupting system operations. This is a violation of the principle of least privilege, which states that a process should only have the minimum privileges necessary to perform its function.<br><br>3. Secure Code Example:<br>   To fix this issue, you need to add a `securityContext` to your Kubernetes configuration that drops unnecessary capabilities. Here's an example:<br><br>   ```yaml<br>   apiVersion: v1<br>   kind: Pod<br>   metadata:<br>     name: secure-pod<br>   spec:<br>     containers:<br>     - name: secure-container<br>       image: secureimage<br>       securityContext:<br>         capabilities:<br>           drop:<br>           - ALL<br>           add:<br>           - NET_BIND_SERVICE<br>   ```<br>   In this example, we're dropping all capabilities and then only adding back the `NET_BIND_SERVICE` capability, which might be necessary for the container's operation.<br><br>4. Best Practices:<br>   - Always follow the principle of least privilege when configuring your containers. Only grant the capabilities that are absolutely necessary for the container's operation.<br>   - Regularly review and update your container configurations to ensure that they are secure.<br>   - Use automated tools to scan your Infrastructure as Code (IaC) for security issues.<br>   - Educate your team about the importance of container security and best practices.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">No Drop Capabilities for Containers</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>24</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Sees if Kubernetes Drop Capabilities exists to ensure containers security context
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The issue "No Drop Capabilities for Containers" means that the Kubernetes configuration file does not specify any capabilities to be dropped for the containers it's deploying. In Linux systems, capabilities are a set of privileges that can be used by processes to perform certain operations that would otherwise require root access. By not dropping any capabilities, the container may have more privileges than it actually needs to function, which can be a security risk.<br><br>2. Security Concern:<br>The concern here is the principle of least privilege, which states that a process should only have the minimum privileges necessary to perform its function. If a container has more privileges than necessary, it can be exploited by an attacker to perform unauthorized operations. For example, an attacker who gains access to a container with the CAP_NET_RAW capability could potentially sniff network traffic or spoof IP addresses.<br><br>3. Secure Code Example:<br>To fix this issue, you need to specify the capabilities to be dropped in the securityContext of the container specification. Here is an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: security-context-demo<br>spec:<br>  containers:<br>  - name: sec-ctx-demo<br>    image: gcr.io/google-samples/node-hello:1.0<br>    securityContext:<br>      capabilities:<br>        drop:<br>        - ALL<br>```<br>In this example, the "drop: - ALL" line tells Kubernetes to drop all capabilities from the container. You can replace "ALL" with specific capability names if you know exactly what capabilities your container does not need.<br><br>4. Best Practices:<br>To prevent this issue, always follow the principle of least privilege when configuring your containers. Only give them the capabilities they need to function, and drop all others. Regularly review your container configurations to ensure they are still appropriate as your application evolves. Use automated tools to scan your IaC for security issues, as they can help catch problems that manual review may miss.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Pod or Container Without LimitRange</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>74</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Each namespace should have a LimitRange policy associated to ensure that resource allocations of Pods, Containers and PersistentVolumeClaims do not exceed the defined boundaries
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue means that there is a Kubernetes pod or container in your Infrastructure as a Code (IaC) script that does not have a LimitRange policy associated with it. A LimitRange policy in Kubernetes sets constraints on the maximum or minimum amount of compute resources that a pod or container can use. These resources include CPU, memory, and storage.<br><br>2. Security Concern: Without a LimitRange, a pod or container could potentially consume all available resources in a cluster, leading to a Denial of Service (DoS) attack. This could make other services in the cluster unavailable, impacting the overall performance and stability of the system.<br><br>3. Secure Code Example:<br>```yaml<br>apiVersion: v1<br>kind: LimitRange<br>metadata:<br>  name: resource-limits<br>spec:<br>  limits:<br>  - default:<br>      memory: 512Mi<br>      cpu: 500m<br>    defaultRequest:<br>      memory: 256Mi<br>      cpu: 250m<br>    type: Container<br>```<br>This example sets a default limit of 512Mi of memory and 500m CPU for each container. It also sets a default request of 256Mi of memory and 250m CPU. This ensures that each container has enough resources to run and prevents any single container from consuming all available resources.<br><br>4. Best Practices: <br>   - Always define a LimitRange for every namespace in your Kubernetes cluster.<br>   - Regularly review and update your LimitRanges based on the needs of your applications and the capacity of your cluster.<br>   - Use monitoring tools to track resource usage and ensure that your limits are appropriate.<br>   - Educate your team about the importance of resource limits and how to use them effectively.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/policy/limit-range/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Pod or Container Without LimitRange</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>178</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Each namespace should have a LimitRange policy associated to ensure that resource allocations of Pods, Containers and PersistentVolumeClaims do not exceed the defined boundaries
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>   This security issue is about a Kubernetes configuration file where a pod or container is defined without a LimitRange. A LimitRange is a policy to constrain resource allocations (CPU, memory, etc.) for pods, containers, and PersistentVolumeClaims in a namespace. If a pod or container doesn't have a LimitRange, it could potentially consume all available resources, affecting other services running in the same namespace.<br><br>2. Security Concern:<br>   If a pod or container doesn't have resource limits, it could be exploited by an attacker to perform a Denial of Service (DoS) attack by consuming all available resources, causing other services to become unavailable. Also, a misbehaving or inefficient application could unintentionally consume excessive resources, impacting other services.<br><br>3. Secure Code Example:<br>   To fix this issue, you need to define a LimitRange in the namespace where the pod or container is running. Here is an example:<br><br>   ```yaml<br>   apiVersion: v1<br>   kind: LimitRange<br>   metadata:<br>     name: resource-limits<br>     namespace: example-namespace<br>   spec:<br>     limits:<br>     - type: Pod<br>       max:<br>         cpu: "2"<br>         memory: "1Gi"<br>       min:<br>         cpu: "200m"<br>         memory: "100Mi"<br>     - type: Container<br>       max:<br>         cpu: "1"<br>         memory: "500Mi"<br>       min:<br>         cpu: "100m"<br>         memory: "50Mi"<br>   ```<br><br>4. Best Practices:<br>   - Always define a LimitRange for each namespace in your Kubernetes cluster.<br>   - Regularly review and adjust the resource limits based on the actual usage and requirements of your applications.<br>   - Use Kubernetes Resource Quotas to set hard limits on the total amount of resources that can be used in a namespace.<br>   - Regularly use security scanning tools to identify any potential security issues in your IaC configurations.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/policy/limit-range/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Pod or Container Without LimitRange</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>7</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Each namespace should have a LimitRange policy associated to ensure that resource allocations of Pods, Containers and PersistentVolumeClaims do not exceed the defined boundaries
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The issue "Pod or Container Without LimitRange" means that there are Kubernetes pods or containers in your infrastructure that do not have a LimitRange policy set. A LimitRange policy in Kubernetes specifies the minimum and maximum compute resources that are allowed for objects like Pods, Containers, and PersistentVolumeClaims within a namespace.<br><br>2. Security Concern:<br>Without a LimitRange, a Pod or Container could potentially consume all available resources in a namespace. This can lead to resource exhaustion, affecting other services running in the same namespace. It can also lead to potential Denial of Service (DoS) attacks where an attacker intentionally consumes all resources.<br><br>3. Secure Code Example:<br>To fix this issue, you can define a LimitRange in your Kubernetes YAML file. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: LimitRange<br>metadata:<br>  name: resource-limits<br>spec:<br>  limits:<br>  - default:<br>      memory: 512Mi<br>      cpu: "1"<br>    defaultRequest:<br>      memory: 256Mi<br>      cpu: "0.5"<br>    type: Container<br>```<br>In this example, the LimitRange named 'resource-limits' sets default memory limit to 512Mi and CPU to 1 core for each container in the namespace. It also sets default memory request to 256Mi and CPU to 0.5 core.<br><br>4. Best Practices:<br>- Always define a LimitRange for each namespace in your Kubernetes cluster.<br>- Regularly review and adjust these limits based on the actual resource usage of your applications.<br>- Use Kubernetes Resource Quotas to set hard limits on the total amount of resources that can be consumed in a namespace.<br>- Implement monitoring and alerting for resource usage to detect any abnormal behavior or resource exhaustion.<br>- Regularly update and patch your Kubernetes clusters to the latest version to benefit from the latest security enhancements.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/policy/limit-range/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Pod or Container Without LimitRange</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>165</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Each namespace should have a LimitRange policy associated to ensure that resource allocations of Pods, Containers and PersistentVolumeClaims do not exceed the defined boundaries
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This issue means that there is a Kubernetes Pod or Container defined in the Infrastructure as Code (IaC) file that does not have a LimitRange policy associated with it. A LimitRange policy in Kubernetes sets constraints on the maximum and minimum resource usage (like CPU, memory, storage) per Pod, Container, or PersistentVolumeClaim in a namespace. If a Pod or Container doesn't have a LimitRange, it could potentially consume all available resources in the cluster, causing other services to fail due to resource starvation.<br><br>2. Security Concern: This is a security concern because an attacker could exploit this configuration to cause a Denial of Service (DoS) attack by creating a Pod or Container that consumes all available resources, thereby disrupting other services running in the same cluster. It also poses a risk of cost overrun as cloud providers typically charge based on resource usage.<br><br>3. Secure Code Example: <br>   Here is an example of how to define a LimitRange in the Kubernetes YAML file:<br><br>   ```yaml<br>   apiVersion: v1<br>   kind: LimitRange<br>   metadata:<br>     name: resource-limits<br>     namespace: default<br>   spec:<br>     limits:<br>     - type: Pod<br>       max:<br>         cpu: "2"<br>         memory: "1Gi"<br>       min:<br>         cpu: "200m"<br>         memory: "100Mi"<br>     - type: Container<br>       max:<br>         cpu: "2"<br>         memory: "1Gi"<br>       min:<br>         cpu: "100m"<br>         memory: "50Mi"<br>   ```<br><br>   In this example, the maximum CPU limit for a Pod is 2 CPU units and the maximum memory is 1Gi. The minimum CPU limit is 200m (milliCPU units) and the minimum memory is 100Mi. Similarly, for each Container, the maximum and minimum limits are defined.<br><br>4. Best Practices: <br>   - Always define LimitRange for each namespace in your Kubernetes cluster to prevent excessive resource usage.<br>   - Regularly review and adjust the LimitRange values based on the actual resource usage of your Pods and Containers.<br>   - Use tools like Kubernetes Dashboard or command-line tools to monitor the resource usage of your Pods and Containers.<br>   - Incorporate Infrastructure as Code (IaC) security scanning in your CI/CD pipeline to catch such issues early in the development cycle.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/policy/limit-range/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Pod or Container Without LimitRange</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>195</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Each namespace should have a LimitRange policy associated to ensure that resource allocations of Pods, Containers and PersistentVolumeClaims do not exceed the defined boundaries
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The security issue is that a Kubernetes pod or container does not have a LimitRange policy associated with it. A LimitRange policy in Kubernetes is used to specify the minimum and maximum resources that can be used by a pod, container, or PersistentVolumeClaim in a particular namespace. Without a LimitRange, there's no limit on the resources that a pod or container can consume, which could potentially lead to resource exhaustion.<br><br>2. Security Concern:<br>This is a security concern because if a pod or container is allowed to consume unlimited resources, it could potentially exhaust the resources of the node it's running on. This could lead to a Denial of Service (DoS) situation where other pods or containers on the same node are starved of resources and cannot function properly. Additionally, if a container is compromised, the attacker could consume excessive resources, affecting other services.<br><br>3. Secure Code Example:<br>To fix this issue, you need to define a LimitRange in your Kubernetes configuration. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: LimitRange<br>metadata:<br>  name: resource-limits<br>spec:<br>  limits:<br>  - default:<br>      memory: 512Mi<br>      cpu: 500m<br>    defaultRequest:<br>      memory: 256Mi<br>      cpu: 250m<br>    type: Container<br>```<br><br>In this example, a LimitRange named 'resource-limits' is defined. The default memory limit is set to 512Mi and the default CPU limit is set to 500m. The defaultRequest is the amount of resources that a container will request by default if no specific request is made.<br><br>4. Best Practices:<br>- Always define a LimitRange for each namespace in your Kubernetes cluster to prevent pods or containers from consuming excessive resources.<br>- Regularly review and adjust your LimitRanges as necessary based on the actual resource usage of your pods and containers.<br>- Use ResourceQuotas in conjunction with LimitRanges to control the total amount of resources that can be consumed in a namespace.<br>- Use monitoring and alerting tools to keep track of resource usage in your Kubernetes cluster and to get notified when resource usage approaches the limits.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/policy/limit-range/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Pod or Container Without ResourceQuota</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>178</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Each namespace should have a ResourceQuota policy associated to limit the total amount of resources Pods, Containers and PersistentVolumeClaims can consume
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The issue here is that a Kubernetes Pod or Container is not having a ResourceQuota policy. A ResourceQuota policy is a set of rules that limit the total amount of compute resources (like CPU, memory, storage, etc.) that Pods, Containers, and PersistentVolumeClaims can consume in a namespace. Without this policy, a Pod or Container can potentially consume all available resources, leading to resource exhaustion.<br><br>2. Security Concern:<br>The absence of a ResourceQuota policy can lead to a Denial of Service (DoS) attack where a malicious Pod or Container consumes all available resources, causing other legitimate services to fail due to lack of resources. It also makes it difficult to manage and allocate resources efficiently across different services.<br><br>3. Secure Code Example:<br>To fix this issue, you need to define a ResourceQuota policy in your Kubernetes YAML file. Below is an example:<br><br>```yaml<br>apiVersion: v1<br>kind: ResourceQuota<br>metadata:<br>  name: compute-resources<br>  namespace: default<br>spec:<br>  hard:<br>    pods: "100"<br>    requests.cpu: "1"<br>    requests.memory: 1Gi<br>    limits.cpu: "2"<br>    limits.memory: 2Gi<br>```<br>In this example, we are limiting the total number of Pods to 100, total CPU requests to 1 core, total memory requests to 1Gi, total CPU limits to 2 cores, and total memory limits to 2Gi in the 'default' namespace.<br><br>4. Best Practices:<br>- Always define a ResourceQuota for each namespace in your Kubernetes cluster to prevent resource exhaustion.<br>- Regularly review and adjust your ResourceQuota policies based on the actual resource usage of your services.<br>- Use Namespace quotas to ensure that resources are fairly distributed among multiple users or teams.<br>- Use tools like Kubernetes Dashboard or kubectl to monitor the resource usage of your services and enforce ResourceQuota policies.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Pod or Container Without ResourceQuota</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>7</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Each namespace should have a ResourceQuota policy associated to limit the total amount of resources Pods, Containers and PersistentVolumeClaims can consume
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The security issue here is that a Kubernetes pod or container is running without a ResourceQuota. In Kubernetes, a ResourceQuota is a policy that sets limits on the total amount of compute resources (like CPU, memory, or storage) that a namespace can use. A namespace without a ResourceQuota is like a car without a speed limit - it can consume as many resources as it wants, which could potentially starve other applications or services running on the same system.<br><br>2. Security Concern:<br>This is a security concern because an attacker could exploit this to perform a Denial of Service (DoS) attack. They could run a process that consumes an excessive amount of resources, causing other applications to slow down or even crash due to lack of resources. This could lead to service disruption and potential data loss.<br><br>3. Secure Code Example:<br>To fix this issue, you need to define a ResourceQuota for the namespace. Here's an example of how to do this in a Kubernetes YAML file:<br><br>```yaml<br>apiVersion: v1<br>kind: ResourceQuota<br>metadata:<br>  name: compute-resources<br>  namespace: default<br>spec:<br>  hard:<br>    pods: "10"<br>    requests.cpu: "4"<br>    requests.memory: 5Gi<br>    limits.cpu: "10"<br>    limits.memory: 10Gi<br>```<br><br>This ResourceQuota sets a limit of 10 pods, a request limit of 4 CPUs and 5Gi memory, and a hard limit of 10 CPUs and 10Gi memory.<br><br>4. Best Practices:<br>To prevent this issue, always define a ResourceQuota for each namespace in your Kubernetes cluster. Monitor the resource usage regularly to adjust the limits as needed. Also, consider using LimitRange objects to set default, minimum, and maximum resource limits on a per-pod or per-container basis. This can prevent a single pod or container from consuming all available resources.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Pod or Container Without ResourceQuota</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>74</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Each namespace should have a ResourceQuota policy associated to limit the total amount of resources Pods, Containers and PersistentVolumeClaims can consume
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The security issue "Pod or Container Without ResourceQuota" refers to a situation where a Kubernetes pod or container has been configured without setting a limit on the resources it can consume. This means that the pod or container can potentially use up all available resources, causing other services to suffer or even fail due to lack of resources.<br><br>2. Security Concern:<br>This is a security concern because an attacker could exploit this configuration to perform a Denial of Service (DoS) attack. By sending a large amount of requests to the pod or container without a ResourceQuota, the attacker could cause it to consume all available resources, thereby disrupting other services running on the same infrastructure.<br><br>3. Secure Code Example:<br>To fix this issue, you should define a ResourceQuota for each namespace in your Kubernetes configuration. Here is an example:<br><br>```yaml<br>apiVersion: v1<br>kind: ResourceQuota<br>metadata:<br>  name: compute-resources<br>  namespace: test-namespace<br>spec:<br>  hard:<br>    pods: "10"<br>    requests.cpu: "4"<br>    requests.memory: 5Gi<br>    limits.cpu: "10"<br>    limits.memory: 10Gi<br>```<br>In this example, the ResourceQuota named 'compute-resources' is created in the 'test-namespace' namespace. It sets hard limits on the number of pods (10), CPU requests (4 cores), memory requests (5Gi), CPU limits (10 cores), and memory limits (10Gi).<br><br>4. Best Practices:<br>To prevent this issue, always define a ResourceQuota for each namespace in your Kubernetes configuration. This will ensure that each pod or container can only consume a limited amount of resources. Also, regularly review and adjust these limits as necessary based on the actual resource usage of your pods and containers. This will help to optimize resource utilization and prevent potential DoS attacks.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Pod or Container Without ResourceQuota</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>165</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Each namespace should have a ResourceQuota policy associated to limit the total amount of resources Pods, Containers and PersistentVolumeClaims can consume
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue means that there is a pod or container in your Kubernetes configuration that does not have a ResourceQuota policy associated with it. A ResourceQuota policy is a set of rules that limit the total amount of resources (like CPU, memory, storage, etc.) that pods, containers, and persistent volume claims can consume within a namespace.<br><br>2. Security Concern: Without a ResourceQuota, a pod or container could potentially consume all available resources in a namespace, causing other pods or containers to starve and not function properly. This could lead to service disruption and could be exploited by malicious users to perform denial-of-service attacks.<br><br>3. Secure Code Example:<br>Here is an example of how to define a ResourceQuota in your Kubernetes configuration:<br><br>```yaml<br>apiVersion: v1<br>kind: ResourceQuota<br>metadata:<br>  name: compute-resources<br>  namespace: test-namespace<br>spec:<br>  hard:<br>    pods: "10"<br>    requests.cpu: "4"<br>    requests.memory: 5Gi<br>    limits.cpu: "10"<br>    limits.memory: 10Gi<br>```<br><br>In this example, the ResourceQuota named `compute-resources` is limiting the total number of pods to 10, the total CPU requests to 4 cores, the total memory requests to 5Gi, the total CPU limits to 10 cores, and the total memory limits to 10Gi in the `test-namespace`.<br><br>4. Best Practices: To prevent this issue, always define a ResourceQuota for each namespace in your Kubernetes configuration. This will ensure that the resources are fairly distributed among all pods and containers. Also, regularly review and adjust these quotas as needed based on the actual resource usage and requirements of your applications.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Pod or Container Without ResourceQuota</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>195</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Each namespace should have a ResourceQuota policy associated to limit the total amount of resources Pods, Containers and PersistentVolumeClaims can consume
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The issue indicates that there is a Kubernetes pod or container in your infrastructure that does not have a ResourceQuota policy associated with it. A ResourceQuota policy is used to limit the total amount of resources (like CPU, memory, and storage) that a namespace can consume. Without this policy, a pod or container can potentially consume all available resources, causing other services to starve and degrade the performance of the system.<br><br>2. Security Concern:<br>This is a security concern because an attacker could exploit this by running a Denial of Service (DoS) attack, causing the pod or container to consume all available resources, which would disrupt the operation of other services. Additionally, it could lead to cost overruns if you're paying for resources based on usage.<br><br>3. Secure Code Example:<br>You can fix this issue by defining a ResourceQuota in your Kubernetes YAML file. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: ResourceQuota<br>metadata:<br>  name: compute-resources<br>  namespace: test-namespace<br>spec:<br>  hard:<br>    pods: "10"<br>    requests.cpu: "4"<br>    requests.memory: 5Gi<br>    limits.cpu: "10"<br>    limits.memory: 10Gi<br>```<br><br>This ResourceQuota policy limits the namespace `test-namespace` to a maximum of 10 pods, with a total of 4 CPUs and 5Gi of memory requested, and a limit of 10 CPUs and 10Gi of memory.<br><br>4. Best Practices:<br>To prevent this issue, always define a ResourceQuota for each namespace in your Kubernetes cluster. This will ensure that resources are fairly distributed among all services and prevent any single service from consuming all available resources. Additionally, regularly review and adjust these quotas based on the actual resource usage of your services.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Pod or Container Without Security Context</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>196</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> A security context defines privilege and access control settings for a Pod or Container
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>This security issue means that the Kubernetes pod or container defined in the file "kubernetes_vulnerable.yaml" at line 196 does not have a security context. A security context is a property in Kubernetes that controls the security parameters that your pod or container operates under. It can define things like what user the process in the container runs as, what capabilities the container has, and whether the file system of the container is read-only.<br><br>2. Security Concern:<br>Without a security context, a pod or container might run with more privileges than it needs, potentially allowing malicious users to exploit these privileges. For example, if a container runs as root (the default if no security context is specified), a successful attack on the container could allow the attacker to do anything they want within the container, including modifying files or installing malicious software.<br><br>3. Code Example:<br>Here is an example of how you can define a security context for a pod in a Kubernetes YAML file:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  securityContext:<br>    runAsUser: 1000<br>    runAsGroup: 3000<br>    fsGroup: 2000<br>  containers:<br>  - name: secure-container<br>    image: secureimage<br>```<br>In this example, the pod will run as user 1000, group 3000, and any files it creates will be owned by group 2000.<br><br>4. Best Practices:<br>To prevent this issue, always define a security context for your pods and containers. Make sure to follow the principle of least privilege, giving your pods and containers only the permissions they need to do their job. Regularly review your security contexts to ensure they are still appropriate as your application evolves. Use tools to automate the checking of your Kubernetes YAML files for missing or inappropriate security contexts.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Pod or Container Without Security Context</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>202</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> A security context defines privilege and access control settings for a Pod or Container
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue means that a Pod or Container in your Kubernetes configuration file does not have a defined security context. A security context is a set of security settings that control the access and privileges of a Pod or Container. Without a security context, your Pod or Container might have more permissions than it needs, potentially exposing your system to security risks.<br><br>2. Security Concern: The absence of a security context can lead to privilege escalation where a Pod or Container can access resources it shouldn't be able to. This can potentially lead to unauthorized data access, data corruption, or even a complete takeover of the system by malicious actors.<br><br>3. Secure Code Example:<br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  containers:<br>  - name: secure-container<br>    image: secure-image<br>    securityContext:<br>      runAsUser: 1000<br>      runAsGroup: 3000<br>      fsGroup: 2000<br>```<br>In this example, the `securityContext` field is added to the container configuration. It specifies that the container should run as a non-root user (user ID 1000 and group ID 3000) and the file system group ID should be 2000.<br><br>4. Best Practices: To prevent this issue, always define a security context for your Pods and Containers. Minimize the permissions granted to each Pod or Container to only what is necessary for its operation (principle of least privilege). Regularly review and update these settings as needed. Use automated tools to scan your Infrastructure as Code (IaC) for missing or incorrect security contexts.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Pod or Container Without Security Context</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>87</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> A security context defines privilege and access control settings for a Pod or Container
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The issue "Pod or Container Without Security Context" means that the Kubernetes configuration file does not include a security context for a specific pod or container. A security context is a property in Kubernetes that controls the security parameters that a pod or container should adhere to. It can define things like whether the pod or container runs as a privileged user, what user/group to run the process inside the container, and whether to allow privilege escalation.<br><br>2. Security Concern:<br>Without a security context, a pod or container in Kubernetes might run with default settings which could be insecure. For example, it might run as a root user, which would give it full control over the system. This could lead to unauthorized access or malicious activities if the pod or container is compromised.<br><br>3. Secure Code Example:<br>Here is an example of how to add a security context to a Kubernetes configuration file:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  containers:<br>  - name: secure-container<br>    image: secureimage<br>    securityContext:<br>      runAsUser: 1000<br>      runAsGroup: 3000<br>      allowPrivilegeEscalation: false<br>```<br>In this example, the `securityContext` is set to run the process in the container as a non-root user (user ID 1000 and group ID 3000) and disallow privilege escalation.<br><br>4. Best Practices:<br>- Always define a security context for your pods and containers in Kubernetes.<br>- Avoid running containers as root whenever possible. Instead, use a less privileged user.<br>- Set `allowPrivilegeEscalation` to false to prevent a process from gaining more privileges than its parent process.<br>- Regularly review and update your security contexts as part of your code review and deployment processes.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Pod or Container Without Security Context</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>181</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> A security context defines privilege and access control settings for a Pod or Container
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue indicates that a Kubernetes Pod or Container in your Infrastructure as Code (IaC) setup does not have a security context defined. A security context is a set of security parameters that control the security settings of a Pod or Container, such as its privileges, access controls, and whether it can run as a root user.<br><br>2. Security Concern: Without a security context, a Pod or Container may run with default settings, which could potentially be insecure. For example, it could run with root privileges, making it a potential target for attackers to exploit and gain control over the entire system.<br><br>3. Secure Code Example:<br>   Here is a simple example of how to define a security context for a Pod in a Kubernetes YAML file:<br><br>   ```yaml<br>   apiVersion: v1<br>   kind: Pod<br>   metadata:<br>     name: secure-pod<br>   spec:<br>     securityContext:<br>       runAsUser: 1000<br>       runAsGroup: 3000<br>       fsGroup: 2000<br>     containers:<br>     - name: secure-container<br>       image: my_image<br>   ```<br><br>   In this example, the `runAsUser`, `runAsGroup`, and `fsGroup` parameters specify the user ID, group ID, and file system group ID that the Pod will run as, respectively. This ensures that the Pod does not run with root privileges.<br><br>4. Best Practices: To prevent this issue, always define a security context for your Pods and Containers. Avoid running them with root privileges unless absolutely necessary. Regularly review your IaC configurations to ensure they adhere to security best practices. Use automated tools to scan your IaC for potential security issues.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Pod or Container Without Security Context</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>179</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> A security context defines privilege and access control settings for a Pod or Container
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue means that a Pod or Container in a Kubernetes configuration file does not have a security context defined. A security context is a set of security settings that control the access and privileges of a Pod or Container. It can include settings like whether the process runs as a privileged user, what user/group the process runs as, and whether the filesystem is read-only.<br><br>2. Security Concern: Without a security context, a Pod or Container may run with default settings that could be insecure. For example, it could run as a root user, which would give it full system privileges. If an attacker manages to exploit a vulnerability in the application running in the Pod or Container, they could potentially gain full control over the system.<br><br>3. Secure Code Example: Here's an example of how to define a security context in a Kubernetes configuration file:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  containers:<br>  - name: secure-container<br>    image: myimage<br>    securityContext:<br>      runAsUser: 1000<br>      runAsGroup: 3000<br>      readOnlyRootFilesystem: true<br>```<br>In this example, the `securityContext` specifies that the process in the container should run as user ID 1000 and group ID 3000, and that the root filesystem should be read-only.<br><br>4. Best Practices: To prevent this issue, always define a security context for your Pods and Containers. Make sure to specify a non-root user and group for the process to run as, and make it a habit to set the root filesystem as read-only if possible. Regularly review your security contexts to ensure they follow the principle of least privilege, meaning they only have the minimum permissions necessary to function.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Root Container Not Mounted Read-only</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>202</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Check if the root container filesystem is not being mounted as read-only.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: This security issue means that the root filesystem of the container in your Kubernetes configuration is not set to be read-only. This means that once inside the container, it's possible to write or modify the files in the root filesystem.<br><br>2. Security Concern: If an attacker gains access to the container, they could potentially modify the container's root filesystem, which could lead to unauthorized changes, data corruption, or even the introduction of malicious software. This could compromise the security and integrity of your application and data.<br><br>3. Secure Code Example: To fix this, you need to set the root filesystem of the container to be read-only. In your Kubernetes configuration file (YAML), this can be done by adding `readOnlyRootFilesystem: true` under the securityContext of the container spec. Here is an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  containers:<br>  - name: secure-container<br>    image: secure-image<br>    securityContext:<br>      readOnlyRootFilesystem: true<br>```<br><br>4. Best Practices: To prevent this issue, always ensure that the root filesystem of your containers is set to read-only, unless there is a specific need for write access. This should be part of your standard security practices for container configuration. Additionally, consider using automated tools to scan your Infrastructure as Code (IaC) for potential security issues. Regularly review and update your security practices to keep up with evolving threats and best practices.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Root Container Not Mounted Read-only</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>87</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Check if the root container filesystem is not being mounted as read-only.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The issue "Root Container Not Mounted Read-only" means that the root filesystem of the container, in this case, is not set to read-only mode. This implies that the container has write access to the root filesystem, which is not a recommended practice.<br><br>2. Security Concern:<br>If the root filesystem is not mounted as read-only, it opens up potential security vulnerabilities. An attacker who gains access to the container could write or modify the files in the root filesystem, potentially leading to data corruption, data loss, or even a system compromise.<br><br>3. Secure Code Example:<br>To fix this issue, you need to set the root filesystem of the container to read-only. In Kubernetes, this can be done in the security context of the pod specification. Here is an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  containers:<br>  - name: secure-container<br>    image: secureimage<br>    securityContext:<br>      readOnlyRootFilesystem: true<br>```<br>In this example, `readOnlyRootFilesystem: true` ensures that the root filesystem of the container is mounted as read-only.<br><br>4. Best Practices:<br>- Always set the root filesystem of the container to read-only unless there is a specific need for write access.<br>- Regularly review and update your Infrastructure as Code (IaC) configurations to ensure they follow security best practices.<br>- Use automated tools to scan your IaC for potential security issues.<br>- Implement least privilege principle - only grant the permissions that are absolutely necessary for the container to function.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Root Container Not Mounted Read-only</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>181</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Check if the root container filesystem is not being mounted as read-only.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The security issue "Root Container Not Mounted Read-only" means that the root filesystem of the container is not set to read-only mode. In other words, the container has write access to the root filesystem, which is not a recommended practice from a security perspective.<br><br>2. Security Concern:<br>If a container's root filesystem is not mounted as read-only, it means that the container can modify the files in the root filesystem. This can be a security concern because if the container gets compromised, an attacker could modify or delete critical system files, leading to potential system instability or further security breaches.<br><br>3. Code Fix:<br>In your Kubernetes configuration file (in this case `kubernetes_vulnerable.yaml`), you should add the `readOnlyRootFilesystem: true` under the securityContext of the container spec. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  containers:<br>  - name: secure-container<br>    image: secureimage<br>    securityContext:<br>      readOnlyRootFilesystem: true<br>```<br><br>This configuration ensures that the root filesystem of the container is mounted as read-only.<br><br>4. Best Practices:<br>- Always set the `readOnlyRootFilesystem` to `true` for your containers unless there's a specific need for write access.<br>- Regularly review your Infrastructure as Code (IaC) configurations for security best practices.<br>- Use automated tools to scan your IaC for potential security vulnerabilities.<br>- Limit the privileges of your containers to the minimum necessary for their function.<br>- Regularly update and patch your container images to the latest secure versions.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge low">LOW</span>
                        <h3 class="finding-title">Root Container Not Mounted Read-only</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>23</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Check if the root container filesystem is not being mounted as read-only.
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Plain English Explanation:<br>This security issue means that the root filesystem of the container, in the Kubernetes configuration file, is not set to read-only. This means that once the container is running, it can write to its own root filesystem.<br><br>2. Why it's a security concern:<br>If a container can write to its own root filesystem, it could potentially be exploited by an attacker to install malicious software or modify system files, which could compromise the container or the entire system. This is especially concerning if the container is running with root privileges.<br><br>3. Secure Code Example:<br>You can fix this issue by setting the `readOnlyRootFilesystem` field to `true` in the security context of your Kubernetes pod specification. Here is an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secure-pod<br>spec:<br>  containers:<br>  - name: secure-container<br>    image: myimage:1.0<br>    securityContext:<br>      readOnlyRootFilesystem: true<br>```<br><br>In this example, the `readOnlyRootFilesystem` field is set to `true`, which means that the root filesystem of the container will be mounted as read-only.<br><br>4. Best Practices:<br>- Always set the `readOnlyRootFilesystem` field to `true` unless the container specifically requires write access to its root filesystem.<br>- Regularly review and update your container and Kubernetes configurations to ensure they are following the latest security best practices.<br>- Use automated tools to scan your Infrastructure as Code (IaC) for security issues.<br>- Follow the principle of least privilege, i.e., only grant the minimum permissions necessary for a container to function.<br>- Don't run containers with root privileges unless absolutely necessary.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                </div><div id="info-section" class="severity-section"><h2 class="severity-header info">INFO (5)</h2>
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge info">INFO</span>
                        <h3 class="finding-title">Liveness Probe Is Not Defined</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>87</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> In case of an unresponsive container, a Liveness Probe can help your application become more available since it restarts the container. However, it can lead to cascading failures. Define one if you really need it
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: <br>The issue here is that a "Liveness Probe" is not defined in your Kubernetes configuration file. A Liveness Probe is a diagnostic tool used in Kubernetes to check if a container in a pod is still running. If the container is unresponsive, Kubernetes will automatically restart it. However, it's not always necessary and can lead to cascading failures if not properly managed.<br><br>2. Security Concern: <br>While this is more of a reliability issue than a direct security concern, an unresponsive or malfunctioning container could potentially be exploited by an attacker. If a service is down, it may also lead to other security mechanisms not working as expected, leaving your application vulnerable.<br><br>3. Code Example: <br>You can define a Liveness Probe in your Kubernetes configuration file like this:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  labels:<br>    test: liveness<br>  name: liveness-http<br>spec:<br>  containers:<br>  - name: liveness<br>    image: k8s.gcr.io/liveness<br>    args:<br>    - /server<br>    livenessProbe:<br>      httpGet:<br>        path: /healthz<br>        port: 8080<br>      initialDelaySeconds: 3<br>      periodSeconds: 3<br>```<br>In this example, Kubernetes will make an HTTP GET request to the '/healthz' endpoint on port 8080 of the container every 3 seconds, after an initial delay of 3 seconds. If the endpoint returns a status code less than 200 or greater than or equal to 400, Kubernetes will restart the container.<br><br>4. Best Practices: <br>- Always define a Liveness Probe for your containers if you expect them to be long-lived and there's a risk they might hang or crash.<br>- Be careful with the parameters of the Liveness Probe. Setting the 'periodSeconds' too low could lead to unnecessary restarts, and setting it too high could mean a long delay before a failed container is restarted.<br>- Consider using a Readiness Probe in addition to a Liveness Probe. A Readiness Probe checks if a container is ready to start accepting traffic. This can prevent a container from being used before it's fully started up.<br>- Monitor the results of your Liveness Probes. If a container is being restarted frequently, it could indicate a problem with the application.<br>- Test your Liveness Probes to ensure they are correctly identifying when a container is unhealthy.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#when-should-you-use-a-liveness-probe" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge info">INFO</span>
                        <h3 class="finding-title">Liveness Probe Is Not Defined</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>181</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> In case of an unresponsive container, a Liveness Probe can help your application become more available since it restarts the container. However, it can lead to cascading failures. Define one if you really need it
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The security issue "Liveness Probe Is Not Defined" means that there is no mechanism in place to check if a container in the Kubernetes application is still functioning correctly. A liveness probe is a diagnostic tool that periodically checks the health of a container. If the container is unresponsive, the liveness probe will automatically restart it.<br><br>2. Security Concern:<br>The absence of a liveness probe can lead to a situation where a container is stuck in an unresponsive state, causing service disruption. This can potentially be exploited by an attacker to cause denial of service. However, it's also important to note that an improperly configured liveness probe can cause cascading failures by continuously restarting containers that are experiencing temporary issues.<br><br>3. Secure Code Example:<br>To fix this issue, you should define a liveness probe in your Kubernetes configuration file. Here is an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  labels:<br>    test: liveness<br>  name: liveness-http<br>spec:<br>  containers:<br>  - name: liveness<br>    image: k8s.gcr.io/liveness<br>    args:<br>    - /server<br>    livenessProbe:<br>      httpGet:<br>        path: /healthz<br>        port: 8080<br>      initialDelaySeconds: 3<br>      periodSeconds: 3<br>```<br>In this example, the liveness probe checks the `/healthz` endpoint on port `8080` of the container every 3 seconds, starting 3 seconds after the container is started.<br><br>4. Best Practices:<br>- Always define a liveness probe for your containers to ensure they are functioning correctly.<br>- Configure the liveness probe correctly. Avoid setting the `periodSeconds` value too low, which could lead to unnecessary restarts.<br>- Use readiness probes in addition to liveness probes. While liveness probes check if a container is running, readiness probes check if a container is ready to accept requests.<br>- Monitor the performance of your containers and adjust the liveness probe settings as needed.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#when-should-you-use-a-liveness-probe" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge info">INFO</span>
                        <h3 class="finding-title">Liveness Probe Is Not Defined</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>17</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> In case of an unresponsive container, a Liveness Probe can help your application become more available since it restarts the container. However, it can lead to cascading failures. Define one if you really need it
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>   This security issue is indicating that a Liveness Probe is not defined in your Kubernetes configuration file. A Liveness Probe is a diagnostic tool used by Kubernetes to check if a container is still running as expected. If the container is unresponsive, the Liveness Probe will trigger a restart of the container. However, it's not defined in your current configuration, which means if your container becomes unresponsive, Kubernetes won't know to restart it.<br><br>2. Security Concern:<br>   If a container becomes unresponsive and a Liveness Probe is not defined, the container could remain in an unresponsive state indefinitely. This could lead to service disruptions, degraded performance, and potential security vulnerabilities if the unresponsive container is part of a larger system.<br><br>3. Secure Code Example:<br>   Here's how you can define a Liveness Probe in your Kubernetes configuration file:<br>   ```<br>   apiVersion: v1<br>   kind: Pod<br>   metadata:<br>     labels:<br>       test: liveness<br>     name: liveness-http<br>   spec:<br>     containers:<br>     - name: liveness<br>       image: k8s.gcr.io/liveness<br>       args:<br>       - /server<br>       livenessProbe:<br>         httpGet:<br>           path: /healthz<br>           port: 8080<br>         initialDelaySeconds: 3<br>         periodSeconds: 3<br>   ```<br>   In this example, the Liveness Probe is checking the `/healthz` endpoint on port `8080` of the container every 3 seconds, starting 3 seconds after the container starts.<br><br>4. Best Practices:<br>   - Always define a Liveness Probe for your containers in your Kubernetes configuration file. This will help ensure that your containers are always responsive and running as expected.<br>   - Be careful with the parameters of your Liveness Probe. Setting the `initialDelaySeconds` or `periodSeconds` too low could lead to unnecessary restarts of your container.<br>   - Monitor the results of your Liveness Probes. If a container is frequently becoming unresponsive and being restarted, this could indicate a larger issue with your application.<br>   - Use Readiness Probes in addition to Liveness Probes. While a Liveness Probe checks if a container is running, a Readiness Probe checks if a container is ready to accept traffic. This can help prevent traffic from being sent to a container that is running but not yet ready to handle requests.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#when-should-you-use-a-liveness-probe" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge info">INFO</span>
                        <h3 class="finding-title">Liveness Probe Is Not Defined</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>202</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> In case of an unresponsive container, a Liveness Probe can help your application become more available since it restarts the container. However, it can lead to cascading failures. Define one if you really need it
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation: <br>This issue means that the Kubernetes configuration file does not have a Liveness Probe defined for a container. A Liveness Probe is a diagnostic tool used by Kubernetes to check if a container is still running and responsive. If the probe fails, Kubernetes will automatically restart the container. Not having a Liveness Probe could potentially lead to unresponsive or hung containers going unnoticed, affecting the availability of your application.<br><br>2. Security Concern: <br>While this is more of a reliability concern than a direct security issue, it can indirectly affect security. If a container is unresponsive, it could be due to a variety of reasons, including a potential security breach. An unresponsive container might also affect the overall health of your application, making it more susceptible to attacks.<br><br>3. Secure Code Example: <br>To fix this, you need to define a Liveness Probe in your Kubernetes configuration file. Here's an example:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  labels:<br>    test: liveness<br>  name: liveness-http<br>spec:<br>  containers:<br>  - name: liveness<br>    image: k8s.gcr.io/liveness<br>    args:<br>    - /server<br>    livenessProbe:<br>      httpGet:<br>        path: /healthz<br>        port: 8080<br>      initialDelaySeconds: 3<br>      periodSeconds: 3<br>```<br><br>In this example, the Liveness Probe is set to make an HTTP GET request to the '/healthz' endpoint on port 8080 of the container every 3 seconds, starting 3 seconds after the container launches.<br><br>4. Best Practices: <br>- Always define a Liveness Probe for your containers to ensure they are running and responsive.<br>- Be careful with the parameters of the Liveness Probe. Setting the probe to check too frequently can cause unnecessary restarts. Conversely, setting it to check too infrequently can lead to long periods of unresponsiveness.<br>- Consider also using Readiness Probes and Startup Probes. A Readiness Probe checks if your application is ready to accept traffic, and a Startup Probe checks if your application has started successfully.<br>- Regularly review and update your Liveness Probes as your application evolves.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#when-should-you-use-a-liveness-probe" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                
                <div class="finding-card">
                    <div class="finding-header">
                        <span class="severity-badge info">INFO</span>
                        <h3 class="finding-title">Using Kubernetes Native Secret Management</h3>
                    </div>
                    <div class="finding-details">
                        <div class="detail-item">
                            <strong>üìÅ File:</strong> <code>./test_data/iac_example/kubernetes_vulnerable.yaml</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìç Line:</strong> <code>63</code>
                        </div>
                        <div class="detail-item">
                            <strong>üìù Description:</strong> Kubernetes External Secret Storage and Management System usage should be considered if you have more complex secret management needs, rather than using Kubernetes Secrets directly. Additionally, ensure that access to secrets is carefully limited
                        </div>
                    </div>
                    <div class="finding-explanation">
                        <h4>üîç Explanation & Fix</h4>
                        <div class="explanation-content">
                            1. Explanation:<br>The issue is related to the use of Kubernetes Secrets for storing sensitive data. Kubernetes Secrets are a simple way to store and manage sensitive data like passwords, OAuth tokens, and ssh keys. However, if your application has more complex secret management needs, it's recommended to use an external secret storage and management system. Additionally, access to these secrets should be limited to only those who need it.<br><br>2. Security Concern:<br>Using Kubernetes Secrets directly can be a security concern because they are not designed for complex secret management needs. They are stored in etcd which is accessible to any user with API access. If not properly configured, sensitive data could be exposed to unauthorized users. <br><br>3. Secure Code Example:<br>Instead of using Kubernetes Secrets directly, consider using an external secret management system like HashiCorp Vault. Here is an example of how to use Vault with Kubernetes:<br><br>```yaml<br>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: mypod<br>spec:<br>  containers:<br>  - name: mypod<br>    image: redis<br>    volumeMounts:<br>    - name: foo<br>      mountPath: "/etc/foo"<br>  volumes:<br>  - name: foo<br>    projected:<br>      sources:<br>      - secret:<br>          name: mysecret<br>          items:<br>          - key: username<br>            path: my-group/my-username<br>```<br>In this example, the secret is stored in Vault and the path to the secret is provided in the Kubernetes Pod specification.<br><br>4. Best Practices:<br>- Use an external secret management system like HashiCorp Vault, AWS Secrets Manager, or Google Secret Manager for complex secret management needs.<br>- Limit access to secrets. Only give access to those who need it and use RBAC to enforce access controls.<br>- Regularly rotate secrets.<br>- Enable encryption at rest for your secret management system.<br>- Monitor and log access to secrets to detect any unauthorized access.
                        </div>
                    </div>
                    <div class="finding-footer">
                        <a href="https://kubernetes.io/docs/concepts/configuration/secret/" target="_blank" class="learn-more-btn">üìö Learn More</a>
                    </div>
                </div>
                </div>
            </div>
        </div>
        
        <div class="footer">
            <p>Generated by DriftBuddy - AI-Powered Security Scanner</p>
        </div>
    </body>
    </html>
    